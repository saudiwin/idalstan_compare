---
title: 'Generalized Ideal Point Models for Noisy Dynamic Measures in the Social Sciences'
date: last-modified
execute: 
  cache: true
  echo: false
  warning: false
  error: false
date-format: long
format:
  wordcount-pdf: 
    wordcount-banner: true
linestretch: 1.5
abstract: "This paper presents idealstan, a measurement framework that addresses long-standing issues in the estimation of dynamic ideal points by incorporating diverse time series processes, techniques for sparse data, and adjustments for selection bias. In addition, the model employs Stan, a robust Markov Chain Monte Carlo sampler, to permit parallelization of Bayesian inference and to resolve issues of convergence for high-dimensional time-series. The model is compared to existing dynamic ideal point models using Monte Carlo simulations to show that idealstan is both faster and less biased than existing approaches. The model is then applied to the challenge of estimating monthly trajectories from 1990 to 2018 for Congressnpersons' ideal points while accounting for selection bias. WORD COUNT: {{< words-sum body-note >}}"
bibliography: "BibTexDatabase.bib"
fig-cap-location: top
csl: apsr.csl
---

```{r setup, include=FALSE}

require(dplyr)
require(ggplot2)
require(tidyr)
require(readr)
require(lubridate)
require(idealstan)
require(forcats)
library(patchwork)
library(posterior)
library(marginaleffects)
library(tinytable)

# set to true to create all analyses from fitted idealstan models
# requires significant time and RAM (especially the latter)
run_all <- F

# see file to_cluster.R for the fitting of the actual idealstan models

# load the datas

rollcalls <- readRDS('data/rollcalls.rds')

unemp1 <- rollcalls %>% 
  select(cast_code,rollnumber,congress,
         bioname,party_code,date_month,unemp_rate) %>% 
  mutate(item=paste0(congress,"_",rollnumber),
         cast_code=recode_factor(cast_code,Abstention=NA_character_),
         cast_code=as.numeric(cast_code)-1,
         bioname=factor(bioname),
         unemp_rate=100*unemp_rate,
         bioname=relevel(bioname,"DeFAZIO, Peter Anthony")) %>% 
  distinct %>% 
  filter(party_code %in% c("R","D")) %>% 
  mutate(party_code=factor(party_code))

# drop legislators who vote on fewer than 25 unanimous bills

check_bills <- group_by(unemp1,item,party_code,cast_code) %>% count %>% 
  group_by(item,party_code) %>% 
  summarize(prop=n[cast_code==1] / sum(n),
            n_vote=sum(n)) %>% 
  ungroup %>% 
  mutate(prop=ifelse(prop==.5,
                          sample(c(.49,.51),1),
                     prop),
    util_func=(1 / sqrt((.5 - prop)^2))*n_vote) %>% 
  arrange(desc(util_func))

legis_count <- group_by(unemp1, item) %>% 
  mutate(unan=all(cast_code[!is.na(cast_code)]==1) || all(cast_code[!is.na(cast_code)]==0)) %>% 
  group_by(bioname) %>% 
  summarize(n_votes_nonunam=length(unique(item[!unan])))

# check number of days in legislature

num_days <- distinct(unemp1,bioname,date_month) %>% 
  count(bioname)

```

\newpage

This paper resolves long-standing issues in dynamic ideal point modeling within a novel Bayesian modeling framework. The goal is to make the ideal point model a robust tool for measuring complex latent traits, applicable to data from social media, surveys, legislatures, bureaucracies, or social networks. Implemented in Stan [@CarpenterGelmanHoffmanEtAl2017], a probabilistic programming language for Bayesian inference, this framework allows for flexible time-series processes, diverse data distributions, and novel missing data adjustments. The `idealstan` R package implements these models with advances in Bayesian computation to handle larger datasets than was previously feasible with Markov Chain Monte Carlo methods.

Initially designed for measuring U.S. Congressional ideology, the ideal point model has proven valuable for various social science measurement challenges, including Twitter data [@barbera2015; @kubinec2018], the UN Security Council [@baileyEstimatingDynamicState2017], and the level of democracy [@coppedgeMethodologyVarietiesDemocracy2019; @pemsteinDemocraticCompromiseLatent2017], to name only a few. Its utility lies in measuring latent traits where data are imperfect proxies of theoretically defined concepts. However, while social scientists increasingly have access to granular time-varying data--and often with considerable measurement error--dynamic ideal point modeling remains difficult to apply to big datasets, especially in the presence of non-ignorable missing data, sparse measures, and mixed outcomes.

This paper's contributions are two-fold: 1) implementing and evaluating robust time-series specifications for dynamic ideal point estimation, and 2) addressing estimation challenges with big and sparse datasets. The main advantage of this framework is that it permits nuanced identification of time-varying latent traits, permitting researchers to overcome vexing measurement challenges and avoid downstream inferential errors. The approach further democratizes advanced measurement models, making them accessible to applied researchers without advanced training in Bayesian methods [@claassen2020].

I first show with Monte Carlo simulations how the `idealstan` method compares to existing approaches. As I show, `idealstan` is notably faster than existing Bayesian implementations but also shows improved identification of latent traits in terms of RMSE and sign rotation. Furthermore, idealstan's implementation of novel time series specifications like Gaussian Processes helps it to identify latent traits that existing methods lack the flexibility to estimate. Finally, idealstan is robust to the inclusion of selection bias (i.e., non-ignorable missing data), which can exacerbate identification and measurement issues in available methods.

To show how idealstan can identify granular latent traits, I estimate monthly ideal points for all legislators in the U.S. House from 1990 to 2018. This type of estimation is impossible with existing methods because sparsity undermines models' ability to converge. As I show, properly estimating latent traits involves carefully selecting a time-series specification that is theoretically-motivated and also employing a statistical methodology that can maximally extract latent signals from the data.

By combining missing data adjustments, time-varying inference, and scalable computation, idealstan provides a robust method for addressing next-generation dynamic measurement challenges across the social sciences.

# The Importance of Measuring Over Time

<!-- > "There is nothing more deceptive than an obvious fact." -->

<!-- > -- Sherlock Holmes -->

<!-- The teaching of applied statistics generally presents models as estimating unbiased quantities given a recipe list of datasets. Of course, the datasets that social scientists produce rarely fit these clean molds, which leads to numerous statistical fixes to correct for poorly-fitting models, such as zero-inflated Poisson models in the case of counts [@lambertZeroinflatedPoissonRegression1992] and other more generic fixes like clustered standard errors [@abadieWhenShouldYou2023]. What all of these approaches have in common is a focus on adjusting for measurement issues in the dependent variable, while the data are assumed to represent known facts. -->

<!-- Measurement models flip this perspective on its head. Rather than assuming that only the outcome needs a statistical distribution, a measurement model implements a statistical distribution *for the data*. Making this mental switch enables us to think of the data as only one set of observed indicators generated by a latent, unseen construct. Using a single indicator—as is often done in applied analyses—amounts to the very strong assumption that an indicator is a perfect stand-in for concept to be measured; i.e., that measurement error is zero. -->

<!-- In the social sciences, measurement error is almost always non-trivial. For example, voting records, which are one of the most directly observable forms of political data, are nonetheless riddled with inaccurate records and duplicates [@ansolabehereQualityVoterRegistration2010]. Measurement error is more commonly recognized with higher-level concepts like democracy that have led to ongoing scholarly debates [@littleMeasuringDemocraticBacksliding2024; @millerHowLittleMengs2024; @knutsenConceptualMeasurementIssues2024]. However, virtually all social science variables have at least some measurement error because collecting data on humans, who have agency that allows them to hide or misreport information, is never a trivial matter. While many scholars recognize these structural limitations, and measurement models have a long history in statistics [@flake2019], it can be relatively difficult to apply measurement models to real-world datasets, especially when the tool needs to be able to incorporate relevant theoretical priors about the latent trait. Building a theoretically-valid measurement model can be a very useful contribution to our ability to analyze concepts, such as the influential work on legislative ideology in the United States [@poole1997; @jackman2004], but deriving an appropriate model can involve extensive technical work to incorporate theoretical priors [@caugheyDynamicEstimationLatent2015]. The model I present in this paper, which I shall refer to as `idealstan` as that is the name of the associated software package, aims to contribute a general purpose measurement model that applies to diverse datasets and problems while also permitting the incorporation of theory into measurement choices. By doing so, I hope to mainstream relatively complex Bayesian measurement models that have often taken months or years to derive and permit them to be rapidly deployed to diverse phenomena. -->

Ideal point models have a long history in political science, and time-varying ideal point models have been available for almost as long. Latent measurements derived from these models, including DW-NOMINATE scores for the U.S. Congress and comparable measures for the U.N. General Assembly, democracy, state capacity, and human rights compliance [@fariss2014; @baileyEstimatingDynamicState2017; @poole1997; @shor2022; @hansonLeviathansLatentDimensions2021; @coppedgeMethodologyVarietiesDemocracy2019], have made an outsize impact on research in the discipline. These uni-dimensional estimates of hard-to-measure traits permit researchers to test hypotheses with over-time variation to explore evolutionary changes. While over-time variation is no substitute for a causally-identified experiment, it can help rule out important inference issues like reverse causality and is necessary for dynamic causal inference methods like difference-in-differences.

For these reasons, the growth of the availability of big datasets with over-time variation, such as datasets derived from social media, digitized historical documents, or long-running surveys, offer important new areas of application of the ideal point model. However, it can be difficult to repurpose ideal point models to new time-varying domains as over-time variation poses significant hurdles when trying to identify a noisy latent trait. Many advances in ideal point modeling remain limited to static models, such as missing-data adjustment [@rosas2015] and diverse outcomes [@hare2018; @duck-mayr2020], while available dynamic models offer a limited range of time-series processes that may not apply well to diverse phenomena [@reunig2019].

The aim of this paper is to derive novel parameterizations of ideal point models to address these long-standing issues. I argue that a useful time-varying ideal point model should be able to handle diverse types of data, adjust for non-ignorable missing data (i.e., selection bias), allow for diverse time series processes, and be robust to both computational complexity and high-dimensional inference. These are considerable challenges, but the model presented here makes progress on all of these fronts.

Before beginning the model definition, I first note that I will only consider the case of a 1-dimensional latent variable. The reasons for this focus is largely practical; analyzing multiple-dimensional latent variables would complicate the syntax without adding any useful new features as multi-dimensional ideal point analysis has already been extensively explored [@poole2005; @armstrong2014]. Furthermore, in social science settings, the dimensionality of latent variables is usually quite low and adding in additional dimensions can lead to overly-complex models that are difficult to interpret [@morucciModelComplexitySupervised2024], especially in the case of time-varying latent variables. In any case, all of the analyses that I present below can be straightforwardly modified to accommodate additional noncompensatory dimensions, and for that reason I leave that extension for future research.

In this paper I employ a Bayesian interpretation of this model [@jackman2004] that I estimate using Hamiltonian Markov Chain Monte Carlo (HMC) with Stan [@CarpenterGelmanHoffmanEtAl2017]. The two main advantages of using Stan and HMC is that HMC is an algorithm that is remarkably robust for high-dimensional inference of highly correlated parameters—as often occurs in dynamic latent variable modeling—and Stan's use of auto-differentiation permits it to incorporate a wide array of statistical distributions, whether for likelihoods or priors. This combination of robustness and flexibility is what underpins the advances described in this paper. A third and not-insignificant feature, as I explain later, is the ability to parallelize computations, which can dramatically reduce one of the biggest barriers to Bayesian modeling—computational overhead.

I am interested in posterior inference on a set of unobserved parameters $\{\alpha_{it},\gamma_j,\beta_j\} \in \theta$ that is conditional on the observed data $Y_{ij}$, or $Pr(\theta|Y_{ij})$. To do so, I define independent priors on $\theta$, $Pr(\theta)$, which is multiplied by the likelihood $L(\cdot)$ that is defined by @eq-Bayes:[^1]

[^1]: I use the proportional symbol $\propto$ to reflect the fact that the term in the denominator representing the probability of the data $Pr(Y_{ij})$ is normally omitted in computational Bayesian analysis as it is fixed with respect to the parameters [@gelman2013].

$$
Pr(\theta|Y_{ij}) \propto Pr(\theta)L(Y_{ij}|\theta)
$$ {#eq-Bayes}

Given the standard definition of an ideal point model as a form of an item-response theory specification [@bafumiPracticalIssuesImplementing2005], we can estimate the time-varying ideal points $\alpha_{it}$, discrimination parameters $\gamma_j$, and difficulty parameters $\beta_j$ given our observations $Y_{ijt}$ for each person $i$, time point $t$, and indicator $j$ with the following likelihood $L(\cdot)$:

$$
L(Y_{ijt}=1) = \prod^{I}_{i=1}\prod^{T}_{t=1} \prod^{J}_{j=1} logit^{-1}(\gamma_j \alpha_{it} - \beta_j)
$$ {#eq-basic}

We begin with the assumption that $Y_{ijt}$ is distributed Bernoulli and requires a logit link function as is customary in the literature; later on we will relax this restriction.

Given that @eq-basic has no data on the right-hand side, we must impose some restrictions or priors to identify the parameters [@bafumiPracticalIssuesImplementing2005]. In a Bayesian context, we assign informative prior distributions to identify the scale and rotation of the latent traits/ideal points $\alpha_{it}$. To implement a time-series process for $\alpha_{it}$, we incorporate the time-series process in the prior [@martin2002]. In other words, the time-series method is defined over the latent trait in the latent space, which permits direct inference of the unobserved quantity and robust uncertainty estimation so long as the MCMC algorithm converges.

## Time-varying Inference

In this section, I show how the dynamic ideal point model can be defined for several different time-series processes for a set of latent dynamic ideal points $\alpha_{it}$.

There are two main specifications that have been employed in modeling approaches to date. The first method, polynomials or splines, were first used by the DW-NOMINATE model [@rosenthal2007; @armstrong2014; @Caughey2016], which employs a linear time trend to allow for over-time variation at the legislator level across Congressional sessions. The DW-NOMINATE method allows for up to a 3rd-order polynomial trend, but these have not been implemented in DW-NOMINATE scores to date. @bailey2007 employs a 4th-order polynomial to provide common space scores for the American judiciary, which he argues, "represents a trade-off between flexibility and computation."

The other widely-used approach is the random walk model of @quinn2002 in which ideal points in time $t$ are equal to ideal points in $t-1$ plus a Normally-distributed random jump. This much more flexible model—at the cost of estimating one latent trait per person per time point—is quite helpful at determining the probable trend in ideal points over time because it imposes virtually no structure on the time-series trends, although as I discuss below, it adds a considerable greater demand in terms of finding a unique identified solution. While the random walk prior has been widely applied in empirical settings, @reuning2019 note that it may have difficulty capturing rapid shifts in time trends.

While I implement both of the methods above—including a generalized parameterization for polynomial splines—I also implement two new methods, the autoregressive equation of order 1 (AR(1)) and the Gaussian process. These two time-series specifications have not been employed to date in the dynamic ideal point literature, and I incorprorate them because they may be relevant to specific measurement challenges. An AR(1) time-series variable will always return to an over-time mean, i.e, it is "stationary." By contrast, a Gaussian process is one of the most flexible kinds of time-series processes, permitting dramatic shifts from one time point to the next—at the cost of considerably more computational complexity than the random-walk parameterization.

It is not my intention in this paper to determine which of these time-series specifications is superior. Rather, I aim to show how these different processes can solve different measurement challenges given costs and trade-offs in terms of the availability of prior information, the nature of the latent trait, and computational feasibility.

I begin by defining the random walk by stipulating a relationship in the prior between $\alpha_{it}$ and $\alpha_{it-1}$:

$$
\alpha_{it} \sim N(\delta_i+ \alpha_{it-1},\sigma_i)
$$

We further give every person $i$ a separate over-time variance parameter $\sigma_i$ and intercept/offset $\delta_i$. For computational reasons, the implementation of this model in `idealstan` employs a non-centered parameterization [@betancourt2013] that reduces dependence between the variance $\sigma_i$ and the prior value of $\alpha_{it-1}$:

$$
\begin{split}
\epsilon_{it} &\sim N(0,1)\\
\sigma_i &\sim E(1)\\
\alpha_{it} &= \delta_i + \alpha_{it-1} + \sigma_i\epsilon_{it}
\end{split}
$$

The first new time-series model that I consider is an AR(1) or autoregressive parameterization, which can be understood as a generalization of the random-walk model. To do so, we must add a parameter to the model, $\psi_i$ that is constrained to lie in the $(-1,1)$ interval:

$$
\begin{split}
\epsilon_{it} &\sim N(0,1)\\
\sigma_i &\sim E(1)\\
\alpha_{it} &= \delta_i + \psi_i\alpha_{it-1} + \sigma_i\epsilon_{it}
\end{split}
$$

If $\psi_i$ lies in the $(-1,1)$ interval, then the series is stationary and will always return or decay to the long-term mean of the series $\delta_i$. By comparison, the random walk model can be thought of as an AR(1) model where $\psi_i$ is fixed at 1. This type of time series model implies that a given person or legislator has a stable over-time average ideal point and that movement away from that over-time average is some kind of exogenous shock that will eventually decay.

Both the AR(1) model and the random walk are models of discrete time, or a set number of time points. The next two time series processes that I consider are both continuous models of time; given any set of time points, they are capable of producing smooth interpolations between observed time points. The first such time process I consider is a Gaussian process ideal point model in which the ideal points $\alpha_{it}$ vary in terms of a Gaussian process with a squared-exponential kernel. A Gaussian process (GP) is chosen because it represents the most flexible framework possible for semi-parametric inference of either spatial or temporal autocorrelation [@rasmussen2006]. Given the limited application of this model in political science research (though see @duck-mayr2020 for an application to item-response theory utility functions), I briefly review the notation for a GP before combining it with the ideal point model.

A GP is usually defined as a "distribution over functions" [@rasmussen2006, 13]. We assume that underlying an observed time series $x_t$ is an unobserved function $f(x_t)$. To estimate the most likely values of $f(x_t)$, $f(x_t)$ is defined as a multivariate Normally-distributed latent variable where the mean and covariance of this distribution are themselves functions of the mean and covariance of $f(x_t)$:

$$
f(x_t) \sim N(\mu(f(x_t)),\Sigma(f(x_t)))
$$ {#eq-gp}

Usually the $\mu(f(x_t))$ function is assumed to be 0 as it does not vary with $t$. Instead, much of the flexibility of the distribution involves specifying a function for $\Sigma(f(x_t))$, the covariance. The function most often used, and incorporated here, is the squared-exponential kernel $k_t(\cdot)$:

$$
k_t(x_t,x_{t'}) = \large \sigma^2_f  e^{\displaystyle -\frac{(x_t - x_{t'})^2}{ 2l^2}} + \sigma^2_{x_t}  
$$ {#eq-gpcov}

What this function does is convert all the distances in terms of time between $x_t$ and $x_{t'}$ into a positive semi-definite covariance matrix $\Sigma$. We can then sample from this multivariate Normal with covariance $\Sigma$ to estimate a distribution over the possible time-series functions $f(x_t)$. The GP framework is so flexible that it can fit an *infinite* number of basis functions of $x_t$ [@rasmussen2006, 14]. Other semi-parametric functions, such as splines and polynomials, are special cases of the GP for certain values of the hyper parameters used in the covariance function [@rasmussen2006, 137-140]. What is even more appealing is that the GP can handle time-varying covariates $X_t\phi'$ simply by including them in place of $\mu(x_t)$. The multivariate Normal will then sample from the specified covariate matrix while averaging over the possible values of the covariates.

What gives the GP the ability to fit so many different functions are the three hyper-parameters in @eq-gpcov. $\sigma^2_{if}$ can be referred to as the marginal standard deviation and represents the total amount of variance in $x_t$ explained by the covariance function $k(x_t,x_{t'})$. A higher value for this hyperparameter will result in more bounce in the time series. $\sigma^2_{ix_t}$, on the other hand, represents residual variance in the time series $x_t$ that the covariance function does not fully explain (which could be interpreted as additional measurement error). Finally, the length-scale parameter $l^2_i$ represents a smoothing factor controlling how much different in time are correlated together. A very low length-scale will allow the time-series to cross the origin at a much higher rate. Conversely, higher length-scales result in smoother functions. The three hyper-parameters both overlap and interact, which makes isolating the effect of any one hyper-parameter difficult.

To employ the GP as a time process for ideal points, we simply replace the observed $x_t$ with the latent ideal points $\alpha_{it}$:

$$
\alpha_{it} \sim N(0,k_t(\alpha_{it},\alpha_{it'}))
$$

This straightforward parameterization shows some of the power of Bayesian modeling. We can include virtually any time process by simply changing the definition of the prior of the ideal points $\alpha_{it}$. Of course, whether this model can be identified in a latent variable model with measurement error is a different question. Because of the GP's complexity, I turn next to consider a different option, basis splines, that permit more control over the flexibility of the time-series process.

Splines are a method of constructing semi-parametric time-series by combining polynomial functions of time [@ratkovic2010]. While there are many different kinds, I consider here basis splines due to their ability to fit into the latent variable framework. A basis spline separates a polynomial function of time into individual basis functions that the space of all possible polynomial functions for a given polynomial degree [@deboorCalculatingBsplines1972; @gordonBSPLINECURVESSURFACES1974]. Because each of the individual functions are relatively simple, this construction allows for a wide variety of polynomial functions to be derived while also enabling efficient sampling.

One central feature of splines is that they are polynomial functions defined over a series of sequential partitions of a given time series known as knots. A spline of a given order is a polynomial function of a given degree (by convention, one less than the order) that is defined for each sequential partition given by the location of the knots. Each piecewise polynomial function must begin and end at the same point of each sequential partition, ensuring a smooth combined function.

The presentation below follows that of @kharratzadeh2017. Given a spline order $d$ and a given number $s$ of sequential knots $q$, we can define a basis matrix $B_{q,d}$ as a function of continuous time points $t \in \mathbb{R}|t_{min} < t < T$:

$$
B_{s,d}(t) = \omega_{s,d} B_{s,d}(t) + (1 - \omega_{s+1,d}B_{s+1,d-1}(t))
$$

where the function $\omega(\cdot)$ can be defined as:

$$
\omega_{s,d} = \begin{cases} 
\frac{t-q_s}{q_{s+d-1}-q_s}, & \text{if } q_s \neq q_{s+d-1} \\
0, & \text{otherwise}
\end{cases}
$$

The resulting basis matrix $B_{s,d}(t)$ is a set of vectors of number of columns $T$ and number rows equal to the order of the spline. The polynomial function of $t$ can then be generated by multiplying the basis matrix by a vector of spline coefficients $A_i$ :

$$
S_{q,d}(t) = B_{s,d}(t)A_i
$$

where the length of spline coefficient vector $A_i$ is equal to the number of knots plus the degree of the spline. While in practice $t$ is in a set of discrete time points, defining the function over a continuous interval permits interpolation of the time series, a convenient feature of the spline technique.

We can then sample the spline coefficients by defining a loose prior distribution for $A_i$ as a function of a scaling parameter $\tau$ and a location parameter $A'_i$:

\begin{align}
A'_i &\sim N(0,1)\\
\tau &\sim E(1)\\
A_i &= A_i'\tau
\end{align} \label{eq-splineprior}

The adjustment with $A_i'$ and $\tau$ is to permit non-centered sampling which is more efficient.

The main advantage of using basis splines as I demonstrate in the empirical application is that it is possible to fine-tune the complexity of the time series function, which is valuable as it allows for theoretical priors to govern the amount of possible over-time variation in the latent trait. Increasing either the degree of the polynomial or the number of knots increases the number of coefficients per person by one. In cases of high sparsity, a low number of knots (or even no knots at all) and a low degree polynomial can be used to find a stable latent variable estimation even with very limited data per time point. This point was first made by @bailey2007 and, I argue, is a major reason why splines can be a useful way to incorporate theoretical priors into ideal point estimation.

In summary, these different time series models can be understood in terms of order of complexity. Except for splines, all of the time series models require at least one parameter per time point $t$. The random walk is the simplest of these models as it only requires one additional parameter, $\sigma_i$, per person $i$. The AR(1) model is more complex as it introduces an additional parameter to control the decay rate in the series, $\psi_i$. The GP is the most complex as it has three separate parameters per person, $\sigma^2_{if}$, $l_i^2$ and $\sigma^2_{iY}$, in addition to one estimate per time point $t$. For these reasons, the GP is the most demanding model to fit for a latent variable model as there needs to be a substantial amount of data per time point to estimate a flexible time trend while also separating our measurement noise.

For these reasons, I argue that lower-dimensional time series methods like splines can be useful even given that functions like the GP (or even a random walk) are by nature more flexible. As the ideal point model is a measurement model, it can be difficult to separate measurement noise from the additional noise arising from flexible time series processes. In order for measurement to be successful, the time series specification should match the theoretical understanding of the latent construct of interest, which I will illustrate in the empirical section.

## Time-Varying Non-Ignorable Missing Data

Missing data is a tricky problem to address in latent variable models because latent variables are themselves defined as missing data. For this reason, a straightforward approach of imputing data conditional on the model assumes that missingness is orthogonal to observed ideal points [@rubin2002]. In many situations, this assumption may not be realistic when there is reason to believe that social actors, such as legislators, do not respond to stimuli out of a desire to avoid revealing their latent trait. In this section I define a model for non-ignorable missingness in ideal points that can be integrated with time-varying processes (and of course applies to static models as well).

There are relatively few discussions of missing data in the ideal point literature, in part because missingness in latent variable models is a tricky problem. The most robust discussion of this problem to date is in @rosasNoNewsNews2015 who derive an extension in which legislators decide not to show up to vote if their ideal point differs from their party's ideal point. This model turns the missing data problem on its head by deriving interesting empirical findings from missingness. I adopt the insight of this model by focusing on endogenizing missingness rather than attempting to non-parametrically impute it away.

In the general IRT literature, there has been a more extensive discussion of non-ignorable missingness in terms of students failing to complete all test questions due to time constraints [@rose2017; @mislevy2016; @ulitzsch2020]. @holman2005 derived a general model of missingness in IRT by allowing for separate ability scores for observed and missing data where the scores are related to each other via a multivariate Normal distribution. These approaches are not as easy to apply to ideal point models because traditional IRT models make the assumption that positive item scores are always associated with higher ability scores, while the ideal point model allows for some items to negatively predict the latent trait (such as liberal and conservative bills in a legislature). Consequently, an appropriate missingness adjustment for an ideal point model must also allow for non-ignorable missingness to affect both poles of the latent trait.

The approach that I present here bears similarity to these other approaches in that it jointly estimates a set of latent missingness item parameters with the "observed" latent item parameters. The relationship between the missing and observed latent factors is endogenized by adopting a selection approach, specifically, a hurdle model, as I show below. Given the issues of identifying time-varying latent variables that I discuss later, the proposed missingness model allows for substantial inflation or deflation of the latent trait due to selection bias while still remaining tractable for dynamic ideal point estimation and identification.

To define the missingness hurdle extension, I extend my notation of the outcome $Y_{ijt}$ by adding a subscript $r \in \{0,1\}$ for whether person $i$ chose to vote or answer item $j$ ($r=1$) or chose not to answer ($r=0$). We can then add a separate selection model that first estimates $Pr(r=0)$ and deflates the likelihood $L(Y_{ijtr}|\theta)$ accordingly:

$$
    L(Y_{ijtr}|\alpha_{it},\gamma_j,\beta_j,\nu_j,\omega_j) = 
    \prod^{I}_{i=1} \prod^{J}_{j=1}
    \begin{cases}
    \zeta(\alpha_{i}'\nu_j - \omega_j ) & \text{if } r=0, \text{ and} \\
    (1-\zeta({\alpha_{it}'\nu_j - \omega_j}))L(Y_{ijtr}|\alpha_{it},\gamma_j,\beta_j) & \text{if } r=1
    \end{cases}
$$ {#eq-inflate2}

I let $\zeta(\cdot)$ stand for the inverse logit function in the equation above. As can be seen, the selection model for missing data $\zeta(\alpha_i'\nu_j - \omega_j)$ is very similar to @eq-basic except that I have substituted a new set of discrimination $\nu_j$ and difficulty $\omega_j$ parameters. The ideal points $\alpha_{it}$ enter into both the selection model and the main ideal point model $L(Y_{ijtr}|\alpha_{it},\gamma_j,\beta_j)$. As such, the selection model is able to inflate or deflate the ideal points by taking into account a first stage process. The inclusion of $\nu_j$ and $\omega_j$ in the first-stage selection model creates a new "missingness" space where person $i$ first chooses whether she will cast a vote, send a tweet or answer a survey question, as the case may be. Only if the item is close to person $i$ in this missingness space will person $i$ then also decide to participate and provide information $Y_{ijtr=1}$ that is informative of their ideal point. This model can be interpreted as a censoring model where persons may choose to self-censor their ideal points.

The general form of the hurdle model--which is essentially a first-stage ideal point model--allows it to pick up a range of non-ignorable missingness patterns if missingness correlates with ideal points. The model does not make a priori assumptions about exactly why ideal points may determine missingness, and as such the model cannot provide such an interpretation a posteriori without taking time to interpret what the parameters of the selection model reveal about actors' intentions. The missingness discrimination parameter $\nu_j$ is very helpful for this purpose because it will indicate which set of items show correlation between missingness and one of the poles of the latent variable. For example, if the latent variable is constrained positive for liberal Senators, then high positive discrimination would indicate that more liberal Senators tend to be absent on a particular bill. A naive model that assumed that absences were ignorable would miss this pattern and treat these Senators as more moderate based solely on their observed voting record.

This model is also general enough to allow for the possibility that data is actually missing at random at the item level. If the discrimination parameter $\nu_j$ is zero then ideal points do not enter into the equation for missingness and $Pr(r=0)$ is equal to the item-specific intercept $\omega_j$. In the legislative context, this would be the same as estimating the probability of absence by the proportion of legislators who do not show up for particular bill. Including this parameter will also separate that item-specific random missingness from missingness patterns suspiciously associated with ideal points. For this reason, the selection model will perform at least as well as a standard imputation method where the missing ideal points are MCAR conditional on each item.

While this missingness model is readily applicable to a legislative context where legislators may not want to show up on votes depending on what the vote would reveal about their ideal points, it is also useful for other non-ignorable missingness patterns. Twitter data is an excellent example. It is well-established that estimating a latent trait like political polarization will be vastly over-stated if the tendency of Twitter (now known as X) users to select who they choose to follow is not taken into account [@Barbera2015]. The two-stage selection model can be used to estimate the people's propensity to retweet ideological content net of their self-selection into whom they follow on social media [@kubinecWhenGroupsFall2021]. For survey data, the two-stage model will take into account that respondents with high or low values on the latent trait could be more or less likely to provide an answer on a given question, and then backwards infer their true ideal point given those varying missingness rates by question.

## Identification of Time-Varying Latent Variables

As I have mentioned, identification for models with time-varying $\alpha_{it}$ is a challenging endeavor that lacks a comprehensive discussion in the literature, often leaving practitioners to spend considerable time guessing what settings might produce an identified latent variable. @quinn2002 addressed this issue for random-walk models by restricting the variance $\sigma_i$ to values strictly between 0 and 0.1. This hard-coded constraint forced the time series to change little over time, though it also limits inference as the time series can only differ so much from each other.

By contrast, I impose generally diffuse priors, or priors strong enough to impose some scaling identifiability, without forcing hard constraints on variance or other parameters. Rather than employ priors that try to fix the ideal point estimation, I argue that an appropriately theoretical model structure should allow for relatively weak priors, which should only be constrained to include relevant empirical information like the known ideological position of actors or items. When the data are both sparse and noisy, and time-varying estimation becomes non-trivial, then the model should employ a time series function like a 3rd-degree spline to remove the possibility of theoretically impossible shifts in ideal points over time. I discuss this approach in more detail in the empirical application and in the simulation results.

I define the priors in terms of the distributions assigned to each parameter in $\theta$:

\begin{align}
\alpha_i &\sim \text{Normal}(0,3)\\
\gamma_j &\sim \text{GeneralizedBeta}(2,2)\\
\beta_j &\sim \text{Normal}(0,3)
\end{align}\label{eq-genprior}

The ideal points $\alpha_i$ and the discrimination parameters $\beta_j$ are given a loose $N(0,3)$ prior that is weakly informative for most scales and is only needed to make the posterior proper. Identification of the posterior, which is an important issue as these types of measurement models can have multiple possible solutions with equal likelihood [@bafumiPracticalIssuesImplementing2005], happens via a Generalized Beta prior on the discrimination parameters $\gamma_j$:

$$
f(x; \alpha, \beta, a, b) = \frac{(x - a)^{\alpha - 1} (b - x)^{\beta - 1}}{(b - a)^{\alpha + \beta - 1} B(\alpha, \beta)}
$$ {#eq-genbeta}

where:

\begin{itemize}
    \item $\alpha > 0$ and $\beta > 0$ are the shape and scale parameters, analogous to the prior sample size,
    \item $a = -1$ and $b = 1$ define the support of the distribution,
    \item $B(\alpha, \beta) = \int_0^1 t^{\alpha - 1} (1 - t)^{\beta - 1} \, dt$ is the Beta function.
\end{itemize}

The reason I use the Generalized Beta distribution for the items is to resolve two problems at once. First, the restrictions on the range of the $\gamma_j$ parameters solve the problem of identifying a scale for the latent variable because the item discrimination parameters must stay within this range. The Generalized Beta distribution further allows this range to be both positive and negative, which is required for the ideal point model to estimate latent variables with bi-polarity (such as both conservative and liberal ideology). To identify the rotation of the ideal points, specific items can be given a very tight prior, such as:

$$
\gamma_{j=k} \sim \text{GeneralizedBeta}(10,1000)
$$ for a parameter with the majority its support around the point -0.98 and

$$
\gamma_{j=k'} \sim \text{GeneralizedBeta}(1000,10)
$$

for a parameter with the majority of its support around the point +0.98. The shape and scale parameters of the Generalized Beta have an intuitive interpretation as they represent the prior observed negative and positive observations for a given proportion. As a result, increasing both the scale and shape proportionally will preserve the sample expected value but reduce uncertainty around the expected value of the distribution. With larger datasets, increasing both the shape and scale to larger values, such as $(20000, 200)$, is necessary as the strength of the prior is relative to the size of the data as in @eq-Bayes. To fit a traditional IRT model, on the other hand, all this is necessary is to modify the lower bound ($a$) of the Generalized Beta distribution to 0 rather than -1.

Constraining the discrimination parameters builds on work that shows that identification of the items (i.e., votes) in an ideal point model has superior theoretical properties as it allows for the ideal points to float freely [@morucciMeasurementThatMatches2024]. In addition, constraining item parameters is helpful for dynamic ideal point estimation as items may occur at different points in the time series, whereas constraining dynamic ideal points is limited to fixing the starting positions of time series. With increasing time points, the starting positions of the actors become less influential, making identification more difficult to achieve.

I assign weakly informative values of +2 to each of the distribution's parameters (scale and shape) so that values of 0, or the midpoint of the distribution, are weakly preferred over the end points. With this prior, no further work is necessary to fix a scale and all other parameters can have weakly informative priors. In fact, if there is enough data, the discrimination parameters can have the uninformative Generalized Beta prior of +1, +1 so that all unconstrained discrimination parameters $\gamma_j$ are equally likely in the $(-1, 1)$ interval–which is much less informative than the Normal prior that is commonly used.

If the missing data is modeled directly as described previously, the model does not generally require more identifying information than previously stipulated (i.e., at least some observed item discriminations $\gamma_j$ pinned to high and low values). However, the missingness space does raise model complexity, it can require more computation and can in some cases require additional pinned item discrimination parameters. However, the missingness parameters themselves can have similar weakly informative Generalized Beta priors without causing any serious problems with identification:

\begin{align}
\nu_j &\sim \text{GeneralizedBeta}(2,2)\\
\omega_j &\sim \text{Normal}(0,3)
\end{align}\label{eq-missprior}

## Diverse Distributions

Up to this point, I have followed convention in assuming that the observed data $Y_{ijt}$ is a binary outcome (Bernoulli distribution). However, this is a limitation which unnecessarily restricts dynamic ideal point models to binary data. There are a number of implementations of mixed outcomes for static ideal point models, such as the Poisson "Wordfish" model [@slapin2008], the ordinal model in @imaiFastEstimationIdeal2016, and the multinomial model in @goplerudMultinomialFrameworkIdeal2019. The restriction of dynamic models to binary outcomes requires either dichotomization of the observed indicators or for scholars to employ a static model that obscures over-time variation.

Within the Bayesian framework I have proposed, and combined with the extraordinary flexibility of Stan's HMC engine, virtually any distribution for $Y_{ijt}$ is possible and it is even possible to let the distribution for $Y_{ijt}$ to vary at the item level, i.e., to include items of binary, ordinal, or even continuous distributions for the same latent variable [@kropko2013]. These types of mixed outcomes are especially important for survey data in which questions could have binary, ordinal, proportions, counts or continuous response distributions.

To use a different distribution, the likelihood function $L(\cdot)$ in @eq-inflate2 can be modified with any probability density or mass function that properly integrates or sums to 1 where $m$ is an index for all $M$ distributions used for the mixed outcome $Y_{ijtm}$:

$$
  L(Y_{ijtm}|\alpha_{it},\gamma_j,\beta_j) =  \prod_{i-1}^{I} \prod_{j=1}^{J} \prod_{t=1}^{T} \prod_{m=1}^{M} L_m(\gamma_j\alpha_{it}-\beta_j)
$$ {#eq-likem}

where $L_m$ represents the likelihood function for the $m$th distribution. All of these models can also be estimated marginal of missing data $r$.

`idealstan` implements such likelihoods for the Normal distribution, the log-Normal distribution, the Poisson distribution, ordinal outcomes for both rating scale and graded response formulations, and the Ordered Beta distribution for bounded continuous responses like percentages and proportions [@kubinecOrderedBetaRegression2022]. Furthermore, because of the modular nature of Stan's probabilistic program engine, these distributions can be mixed at the item level and can be jointly estimated with all of the time series processes and missing-data adjustments discussed previously. For reasons of brevity, I refer the reader to the first section of the supplementary information for the full derivation of these additional likelihoods.

## Identification and Starting Values

Identifying a single mode (or estimate) of time-varying latent variables given noisy data can be very difficult to achieve. As mentioned previously, @quinn2002 used a very restrictive bound on over-time variance to identify their estimates of Supreme Court justices. In order to estimate DW-NOMINATE using optimization methods, Poole and Rosenthal had to undergo a laborious process of choosing starting values for legislator ideal points, estimating dynamic scores, and then reviewing the scores to see if they resembled their theoretical priors [@carrollMeasuringBiasUncertainty2009]. Similarly, @barbera2015 scores were only possible to derive by using starting values for HMC with Stan that accorded with theoretical expectations about ideal point distributions.

While identifying the sign and rotation of ideal points with pinned discrimination parameters is sufficient for static models, high-dimensional time-varying latent variable model posteriors can be difficult for HMC to adequately explore even though it is the most robust and stable MCMC algorithm available [@CarpenterGelmanHoffmanEtAl2017]. For high-dimensional models, such as latent variables with hundreds of time points, HMC can end up in lower-probability modes that may have the opposite polarity of what the identification restrictions require. Specifying starting values that are closer to the posterior mode can help HMC converge and explore the posterior for very difficult problems. However, specifying starting values a priori can either be extremely tedious as the DW-NOMINATE procedure mentioned previously or it can result in latent variables that depend on arbitrarily chosen starting values that can mask convergence issues with HMC and result in fragile inferences.

To address this problem, `idealstan` implements a novel method for identifying starting values for high-dimensional HMC using what is known as the Pathfinder algorithm [@zhangPathfinderParallelQuasiNewton2021]. The Pathfinder algorithm is a form of variational inference [@goplerudMultinomialFrameworkIdeal2019; @Grimmer2011] that approximates the joint posterior by minimizing the Kullback-Leibler divergence with a form of Newton optimization. While this approximation is unlikely to capture the exactly correct form of the joint posterior distribution, it is remarkably robust considering its relative simplicity [@zhangPathfinderParallelQuasiNewton2021]. Importantly, Pathfinder integrates with the Stan framework so that Pathfinder can first be used to identify starting values for the HMC algorithm, which is particularly important for high-dimensional time-varying models. These starting values are optimal in the sense that they should be reasonably close to the posterior mode, and they also do not reflect researcher discretion, which makes them a more robust choice for measurement exercises as they are in principle replicable.

This method of "mode-finding" works so well that in fact it can even be used in place of the identification restrictions on item parameters mentioned previously, but it should be noted that by doing so, the ideal point model is no longer a confirmatory method and instead should be used in an exploratory context in which the derived latent variable may or may not have any clear empirical meaning. For all the empirical examples shown later, Pathfinder was used to ensure convergence of the HMC chains to the optimal mode conditional on informative identification conditions.

## Big Data Inference

The Achilles heel of Bayesian inference is generalizing to large data sets. Markov chains, which form the core of the estimation engine, cannot be easily parallelized as each iteration depends on the value of the previous iteration. However, recently the Stan team has implemented a way to parallelize the *posterior gradient* computation within a Markov chain, which are necessary and very expensive to calculate the iteration trajectories of Hamiltonian MCMC. Any model likelihood can be parallelized in terms of its gradient calculations so long as the parameters are conditionally independent within discrete subsets or "shards" of the data. For a static ideal point model, this means that the likelihood can be parallelized across subsets of persons or items. For a dynamic model, parallelization can be performed across persons as the ideal point for a given person at time $t$ depends on the value of the ideal point in $t-1$, which would break conditional independence of parameters if the data were subset by item.

This type of parallelization of the gradient calculations is implemented for all of the models previously described. This allows for the computation of Bayesian ideal points for datasets that were previously intractable due to the length of time required. However, it should be noted that this type of parallelization does have significant overhead as not all of the computations of the Markov Chain are parallelizable, and as a result significant information must be shared across parallel threads.

Even with these restrictions, though, this type of computation greatly expands the possibilities for Bayesian inference on big datasets, reducing the time required for model convergence by several orders of magnitude depending on the number of cores available. In the supplementary information section 3 I show a simulation of `idealstan` models of varying sizes and the associated speedups from using multiple cores. Moving from a single core to four cores, which is available on many laptops, can cut estimation time for a medium-sized dataset from approximately 20 minutes to approximately 7 minutes. With larger numbers of cores (up to the limit of what is available in single machines in conventional computing clusters), run times can be reduced to as low as a single minute for datasets of up to 100,000 observations.

In addition, variational methods that approximate posterior distributions can be used as well in place of MCMC for the largest datasets [@Grimmer2011; @goplerud2022]. @imaiFastEstimationIdeal2016 were the first to apply variational methods to ideal point estimation, demonstrating that the tool could achieve remarkable gains from speed. However, @imaiFastEstimationIdeal2016 had to derive a separate variational algorithm for each model, restricting the application of variational approximations to binary data in the case of dynamic ideal point modeling.

`idealstan` implements two variational methods that integrate with Stan and allow for all of the time-varying methods mentioned previously to be estimated. The first is the Pathfinder method described previously, which can also be used for final estimation in addition to finding starting values for MCMC. The second is the Laplace approximation of the joint posterior distribution [@rue2009], which estimates the mode of the posterior distribution with optimization and then uses the Normal distribution to calculate uncertainty. As with HMC and Pathfinder, the Laplace approximation makes use of Stan's probabilistic programming, which means that all of the models described previously can also be fit with the Laplace approximation.

Both of these methods are implemented in `idealstan` as both approximations may have utility depending on the particular measurement problem and the extent of bias caused by using an approximation to the true joint distribution. However, given that much is unknown about the performance of these algorithms with latent variable models, I test them against MCMC in the simulation I describe in the next section.

## Monte Carlo Simulation

# Simulation Results

In this section I present original empirical findings with the `idealstan` model while also showing how the model can be best applied to empirical data. In addition, I show the conditions under which `idealstan`'s innovations, especially the missing-data adjustment and varying time-series processes, can affect inferences that scholars make.

# Empirical Application

## Time Processes and Missing Data The U.S. Congress

In this section, I combine the missing-data adjustment along with the new time-series methods in `idealstan` to show how these innovations can affect our knowledge of legislative behavior. To do so, I fit time-varying ideal point models to estimate monthly legislator ideal points for all Congresspeople in the U.S. House. While my total dataset encompasses the period 1990 to 2018, I focus first on the 115th Congress (2016 to 2018) as it permits easier comparison of the different time series methods, especially as the most complicated time series methods are simply not tractable for the full dataset.

The estimation of ideal point marginal effects, which will be defined at the legislator level, allows me to answer a research question about the role of changes in district-level unemployment on political polarization in Congress. Recent research has proposed that changes in economic conditions, whether in terms of economic inequality and trade, may be affecting increasing polarization in the U.S. Congress. It has long been understood that there is an important interaction in terms of how voters perceive economic recession and their willingness to vote for parties in a polarized environment [@alesina1995].

However, we also know that the salience of policy-making varies with economic conditions that correlate with external shocks [@baker2016]. Most of the studies of trade and other shocks on Congress employ outcomes at the year level, whether the unit of analysis are states or Congressional districts. To show the power of the time-series models I employ, I analyze month-level variation in unemployment by Congressional district and its association with legislator ideal points in the House to see to what extent localized economic shocks are associated with either polarizing or de-polarizing voting behavior.

The data I collect come from the Bureau of Labor Statistic's Local Area Unemployment Statistics program, which produces unadjusted estimates of monthly unemployment by local-level units. To aggregate these numbers to the Congressional district level, I performed a spatial join by areal interpolation (i.e., weighted by amount of overlap) between the county-level monthly series and Congressional district boundaries from @lewis2013. The resulting dataset represents the average monthly unemployment data that would be most relevant to a particular Congressperson's district. Even with this high level of disaggregation, the dataset has very low missingness (50 district-year-month observations), which I impute non-parametrically using the technique of @StekhovenBuehlmann2012.

```{r histun}
#| label: fig-hist
#| fig-cap: "Histogram of District-Level Unemployment"

over_states <- readRDS('data/over_states.rds')

out_plot <- over_states %>% 
  ggplot(aes(x=unemp_rate)) +
  geom_histogram() +
  theme(panel.grid=element_blank(),
        panel.background = element_blank()) +
  ylab("") +
  xlab("Unemployment Rates") +
  scale_x_continuous(labels=scales::percent)

print(out_plot)

```

```{r plotcount}
#| label: fig-plotcount
#| fig-cap: Monthly Unemployment Rates by U.S. County, 1990-2018

county_series <- readRDS('data/county_series.rds')

county_series %>% 
  mutate(unemp_rate=unemp/labor_force) %>% 
  filter(state %in% c("AL","CA","NY","MI")) %>% 
    distinct %>% 
  ggplot(aes(y=unemp_rate,x=date_recode)) + 
    geom_line(alpha=0.2,aes(group=fips_county)) +
    theme(panel.grid=element_blank(),
          panel.background = element_blank(),
          strip.background = element_blank(),
          axis.text=element_text(face="bold"),
          strip.text = element_text(face="bold")) +
    facet_wrap(~state,scales='free_y',ncol=2) +
    xlab("") +
    ylab("Unemployment Rates") +
    scale_y_continuous(labels=scales::percent)
  
ggsave("month_unemp.png",width=5,height=4)
```

@fig-hist shows the overall distribution of district-level unemployment rates from 1990 to 2018 inclusive, with a distinct mode at 5.0%, the so-called natural rate of unemployment. @fig-plotcount breaks out the distribution of county-level unemployment rates over time by four states: Alabama, California, Michigan and New York. As can be seen, there is substantial variation both over time and across counties within a state. As such, this series is an excellent data source for showing the utility of time-varying ideal point models. We can also learn empirically from seeing how unemployment rates are differentially associated with legislator polarization in both parties in the House. With the `idealstan` model, we can consider the following hypothesis about the relationship between localized recession and legislators' ideal points:

> H1: When monthly unemployment rates rise, legislator voting behavior in each party become less polarized.

This hypothesis expresses the idea that in times of recession legislators should feel more pressure to work across the aisle to produce bipartisan legislation, which we can measure as the spread of ideal points between both parties. We can test this hypothesis by including unemployment rates as an exogenous covariate in the ideal point model while interacting it with an indicator for each legislator's party ID. The effect of the covariate then is associated with voting behavior differentially depending on the legislator's ideal point, party membership and the relative level of discrimination (polarization) of the vote.

To test this hypothesis, I fit each of the described time-series models separately to rollcall vote data from the 115th Congress both with and without the missing-data 1st stage adjustment. To identify the model, I pin the discrimination parameters $\gamma_j$ for three party-line votes in which all Republicans voted for a measure as +.997, and three Democrat party-line votes in which all Democrats voted against a measure as -.997. Utilizing the same pinned parameters for all models allows me to compare their relative ability to obtain identification of a single rotation of the ideal points.

For the spline specifications, I consider splines of degrees 2, 3 and 4 while using only one knot (i.e., one polynomial function for the whole time period). As I show later, when modeling multiple Congressional sessions, using knots for presidential administrations allows for period effects in the time series. With only one Congressional session, however, a single polynomial function is sufficient as it would not be expected for a member of Congress to switch their ideological position in a noticeable way more than once or perhaps twice per term.

As a result, I do find that the simpler time series models, i.e. the splines, are more stable and easier to identify with this dataset with the same six restrictions on item discriminations. The other more complicated time-series models are largely identified because of the use of Pathfinder to identify starting values in a single mode. While this is a valid form of measurement inference, as I show below, these more flexible models are clearly not appropriate for a dataset with many sparse time points as they permit too much over-time variation.

```{r loadmodels,include=FALSE}

if(run_all) {
  
unemp1_fit <- readRDS('data/1151_12_1_fit.rds')
unemp2_fit <- readRDS('data/unemp1151_12_2_fit.rds')
unemp3_fit <- readRDS('data/unemp1151_12_3_fit.rds')

unemp1_fitm <- readRDS('data/1152_12_1_fit.rds')
unemp2_fitm <- readRDS('data/unemp1152_12_2_fit.rds')
unemp3_fitm <- readRDS('data/unemp1152_12_3_fit.rds')

unemp_gp_fit <- readRDS('data/unemp115_1_12__gp_fit.rds')
unemp_ar_fit <- readRDS("data/unemp115_1_12_1_ar_fit.rds")
unemp_rw_fit <- readRDS("data/unemp115_1_12_1_rw_fit.rds")

unemp_gp_fitm <- readRDS('data/unemp115_2_12__gp_fit.rds')
unemp_ar_fitm <- readRDS("data/unemp115_2_12_1_ar_fit.rds")
unemp_rw_fitm <- readRDS("data/unemp115_2_12_1_rw_fit.rds")

rhats1 <- id_plot_rhats(unemp1_fit) + ggtitle("Spline 2nd Degree") + labs(caption="",x="") + xlim(c(0.95, 2))

saveRDS(rhats1, "data/rhats1.rds")

rhats2 <- id_plot_rhats(unemp2_fit) + ggtitle("Spline 3rd Degree") + labs(caption="",y="",x="") + xlim(c(0.95, 2))

saveRDS(rhats2, "data/rhats2.rds")

rhats3 <- id_plot_rhats(unemp3_fit) + ggtitle("Spline 4th Degree") + labs(caption="",y="",x="") + xlim(c(0.95, 2))

saveRDS(rhats3, "data/rhats3.rds")

rhatgp <- id_plot_rhats(unemp_gp_fit) + ggtitle("Gaussian Process") + labs(caption="") + xlim(c(0.95, 2))

saveRDS(rhatgp, "data/rhatgp.rds")

rhatar <- id_plot_rhats(unemp_ar_fit) + ggtitle("AR(1)") + labs(caption="",y="") + xlim(c(0.95, 2))

saveRDS(rhatar, "data/rhatar.rds")

rhatrw <- id_plot_rhats(unemp_rw_fit) + ggtitle("Random Walk") + labs(caption="",y="") + xlim(c(0.95, 2))

saveRDS(rhatrw, "data/rhatrw.rds")

un1 <- id_plot_legis_dyn(unemp1_fit,use_ci=F,plot_text = F,person_line_alpha = 0.1) + scale_color_manual(values=c(R="red",
                              D="blue"),na.translate=F,name="") + ggtitle("Spline of 2nd Degree") + labs(y="") +
  scale_x_date(guide = guide_axis(n.dodge = 2))
  
  saveRDS(un1, "data/un1.rds")

un2 <- id_plot_legis_dyn(unemp2_fit,use_ci=F,plot_text = F,person_line_alpha = 0.1) + scale_color_manual(values=c(R="red",
                              D="blue"),na.translate=F,name="") + ggtitle("Spline of 3rd Degree") + labs(y="") +
  scale_x_date(guide = guide_axis(n.dodge = 2))

  saveRDS(un2, "data/un2.rds")

un3 <- id_plot_legis_dyn(unemp3_fit,use_ci=F,plot_text = F,person_line_alpha = 0.1) + scale_color_manual(values=c(R="red",
                              D="blue"),na.translate=F,name="") + ggtitle("Spline of 4th Degree") + labs(y="") +
  scale_x_date(guide = guide_axis(n.dodge = 2))

  saveRDS(un3, "data/un3.rds")

gp <- id_plot_legis_dyn(unemp_gp_fit,use_ci=F,plot_text = F,person_line_alpha = 0.1) + scale_color_manual(values=c(R="red",
                              D="blue"),na.translate=F,name="") + ggtitle("Gaussian Process") + labs(y="") +
  scale_x_date(guide = guide_axis(n.dodge = 2))

  saveRDS(gp, "data/gp.rds")

ar <- id_plot_legis_dyn(unemp_ar_fit,use_ci=F,plot_text = F,person_line_alpha = 0.1) + scale_color_manual(values=c(R="red",
                              D="blue"),na.translate=F,name="") + ggtitle("AR(1) Process") + labs(y="") +
  scale_x_date(guide = guide_axis(n.dodge = 2))

  saveRDS(ar, "data/ar.rds")

rw <- id_plot_legis_dyn(unemp_rw_fit,use_ci=F,plot_text = F,person_line_alpha = 0.1) + scale_color_manual(values=c(R="red",
                              D="blue"),na.translate=F,name="") + ggtitle("Random Walk") +
  scale_x_date(guide = guide_axis(n.dodge = 2)) + labs(y="")

  saveRDS(rw, "data/rw.rds")
  
un1m <- id_plot_legis_dyn(unemp1_fitm,use_ci=F,plot_text = F,person_line_alpha = 0.1) + scale_color_manual(values=c(R="red",
                              D="blue"),na.translate=F,name="") + ggtitle("Spline of 2nd Degree") + labs(y="") +
  scale_x_date(guide = guide_axis(n.dodge = 2))
  
  saveRDS(un1m, "data/un1m.rds")

un2m <- id_plot_legis_dyn(unemp2_fitm,use_ci=F,plot_text = F,person_line_alpha = 0.1) + scale_color_manual(values=c(R="red",
                              D="blue"),na.translate=F,name="") + ggtitle("Spline of 3rd Degree") + labs(y="") +
  scale_x_date(guide = guide_axis(n.dodge = 2))

  saveRDS(un2m, "data/un2m.rds")

un3m <- id_plot_legis_dyn(unemp3_fitm,use_ci=F,plot_text = F,person_line_alpha = 0.1) + scale_color_manual(values=c(R="red",
                              D="blue"),na.translate=F,name="") + ggtitle("Spline of 4th Degree") + labs(y="") +
  scale_x_date(guide = guide_axis(n.dodge = 2))

  saveRDS(un3m, "data/un3m.rds")

gpm <- id_plot_legis_dyn(unemp_gp_fitm,use_ci=F,plot_text = F,person_line_alpha = 0.1) + scale_color_manual(values=c(R="red",
                              D="blue"),na.translate=F,name="") + ggtitle("Gaussian Process") + labs(y="") +
  scale_x_date(guide = guide_axis(n.dodge = 2))

  saveRDS(gpm, "data/gpm.rds")

arm <- id_plot_legis_dyn(unemp_ar_fitm,use_ci=F,plot_text = F,person_line_alpha = 0.1) + scale_color_manual(values=c(R="red",
                              D="blue"),na.translate=F,name="") + ggtitle("AR(1) Process") + labs(y="") +
  scale_x_date(guide = guide_axis(n.dodge = 2))

  saveRDS(arm, "data/arm.rds")

rwm <- id_plot_legis_dyn(unemp_rw_fitm,use_ci=F,plot_text = F,person_line_alpha = 0.1) + scale_color_manual(values=c(R="red",
                              D="blue"),na.translate=F,name="") + ggtitle("Random Walk") +
  scale_x_date(guide = guide_axis(n.dodge = 2)) + labs(y="")

  saveRDS(rwm, "data/rwm.rds")
  
un1_a <- id_plot_legis_dyn(unemp1_fit,include="AMASH, Justin",
                         use_ci=T,plot_text = F) + scale_color_manual(values=c(R="red",
                              D="blue"),na.translate=F,name="") + ggtitle("Spline of 2nd Degree") + labs(y="") +
  scale_x_date(guide = guide_axis(n.dodge = 2))
  
  saveRDS(un1_a, "data/un1_a.rds")

un2_a <- id_plot_legis_dyn(unemp2_fit,use_ci=T,include="AMASH, Justin",plot_text = F) + scale_color_manual(values=c(R="red",
                              D="blue"),na.translate=F,name="") + ggtitle("Spline of 3rd Degree") + labs(y="") +
  scale_x_date(guide = guide_axis(n.dodge = 2))
 
  saveRDS(un2_a, "data/un2_a.rds")

un3_a <- id_plot_legis_dyn(unemp3_fit,use_ci=T,include="AMASH, Justin",plot_text = F) + scale_color_manual(values=c(R="red",
                              D="blue"),na.translate=F,name="") + ggtitle("Spline of 4th Degree") + labs(y="") +
  scale_x_date(guide = guide_axis(n.dodge = 2))
  
  saveRDS(un3_a, "data/un3_a.rds")

gp_a <- id_plot_legis_dyn(unemp_gp_fit,use_ci=T,include="AMASH, Justin",plot_text = F) + scale_color_manual(values=c(R="red",
                              D="blue"),na.translate=F,name="") + ggtitle("Gaussian Process") + labs(y="") +
  scale_x_date(guide = guide_axis(n.dodge = 2))

  saveRDS(gp_a, "data/gp_a.rds")

ar_a <- id_plot_legis_dyn(unemp_ar_fit,use_ci=T,include="AMASH, Justin",plot_text = F) + scale_color_manual(values=c(R="red",
                              D="blue"),na.translate=F,name="") + ggtitle("AR(1) Process") + labs(y="") +
  scale_x_date(guide = guide_axis(n.dodge = 2))

  saveRDS(ar_a, "data/ar_a.rds")

rw_a <- id_plot_legis_dyn(unemp_rw_fit,use_ci=T,include="AMASH, Justin",plot_text = F) + scale_color_manual(values=c(R="red",
                              D="blue"),na.translate=F,name="") + ggtitle("Random Walk") +
  scale_x_date(guide = guide_axis(n.dodge = 2)) + labs(y="")

  saveRDS(rw_a, "data/rw_a.rds")
  
  un1_am <- id_plot_legis_dyn(unemp1_fitm,include="AMASH, Justin",
                         use_ci=T,plot_text = F) + scale_color_manual(values=c(R="red",
                              D="blue"),na.translate=F,name="") + ggtitle("Spline of 2nd Degree") + labs(y="") +
  scale_x_date(guide = guide_axis(n.dodge = 2))
  
  saveRDS(un1_am, "data/un1_am.rds")

un2_am <- id_plot_legis_dyn(unemp2_fitm,use_ci=T,include="AMASH, Justin",plot_text = F) + scale_color_manual(values=c(R="red",
                              D="blue"),na.translate=F,name="") + ggtitle("Spline of 3rd Degree") + labs(y="") +
  scale_x_date(guide = guide_axis(n.dodge = 2))
 
  saveRDS(un2_am, "data/un2_am.rds")

un3_am <- id_plot_legis_dyn(unemp3_fitm,use_ci=T,include="AMASH, Justin",plot_text = F) + scale_color_manual(values=c(R="red",
                              D="blue"),na.translate=F,name="") + ggtitle("Spline of 4th Degree") + labs(y="") +
  scale_x_date(guide = guide_axis(n.dodge = 2))
  
  saveRDS(un3_am, "data/un3_am.rds")

gp_am <- id_plot_legis_dyn(unemp_gp_fitm,use_ci=T,include="AMASH, Justin",plot_text = F) + scale_color_manual(values=c(R="red",
                              D="blue"),na.translate=F,name="") + ggtitle("Gaussian Process") + labs(y="") +
  scale_x_date(guide = guide_axis(n.dodge = 2))

  saveRDS(gp_am, "data/gp_am.rds")

ar_am <- id_plot_legis_dyn(unemp_ar_fitm,use_ci=T,include="AMASH, Justin",plot_text = F) + scale_color_manual(values=c(R="red",
                              D="blue"),na.translate=F,name="") + ggtitle("AR(1) Process") + labs(y="") +
  scale_x_date(guide = guide_axis(n.dodge = 2))

  saveRDS(ar_am, "data/ar_am.rds")

rw_am <- id_plot_legis_dyn(unemp_rw_fitm,use_ci=T,include="AMASH, Justin",plot_text = F) + scale_color_manual(values=c(R="red",
                              D="blue"),na.translate=F,name="") + ggtitle("Random Walk") +
  scale_x_date(guide = guide_axis(n.dodge = 2)) + labs(y="")

  saveRDS(rw_am, "data/rw_am.rds")
  
# Now do Tim Walz  
  
un1_tw <- id_plot_legis_dyn(unemp1_fit,include="WALZ, Tim",
                         use_ci=T,plot_text = F) + scale_color_manual(values=c(R="red",
                              D="blue"),na.translate=F,name="") + ggtitle("Spline of 2nd Degree") + labs(y="") +
  scale_x_date(guide = guide_axis(n.dodge = 2))
  
  saveRDS(un1_tw, "data/un1_tw.rds")

un2_tw <- id_plot_legis_dyn(unemp2_fit,use_ci=T,include="WALZ, Tim",plot_text = F) + scale_color_manual(values=c(R="red",
                              D="blue"),na.translate=F,name="") + ggtitle("Spline of 3rd Degree") + labs(y="") +
  scale_x_date(guide = guide_axis(n.dodge = 2))
 
  saveRDS(un2_tw, "data/un2_tw.rds")

un3_tw <- id_plot_legis_dyn(unemp3_fit,use_ci=T,include="WALZ, Tim",plot_text = F) + scale_color_manual(values=c(R="red",
                              D="blue"),na.translate=F,name="") + ggtitle("Spline of 4th Degree") + labs(y="") +
  scale_x_date(guide = guide_axis(n.dodge = 2))
  
  saveRDS(un3_tw, "data/un3_tw.rds")

gp_tw <- id_plot_legis_dyn(unemp_gp_fit,use_ci=T,include="WALZ, Tim",plot_text = F) + scale_color_manual(values=c(R="red",
                              D="blue"),na.translate=F,name="") + ggtitle("Gaussian Process") + labs(y="") +
  scale_x_date(guide = guide_axis(n.dodge = 2))

  saveRDS(gp_tw, "data/gp_tw.rds")

ar_tw <- id_plot_legis_dyn(unemp_ar_fit,use_ci=T,include="WALZ, Tim",plot_text = F) + scale_color_manual(values=c(R="red",
                              D="blue"),na.translate=F,name="") + ggtitle("AR(1) Process") + labs(y="") +
  scale_x_date(guide = guide_axis(n.dodge = 2))

  saveRDS(ar_tw, "data/ar_tw.rds")

rw_tw <- id_plot_legis_dyn(unemp_rw_fit,use_ci=T,include="WALZ, Tim",plot_text = F) + scale_color_manual(values=c(R="red",
                              D="blue"),na.translate=F,name="") + ggtitle("Random Walk") +
  scale_x_date(guide = guide_axis(n.dodge = 2)) + labs(y="")

  saveRDS(rw_tw, "data/rw_tw.rds")
  
# Tim Walz with missing data
  
un1_twm <- id_plot_legis_dyn(unemp1_fitm,include="WALZ, Tim",
                         use_ci=T,plot_text = F) + scale_color_manual(values=c(R="red",
                              D="blue"),na.translate=F,name="") + ggtitle("Spline of 2nd Degree") + labs(y="") +
  scale_x_date(guide = guide_axis(n.dodge = 2))
  
  saveRDS(un1_twm, "data/un1_twm.rds")

un2_twm <- id_plot_legis_dyn(unemp2_fitm,use_ci=T,include="WALZ, Tim",plot_text = F) + scale_color_manual(values=c(R="red",
                              D="blue"),na.translate=F,name="") + ggtitle("Spline of 3rd Degree") + labs(y="") +
  scale_x_date(guide = guide_axis(n.dodge = 2))
 
  saveRDS(un2_twm, "data/un2_twm.rds")

un3_twm <- id_plot_legis_dyn(unemp3_fitm,use_ci=T,include="WALZ, Tim",plot_text = F) + scale_color_manual(values=c(R="red",
                              D="blue"),na.translate=F,name="") + ggtitle("Spline of 4th Degree") + labs(y="") +
  scale_x_date(guide = guide_axis(n.dodge = 2))
  
  saveRDS(un3_twm, "data/un3_twm.rds")

gp_twm <- id_plot_legis_dyn(unemp_gp_fitm,use_ci=T,include="WALZ, Tim",plot_text = F) + scale_color_manual(values=c(R="red",
                              D="blue"),na.translate=F,name="") + ggtitle("Gaussian Process") + labs(y="") +
  scale_x_date(guide = guide_axis(n.dodge = 2))

  saveRDS(gp_twm, "data/gp_twm.rds")

ar_twm <- id_plot_legis_dyn(unemp_ar_fitm,use_ci=T,include="WALZ, Tim",plot_text = F) + scale_color_manual(values=c(R="red",
                              D="blue"),na.translate=F,name="") + ggtitle("AR(1) Process") + labs(y="") +
  scale_x_date(guide = guide_axis(n.dodge = 2))

  saveRDS(ar_twm, "data/ar_twm.rds")

rw_twm <- id_plot_legis_dyn(unemp_rw_fitm,use_ci=T,include="WALZ, Tim",plot_text = F) + scale_color_manual(values=c(R="red",
                              D="blue"),na.translate=F,name="") + ggtitle("Random Walk") +
  scale_x_date(guide = guide_axis(n.dodge = 2)) + labs(y="")

  saveRDS(rw_twm, "data/rw_twm.rds")
  
# need to export covariates, anything else we might need
  
s1_cov <- unemp1_fit@stan_samples$draws("legis_x") %>% 
  summarize_draws %>% 
  mutate(Model="Spline 2nd Degree",Missingness="Unmodeled")
s2_cov <- unemp2_fit@stan_samples$draws("legis_x") %>% 
  summarize_draws %>% 
  mutate(Model="Spline 3rd Degree",Missingness="Unmodeled")
s3_cov <- unemp3_fit@stan_samples$draws("legis_x") %>% 
  summarize_draws %>% 
  mutate(Model="Spline 4th Degree",Missingness="Unmodeled")
sgp_cov <- unemp_gp_fit@stan_samples$draws("legis_x") %>% 
  summarize_draws %>% 
  mutate(Model="Gaussian Process",Missingness="Unmodeled")
sar_cov <- unemp_ar_fit@stan_samples$draws("legis_x") %>% 
  summarize_draws %>% 
  mutate(Model="AR(1)",Missingness="Unmodeled")
srw_cov <- unemp_rw_fit@stan_samples$draws("legis_x") %>% 
  summarize_draws %>% 
  mutate(Model="Random Walk",Missingness="Unmodeled")

s1_covm <- unemp1_fitm@stan_samples$draws("legis_x") %>% 
  summarize_draws %>% 
  mutate(Model="Spline 2nd Degree",Missingness="Modeled")
s2_covm <- unemp2_fitm@stan_samples$draws("legis_x") %>% 
  summarize_draws %>% 
  mutate(Model="Spline 3rd Degree",Missingness="Modeled")
s3_covm <- unemp3_fitm@stan_samples$draws("legis_x") %>% 
  summarize_draws %>% 
  mutate(Model="Spline 4th Degree",Missingness="Modeled")
sgp_covm <- unemp_gp_fitm@stan_samples$draws("legis_x") %>% 
  summarize_draws %>% 
  mutate(Model="Gaussian Process",Missingness="Modeled")
sar_covm <- unemp_ar_fitm@stan_samples$draws("legis_x") %>% 
  summarize_draws %>% 
  mutate(Model="AR(1)",Missingness="Modeled")
srw_covm <- unemp_rw_fitm@stan_samples$draws("legis_x") %>% 
  summarize_draws %>% 
  mutate(Model="Random Walk",Missingness="Modeled")

cov_combined <- bind_rows(s1_cov, s2_cov, s3_cov,
                          sgp_cov, sar_cov, srw_cov,
                          s1_covm, s2_covm, s3_covm,
                          sgp_covm, sar_covm, srw_covm)

saveRDS(cov_combined, "data/cov_combined.rds")

rm(list=ls()[grepl(x=ls(),pattern = "unemp")])
  
  
}

```

I next examine the overall distributions of the ideal point scores over time for each model in @fig-alldist when employing only observed votes (i.e., ignoring absences and abstentions). To help with plotting these distributions, uncertainty intervals are removed and instead I plot the average of posterior draws for each legislator ideal point $\alpha_i$ separately by party. As can be seen, the different methods have differing interpretations of ideal point trajectories, although all models in general show relative stability over the monthly periods of the 115th Congress. The main difference is that the more complex models, including the AR(1), random walk, and GP, show much higher levels of over-time variation for some legislators. Based on the short time period of the 115th Congress, and the fact that we are attempting to study legislator ideology, we should be skeptical of these rapid trajectories as reflecting either noise in the data or other aspects of legislative behavior (such as the scheduling of votes) that are polluting our estimate of the latent trait. The spline-based models, by contrast, show change over time for some legislators but change is relatively smooth, i.e., ideology does not change rapidly from to rollcall to rollcall as we would expect.

```{r dists}
#| label: fig-alldist
#| fig-cap: "Comparison of Time-varying Ideal Point Methods for the 115th Congress with Observed Votes"
#| message: false
#| warning: false
#| fig-height: 6
#| fig-width: 6

  un1 <- readRDS("data/un1.rds")
  un2 <- readRDS("data/un2.rds")
  un3 <- readRDS("data/un3.rds")
  rw <- readRDS("data/rw.rds")
  ar <- readRDS("data/ar.rds")
  gp <- readRDS("data/gp.rds")

un1 + un2 + un3 + ar + gp + rw + plot_layout(nrow=2,
                                             guides="collect") + 
  plot_annotation(caption="Posterior average values shown for 115th Congress legislator ideal points.\nItem discrimination for Republican party-line votes was constrained to be positive.") & theme(legend.position = "top",
                                                                                                                                                                                                    plot.title=element_text(size=9))


```

```{r distsamash1}
#| label: fig-amashdist
#| fig-cap: "Comparison of Justin Amash Time-varying Ideal Points for the 115th Congress with Observed Votes"
#| message: false
#| warning: false
#| fig-height: 6
#| fig-width: 6

  un1_a <- readRDS("data/un1_a.rds")
  un2_a <- readRDS("data/un2_a.rds")
  un3_a <- readRDS("data/un3_a.rds")
  gp_a <- readRDS("data/gp_a.rds")
  rw_a <- readRDS("data/rw_a.rds")
  ar_a <- readRDS("data/ar_a.rds")

un1_a + un2_a + un3_a + ar_a + gp_a + rw_a + plot_layout(nrow=2,
                                             guides="collect") + 
  plot_annotation(caption="Posterior average values shown for 115th Congress legislator ideal points.\nItem discrimination for Republican party-line votes was constrained to be positive.") & theme(legend.position = "top",
                                                                                                                                                                                                    plot.title=element_text(size=9))

```

Even with the relative stability in @fig-alldist, there are some legislators whose ideal points do change over time. In @fig-amashdist, I plot the monthly ideal point trajectory for Justin Amash, a Republican Congressman who famously became embroiled in disputes with then-President Donald Trump over foreign policy. As @fig-amashdist shows, Justin Amash shows strong movement in a positive or conservative direction in the middle of the Congressional session. This seems to correspond with his increasing disputes with the Trump White House over foreign policy in particular. Amash's criticism escalated after Trump had a closed door meeting with Russian president Vladimir Putin on July 16, 2016.[^2] Per the plots in @fig-amashdist, this event coincided with noticeable movement in a conservative direction. All of the models show this movement, although the splines show a more gradual movement, as might be expected, while the more complex time series like GPs show a more sudden jump. While there might be some reason to prefer the more complex time series models for Amash's trajectory, it is also true that the splines, despite being much simpler, are able to capture the approximate same trajectory while using fewer parameters and avoiding the implausible jumps from one month to the next.

[^2]: For more information, see \url{https://thehill.com/homenews/house/397787-gop-lawmaker-trump-went-out-of-his-way-to-appear-subordinate-at-putin-press/}.

```{r distscompwalz}
#| include: false


  un1_tw <- readRDS("data/un1_tw.rds")
  un2_tw <- readRDS("data/un2_tw.rds")
  un3_tw <- readRDS("data/un3_tw.rds")
  gp_tw <- readRDS("data/gp_tw.rds")
  rw_tw <- readRDS("data/rw_tw.rds")
  ar_tw <- readRDS("data/ar_tw.rds")



```

```{r distsamash2}
#| label: fig-amashdist1
#| fig-cap: "Comparison of Justin Amash Time-varying Ideal Points for the 115th Congress with Missing Votes"
#| message: false
#| warning: false
#| fig-height: 6
#| fig-width: 6
#| include: false

  un1_am <- readRDS("data/un1_am.rds")
  un2_am <- readRDS("data/un2_am.rds")
  un3_am <- readRDS("data/un3_am.rds")
  gp_am <- readRDS("data/gp_am.rds")
  rw_am <- readRDS("data/rw_am.rds")
  ar_am <- readRDS("data/ar_am.rds")

un1_am + un2_am + un3_am + ar_am + gp_am + rw_am + plot_layout(nrow=2,
                                             guides="collect") + 
  plot_annotation(caption="Posterior average values shown for 115th Congress legislator ideal points.\nItem discrimination for Republican party-line votes was constrained to be positive.") & theme(legend.position = "top",
                                                                                                                                                                                                    plot.title=element_text(size=9))

```

We can next turn to estimation of legislator ideal points for the 115th Congress when incorporating strategic abstention. To provide context of abstention in the U.S. House, Figures 3 and 4 provide descriptive information about absence rates in the U.S. House from 1990 to 2018 measured for each rollcall vote. The figures reveal that while absence rates are stable over time, they tend to increase towards the end of the legislative session. This within-session variation in absence rates implies that a time-varying model that incorporates this missing data mechanism—i.e. to discover whether legislators tend to show up for rollcall votes that are more or less polarized—could likewise capture new insights into the study of strategic legislative absence [@cloléry2023; @battaglini2023; @rothenberg2000].

@fig-alldist2 shows that there is justification for this belief in the 115th Congress. The spline models show interesting heterogeneity at the end of the session. In general, the more complex models show even more over-time variation, though the movements are so large as to be implausible and are another reason that simpler splines should be preferred given the sparsity of the data. There is also some evidence of increasing overlap in the distributions between parties at the end of the session which is not apparent in the observed data distribution in @fig-alldist. This could be evidence that at the end of the session there is increased opportunity for deal-making.

We can examine then-Representative Tim Walz, who in 2018 announced his run to be governor of Minnesota, as a legislator whose absences may have been strategic. I first plot in @fig-walzdist1 the monthly ideal points for Walz with only observed data and in @fig-walzdist2 the ideal points for Walz when incorporating a missing-data adjustment. In a similar manner to Amash, Walz shows a noticeable change in ideal point trajectory towards the end of the session, but in this case the movement is only apparent in the missing-data model in @fig-walzdist2. Furthermore, the models in @fig-walzdist2 show his ideal point moving in a clear negative or liberal direction. Once it became more costly for Walz to show up to vote, his pattern of voting became more similar to those who were on the left wing of the party. This behavior could correspond to Walz's need to appear more liberal in order to win the Democratic primary in the gubernatorial race, which occurred on August 15, 2018.

```{r dists2}
#| label: fig-alldist2
#| fig-cap: "Comparison of Time-varying Ideal Point Methods for the 115th Congress with Missing Votes"
#| message: false
#| warning: false
#| fig-height: 6
#| fig-width: 6

  un1m <- readRDS("data/un1m.rds")
  un2m <- readRDS("data/un2m.rds")
  un3m <- readRDS("data/un3m.rds")
  rwm <- readRDS("data/rwm.rds")
  arm <- readRDS("data/arm.rds")
  gpm <- readRDS("data/gpm.rds")

un1m + un2m + un3m + arm + gpm + rwm + plot_layout(nrow=2,
                                             guides="collect") + 
  plot_annotation(caption="Posterior average values shown for 115th Congress legislator ideal points.\nItem discrimination for Republican party-line votes was constrained to be positive.") & theme(legend.position = "top",
                                                                                                                                                                                                    plot.title=element_text(size=9))

```

```{r distswalz1}
#| label: fig-walzdist1
#| fig-cap: "Comparison of Tim Walz Time-varying Ideal Points for the 115th Congress with Observed Votes"
#| message: false
#| warning: false
#| fig-height: 6
#| fig-width: 6

  un1_tw <- readRDS("data/un1_tw.rds")
  un2_tw <- readRDS("data/un2_tw.rds")
  un3_tw <- readRDS("data/un3_tw.rds")
  gp_tw <- readRDS("data/gp_tw.rds")
  rw_tw <- readRDS("data/rw_tw.rds")
  ar_tw <- readRDS("data/ar_tw.rds")

un1_tw + un2_tw + un3_tw + ar_tw + gp_tw + rw_tw + plot_layout(nrow=2,
                                             guides="collect") + 
  plot_annotation(caption="Posterior average values shown for 115th Congress legislator ideal points.\nItem discrimination for Republican party-line votes was constrained to be positive.") & theme(legend.position = "top",
                                                                                                                                                                                                    plot.title=element_text(size=9))

```

```{r distswalz2}
#| label: fig-walzdist2
#| fig-cap: "Comparison of Tim Walz Time-varying Ideal Points for the 115th Congress with Missing Votes"
#| message: false
#| warning: false
#| fig-height: 6
#| fig-width: 6

  un1_twm <- readRDS("data/un1_twm.rds")
  un2_twm <- readRDS("data/un2_twm.rds")
  un3_twm <- readRDS("data/un3_twm.rds")
  gp_twm <- readRDS("data/gp_twm.rds")
  rw_twm <- readRDS("data/rw_twm.rds")
  ar_twm <- readRDS("data/ar_twm.rds")

un1_twm + un2_twm + un3_twm + ar_twm + gp_twm + rw_twm + plot_layout(nrow=2,
                                             guides="collect") + 
  plot_annotation(caption="Posterior average values shown for 115th Congress legislator ideal points.\nItem discrimination for Republican party-line votes was constrained to be positive.") & theme(legend.position = "top",
                                                                                                                                                                                                    plot.title=element_text(size=9))

```

I next look at the coefficients $\phi$ that relate the legislator-level covariates to the ideal points scores. These values are shown in @fig-coef for the interaction between membership in the GOP and the monthly unemployment rates. As can be seen, the constituent term for Unemployment, which is equivalent to the effect of Unemployment for Democrats, is consistently positive while the effect for the GOP X Unemployment interaction term has the opposite sign and approximately equal magnitude. Substantively, this means that Democratic legislators exhibit de-polarizing behavior when district-level unemployment rates rise because negative values of the scale indicate liberal ideology. For Republican legislators, however, the relationship is a null relationship. Because the constituent term for Unemployment and the interaction term are of equal and opposite signs, the combined effect is equal to zero for GOP legislators [@brambor2006].

Interestingly, the coefficients are much larger for the spline-based models, which shows another drawback of using time-series models that are over-parameterized. It would appear that the more complex models may have absorbed the unemployment variation, implying that the legislator ideal points changed solely due to autocorrelation rather than in reference to district-level unemployment. Again, this artifact of time series processes is why a model needs to be chosen that accords with theoretical priors about the latent trait as it can have downstream consequences for inference.

```{r legisx}
#| label: fig-coef
#| tbl-cap: Coefficients of GOP and District Unemployment Interaction by Model Type ($\phi$)
#| fig-height: 5
#| cache: true

library(tinytable)
library(dplyr)

readRDS("data/cov_combined.rds") %>% 
  bind_rows %>% 
  mutate(variable=recode(variable,
                         `legis_x[1]`="Unemployment",
                         `legis_x[2]`="GOP",
                         `legis_x[3]`="UnemploymentXGOP")) %>% 
  filter(variable %in% c("Unemployment",
                         "UnemploymentXGOP")) %>% 
  select(Model,Variable="variable",
         Missingness,
         `Posterior Median`="median",
         `5% Quantile`="q5",
         `95% Quantile`="q95") %>% 
  ggplot(aes(y=`Posterior Median`,x=Model)) +
  geom_pointrange(aes(ymin=`5% Quantile`, 
                      ymax=`95% Quantile`,
                      linetype=Missingness),
                  position=position_dodge(0.5),colour="black") +
  facet_wrap(~Variable,nrow=2) +
  geom_hline(yintercept=0,linetype=3,colour="black") +
  ggthemes::theme_clean() +
  labs(caption=stringr::str_wrap("Plot shows hierarchical covariates that predict ideal points. Estimates are organized by the type of latent time process and by whether missing data was incorporated into the model. Coefficient estimates are available as a table in the supplementary information.",width=55)) +
  coord_flip()
  # arrange(Variable,Model) %>% 
  # filter(Variable %in% c("Unemployment",
  #                        "GOP",
  #                        "UnemploymentXGOP")) %>% 
  # tt(digits=3,
  #    notes="Plot shows hierarchical legislator-levle covariates for different ideal point models, including time functions and whether or not the model accounted for missingness.") %>% 
  # style_tt(i=1:36,
  #          fontsize=.7)

```

We can next examine the ideal point marginal effects of unemployment to better interpret the association in the context of the outcome. Given that we are including an interaction, we need to consider the marginal effect of unemployment for Democrats and Republicans separately. I show item-level marginal effects in @fig-allplot from the second-degree spline model with observed data given the minimal differences across the different spline models for this covariate. Each rollcall vote in @fig-allplot is colored by its relative level of discrimination $\gamma_j$, where higher values signify more polarizing votes.

As can be seen, we do see a relationship between district-level unemployment and legislator voting in the 115th Congress--but only for Democratic legislators. For bills that are the most polarizing in the liberal (negative) direction, the probability of a Democratic legislator voting for one of these bills decreases by approximately 8% as district unemployment increases by 1%. Because item (vote) discrimination does not need to be symmetric, the relationship is slightly stronger for very conservative (positive) votes: a 1% increase in district unemployment is associated with an approximately 10% increase in the probability of Democrats voting for a very conservative bill. Again, for Republicans, there does not appear to be any association as the two coefficients for Unemployment cancel out for this subgroup, resulting in zero marginal change.

```{r loadmarg}
#| include: false

by_party <- readRDS("data/by_party.rds")

```

```{r calcmargeffs}
#| label: fig-allplot
#| fig-cap: "Item-level Ideal Point Marginal Effects of Monthly District Unemployment on Legislator Ideal Points"
#| out-width: 100%
#| fig-height: 6

c1 <-   by_party %>% 
    mutate(group_id=factor(group_id,levels=c("D","R"),
                           labels=c("Democrats", "Republicans"))) %>% 
    #filter(!(item_orig %in% c("115_1050","115_588"))) %>% 
    ggplot(aes(y=mean_est,
               x=reorder(item_id,mean_est))) +
    geom_linerange(aes(ymin=low_est,
                       ymax=high_est,
                       colour=`median`)) +
    facet_wrap(~group_id) +
    scale_colour_viridis_c(name="Discrimination") +
    coord_flip() +
    labs(y="Marginal Change in Probability of Voting",
         x="Rollcalls",
         caption="Marginal effect of unemployment on voting on a specific rollcall in the 115th Congress.\nEach rollcall uncertainty interval is colored by the average posterior discrimination of that rollcall.\nMore positive rollcalls tend to be more conservative votes,\nwhile more negative rollcalls tend to be more liberal votes.") +
    geom_hline(yintercept=0,linetype=2) +
    ggthemes::theme_tufte() +
    theme(axis.text.y=element_blank(),
          axis.ticks.y=element_blank(),
          axis.line = element_blank(),
          panel.grid=element_blank(),
          legend.position = "bottom")

ggsave("idealpt_marg_eff.png",plot=c1,width=6,height=4)

c2 <- c1 + ggdark::dark_mode() + 
  theme(axis.text.y=element_blank(),
          axis.ticks.y=element_blank(),
          axis.line = element_blank()) +
  labs(y="Marginal Change in Probability of Voting",
         x="Rollcalls",
         caption="Marginal effect of unemployment on voting on a specific rollcall in the 115th Congress.\nEach rollcall uncertainty interval\nis colored by the average posterior discrimination of that rollcall.\nMore positive rollcalls tend to be more conservative votes,\nwhile more negative rollcalls tend to be more liberal votes.")

ggsave("idealpt_marg_eff_dark.png",plot=c2,width=6,height=4)
  
print(c1)
```

```{r calcmargeffsp,include=FALSE, eval=FALSE}
#| label: fig-allplotp
#| fig-cap: "Comparison of Item-level Ideal Point Marginal Effects of Unemployment on Legislator Ideal Points and Party Moderation"

by_party %>% 
    ungroup %>% 
    mutate(group_id=factor(group_id,levels=c("D","R"),
                           labels=c("Democrats", "Republicans")),
           item_rank=rank(mean_est)) %>% 
    filter(!(item_orig %in% c("115_1050","115_588"))) %>% 
    ggplot(aes(y=mean_est,
               x=item_rank)) +
    geom_ribbon(aes(ymin=low_est,
                    ymax=high_est,
                    fill=group_id),alpha=0.5) +
    #facet_wrap(~group_id) +
    ggthemes::theme_tufte() + 
    scale_fill_manual(values=c(Republicans="red",
                              Democrats="blue"),name="") +
    coord_flip() +
    labs(y="Marginal Change in Probability of Voting",
         x="Rollcalls") +
    geom_hline(yintercept=0,linetype=2) +
    ggdark::dark_mode() +
    theme(axis.text.y=element_blank(),
          axis.ticks.y=element_blank()) +
    ggtitle("Marginal Effect of District Monthly Unemployment on Rollcall Votes in 115th Congress",
            subtitle="Marginal Effect of Unemployment Mediated by Legislator Ideal Point and Bill Discrimination")

```

Overall, the results of these models do show that legislative behavior changes during times of higher local unemployment. Furthermore, we can measure these movements in ideal points down to the monthly-level, permitting very precise statements about how legislative behavior changes even if we do not have a causally-identified design at present. In general, the association of a 1% change in district-level unemployment with legislator ideal points are modest, though we would expect modest effects as this 1% change represents one-month's worth of unemployment data. Aggregated over time, these associations could grow to be substantial.

## Full Trajectories of U.S. House from 1990 to 2018

In this section, I apply the spline-based models to all House sessions from 1990 to 2018. The complete distributions for all legislators for both observed-data and missing-data models are shown in @fig-fulldist. These plots show a similar stability of over-time trends comparable to the much shorter time series in @fig-alldist, which indicates that Congressional ideal points do not change very rapidly. There does not appear to be significantly more or less movement as the degree of the spline increases. There is some additional movement in the missing-data models, though not of a very extreme nature.

![Monthly Ideal Points for U.S. House Legislators from 1990 to 2018](full_dist.pdf){#fig-fulldist}

In @fig-sumbigdist I show the party-level ideal points for both Democrats and Republicans over the same time period. To derive these scores, the individual-level ideal point trajectories are first aggregated to the group level, and then averages are taken of all posterior draws. This method preserves uncertainty in the posterior expectation, although the uncertainty intervals are not shown to make the differences between models more apparent.

```{r}
#| fig-cap: Monthly Party-level Ideal Points for the U.S. House
#| label: fig-sumbigdist
#| fig-height: 5
#| fig-width: 6

# load all these distributions, plot them 

all_group_big <- lapply(list.files("data/","idealpts",full.names = T), 
                        readRDS) %>% 
  bind_rows(.id="model") %>% 
  mutate(Spline=case_match(as.character(model),
                          c("1","2")~"2°",
                          c("3","4")~"3°",
                          c("5","6")~"4°"),
         Type=case_match(model,
                          c("2","4","6")~"With Abstentions",
                          c("1","3","5")~"Only Observed\nVotes"))

# make labels 

annotations <- tibble(x=rep(c(ymd("1993-01-01"),ymd("1993-01-01")),2),
           y=rep(c(-7,7),2),
           label=rep(c("Democrats","Republicans"),2),
           Type=rep(unique(all_group_big$Type),each=2))

all_group_big %>% 
  ggplot(aes(y=mean_est,x=Time_Point)) +
  geom_line(aes(linetype=Spline,colour=Group)) +
  facet_wrap(~Type,nrow=2) + 
  ggthemes::theme_clean() +
  scale_color_manual(values=c(R="red",
                              D="blue"),na.translate=F,name="") + 
  guides(color="none",
         linetype=guide_legend(override.aes = list(color = "black"))) + 
  scale_y_continuous(labels=c("More\nLiberal","-5","0","5","More\nConservative"),breaks=c(-10,-5,0,5,10)) +
  geom_text(data=annotations,aes(label=label,x=x,y=y),colour="black") +
  labs(y="Ideal Point",x="",
       caption=stringr::str_wrap("Plot shows party-average monthly ideal points for the U.S. House from 1990 to 2018 that are aggregated from individual legislator trajectories. Ideal point is derived from a Bayesian IRT model fit with idealstan. Missing-data models account for strategic abstention while observed models only use recorded votes. Each line is an estimate from a different type of spline model in terms of degrees with knots for each presidential administration. Discrete jumps in the time series show periods when new legislators entered.")) 



```

What is clear is that this plot departs noticeably from the DW-NOMINATE scores that generally show a GOP that is moving away from Democrats since 1990 [@carroll2009]. Instead, it would appear that Democrats were moving away from Republicans after 1990. This polarization reached a peak in 2012, and then moderated until the end of 2018. The missing-data models that incorporate strategic abstention show much greater moderation, which could be due to either increasing strategic abstention or the artificial end of the time series in 2018 (or a combination of both factors). However, despite these differences, both the observed data and missing-data models show a similar trajectory. Furthermore, there are very minor differences between the different spline models in terms of degrees and the flexibility of ideal points over time.

It is worth considering the differences between these scores and those of DW-NOMINATE. DW-NOMINATE party-level ideal points are based on legislator ideal points that vary linearly and are estimated once for each Congressional session. Furthermore, these ideal point trajectories are scaled to fit within a unit hypersphere for identification purposes [@carroll2009]. As such, there is considerable stretching of the ideal points to fit within these constraints. By contrast, in the idealstan model, constraints are placed on the item (vote) parameters while the ideal points are allowed to float with a weakly informative prior. As such, there is reason to believe that the trajectories in @fig-sumbigdist are closer to the actual political trajectories of the parties.

There are obviously many interesting hypotheses that can follow from these new party-level and individual-level measures for the U.S. House, and I leave these questions for scholars to pursue as these estimates will be available for further study.

# Conclusion

In this paper, I presented a generalization of the increasingly popular ideal point model in the Bayesian IRT framework. I extended the model with new modes of missing-data, time-varying processes, joint distributions, and quantities of interest. The main contribution of this paper is an analytical tool that can extend the domain and applicability of ideal point models across the discipline as all of these models are available in a single R package, `idealstan`. Crucially, all of these methods build on each other so that missing data can be incorporated in time-varying models or in joint distributions and vice versa.

The package `idealstan` is designed to automate the sometimes arduous process of preparing data for ideal point modeling, including identifying parameters. Furthermore, the use of a single R package for all models enables researchers to compare different models on the same data with ease. Parallelization of standard Bayesian inference is also incorporated in the package to enable estimation of the ever larger data sets available to political scientists.

# References
