---
title: 'Generalized Ideal Point Models for Time-Varying and Missing-Data Inference'
author: "Robert Kubinec \\ New York University Abu Dhabi"
date: "January 5th, 2024"
output: 
  bookdown::pdf_document2:
    includes:
      in_header: preamble.tex
abstract: 'This  paper presents an item-response theory parameterization of ideal points that unifies existing approaches to ideal point models while also extending them. For time-varying inference, the model permits ideal points to vary in a random walk, in a stationary autoregressive process, or in a semi-parametric Gaussian process. For missing data, the model implements a two-stage selection adjustment to account for non-ignorable missingness. In addition, the ideal point model is extended to handle new distributions, including continuous, positive-continuous and ordinal data. To enable modeling of datasets with mixed data (discrete and continuous), I incorporate joint modeling of different distributions. Finally, I also address ways of implementing Bayesian inference with big data sets, including variational inference and within-chain MCMC parallelization.'
bibliography: "/Users/rmk7/firmbook/BibTexDatabase.bib"
citation_package: biblatex
classoption: 12pt
toc: false
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE,
                      warning=FALSE,
                      message = FALSE,
                      fig.align = 'center')
# for debugging

options(tinytex.verbose = TRUE)

require(dplyr)
require(ggplot2)
require(tidyr)
require(readr)
require(blscrapeR)
require(lubridate)
require(tigris)
require(sf)
require(areal)
require(missRanger)
require(idealstan)
require(forcats)

# set to true to run all models (will take a long time)
run_all <- F

# whether to use a subset of items for identification

test <- T

# whether to use the prior predictive distribution for all models

prior_only <- FALSE

# set to true to create congressional datasets (will take a long time)
create_data <- F

cluster <- FALSE

# load the datas

rollcalls <- readRDS('data/rollcalls.rds')

unemp1 <- rollcalls %>% 
  select(cast_code,rollnumber,congress,
         bioname,party_code,date_month,unemp_rate) %>% 
  mutate(item=paste0(congress,"_",rollnumber),
         cast_code=recode_factor(cast_code,Abstention=NA_character_),
         cast_code=as.numeric(cast_code)-1,
         bioname=factor(bioname),
         unemp_rate=100*unemp_rate,
         bioname=relevel(bioname,"DeFAZIO, Peter Anthony")) %>% 
  distinct %>% 
  filter(party_code %in% c("R","D")) %>% 
  mutate(party_code=factor(party_code))

# drop legislators who vote on fewer than 25 unanimous bills

check_bills <- group_by(unemp1,item,party_code,cast_code) %>% count %>% 
  group_by(item,party_code) %>% 
  summarize(prop=n[cast_code==1] / sum(n),
            n_vote=sum(n)) %>% 
  ungroup %>% 
  mutate(prop=ifelse(prop==.5,
                          sample(c(.49,.51),1),
                     prop),
    util_func=(1 / sqrt((.5 - prop)^2))*n_vote) %>% 
  arrange(desc(util_func))

legis_count <- group_by(unemp1, item) %>% 
  mutate(unan=all(cast_code[!is.na(cast_code)]==1) || all(cast_code[!is.na(cast_code)]==0)) %>% 
  group_by(bioname) %>% 
  summarize(n_votes_nonunam=length(unique(item[!unan])))

# check number of days in legislature

num_days <- distinct(unemp1,bioname,date_month) %>% 
  count(bioname)

```

\newpage

While ideal point models were conceived as a way to measure ideology in the U.S. Congress, this powerful measurement tool has wide applicability as a general tool for analyzing social situations. Political scientists have continued to apply the ideal point model to new datasets and empirical challenges, such as Twitter data [@barbera2015; @kubinec2018], the United Nations Security Council [@bailey2017], state capacity [@hanson2021], democracy [@coppedge2019; @pemstein2017], campaign finance data [@bonica2014] and legislative speech [@lauderdale2016]. Through these applications, it has become apparent that the ideal point model has utility for many more measurement problems in the social sciences where data rarely exactly capture the concept of interest. To encourage further usage of this model, and to help address difficult measurement problems in political science, this paper extends the existing ideal point model in terms of mixed continuous and discrete outcome distributions, time processes and missing data, while building from a Bayesian framework for inference and incorporating computational advances for the purposes of analyzing big data sets [@martin2002; @martin2011a; @barbera2015; @kropko2013; @carroll2009; @bafumi2005; @jackman2004; @imai2016a; @lewis2018; @goplerud2019; @caughey2018].

I further show in this paper that combining advances in Bayesian computation with flexible time processes in the form of splines can solve a particularly difficult problem of scaling sparse data with measurement error, such as that available from finely grained time series in social media and other digital sources. While existing applications of dynamic ideal points are generally at a relatively high level of aggregation, such as the Congressional session [@carroll2009] or the judicial session [@martin2002], social media data like Twitter/X/Bluesky can be available in minute by minute increments [@kubinec2018; @eady2024], not to mention the wealth of preference information available in TikTok, Youtube, and other sources. However, existing time-series methods for ideal points are difficult to employ with sparse data, and for that reason I employ splines as a way of estimating restricted polynomials that can separate time trends from measurement noise. To illustrate the method, I estimate monthly time-varying ideal points for Congresspeople in the House in the 115th Congress.

In addition, I show in this paper how Bayesian post-estimation [@gill2020] enables the calculation of a new quantity of interest from ideal point models: ideal point marginal effects. I define an ideal point marginal effect as the unit change in the set of items for a unit change in a covariate that is mediated by a given person's ideal point. This quantity, which is estimated separately for each item (or bill) in the data, enables us to understand how an external covariate can influence a person's decision-making while taking into account the relative latent position of the items on which they are taking positions. I illustrate this method with an analysis of the association between monthly district-level unemployment rates and voting behavior in the 115th Congress.

All the models in this paper can be fit with the R package `idealstan`, which employs the probabilistic programming language Stan to allow for a wide variety of models to be fit within one joint framework. Furthermore, the application of Hamiltonian Monte Carlo and its associated diagnostics permits very robust inference of difficult posterior geometry, ensuring that measurement inferences are stable. Combining missing-data inference, time-varying inference and inference over joint distributions provides scholars with a multi-purpose tool for tackling measurement challenges that can extend the reach and applicability of the ideal point model.

# Latent Variables as Statistical Deduction

> "There is nothing more deceptive than an obvious fact."

> -- Sherlock Holmes

The traditional presentation of applied statistics in political science, borrowing heavily from econometrics, presents models as estimating un-biased quantities given a recipe list of datasets. If an outcome is a discrete variable that can take on any number of values, then the standard advice is to use a Poisson distribution. Of course, the datasets that political scientists produce rarely fit these clean molds, which leads to numerous statistical fixes to correct for poorly-fitting models, such as zero-inflated Poisson models in the case of counts and other more generic fixes like clustered standard errors. What all of these approaches have in common is a focus on adjusting for measurement issues in the dependent variable, while the data are assumed to represent known facts.

Measurement models flip this perspective on its head. Rather than assuming that only the outcome needs a statistical distribution, a measurement model implements a statistical distribution *for the data*. Making this mental switch enables us to think of the data as only one set of observed indicators generated by a latent, unseen construct. In other words, the data are measured *with error*, where the exact errors are defined by a particular statistical distribution.

The simplest measurement model is the econometric errors-in-variables model where the data were generated by a Normal distribution with a pre-specified standard deviation [@durbin1954]. This model assumes that any noise in the data is evenly distributed across all the data points, and its only effect on the final statistical estimation is to increase the uncertainty of estimates. There are many more measurement models available, however, all of which can be combined with traditional statistical estimation techniques or employed as the primary model of interest.

It is possible to define measurement models in terms of the structure they impose on the distribution underlying the data. As mentioned previously, the errors-in-variables model may be the simplest such distribution in which relatively little is known or assumed about noise in the data. Most other measurement models assume that the data are generated by a reduced-form expression. On one end of the spectrum, in cases where there is little prior information about what underlies the data, reduction methods like factor analysis [@harman1960], principal components analysis [@hotelling1933] and correspondence analysis [@greenacre2017; @barbera2015] can compress the columns of a data matrix into an eigenvalue-based summary of the variation within the matrix. This kind of information can be thought of a generalization of a correlation matrix of the data.

On the other end of the spectrum are methods that impose a very specific structure on the data distribution. Structural equation modeling is arguably the most demanding as researchers can stipulate a very precise process through which latent variables are related to each other, permitting complex unobserved feedback networks [@ullman2012]. There are other models involving latent variable analysis in time-series that impose a very specific functional form, such as the hidden Markov model [@rabiner1989] and the state-space model [@roesser1975], in which the data were generated by an unobserved time-varying construct. The number of latent variable models with a precise structure can be almost endless as it can even include two-stage models like the Heckman selection model [@heckman1985].

The ideal point model, and its close kin, the item-response theory framework, fit somewhere in the middle of this spectrum. These approaches put some structure on the latent trait underlying the data by assuming that the data represent a social choice process. By observing the data, we can learn something about people's latent abilities, in item-response theory terms, or people's latent ideal points in the political science terminology. Formally, an ideal point model for any person $i$ and any vote $j$ is defined as the difference in utilities between the person's ideal point $\alpha_i$ and the vote's Yes position, $\beta_{jY}$ and No position, $\beta_{jN}$, such that $i$ always votes for the bill if:

$$
\sqrt{(\alpha_i - \beta_{jY})^2} > \sqrt{(\alpha_i - \beta_{jN})^2} 
$$

That is, if $\alpha_i$ is closer in Euclidean space to the Yes position than the No position. While @enelow198 originally defined this model in deterministic terms, most statistical applications add in vote-specific error terms to make the decision stochastic. The two main statistical parameterizations of this model, the DW-NOMINATE family [@poole2008] and the Bayesian IRT family [@jackman2004], differ primarily in the distribution they assign to that error term [@carroll2009]. While an argument can be made for the preference of one over the other [@carroll2013], or even to use a fully non-parametric estimator [@tahk2018], I concentrate on the IRT formulation due to its ease of implementation in a Bayesian framework and its connection to the IRT literature.

@jackman2004 showed that a certain IRT model, the 2-Pl family, was equivalent to the ideal point model if the actual Yes and No positions of the bill were left unidentified. Instead, this parameterization can estimate the ideal points $\alpha_i$ and a midpoint where the person $i$ is indifferent to voting on the bill $j$. The IRT 2-Pl model in terms of ideal points $\alpha_i$, or what are called ability parameters in the IRT literature, discrimination parameters $\gamma_j$ and difficulty parameters $\beta_j$, is as follows for a binary observed vote $Y_{ij}$:

```{=tex}
\begin{equation}
Pr(Y_{ij}=1) = \prod^{I}_{i=1} \prod^{J}_{j=1} logit^{-1}(\gamma_j \alpha_i - \beta_j)
(\#eq:basic)
\end{equation}
```
where the midpoint or line of indifference for each vote can be found by re-arranging the linear model so that

$$
\alpha_i = \frac{\gamma_j}{\beta_j}
$$ In addition to estimating the line of indifference, the bill or item parameters (in IRT speak) have their own meaning. The sign of the discrimination parameter $\gamma_j$ indicates the "polarity" of the bill/item, or whether voting yes indicates one is more liberal or conservative in the Congressional case. The absolute magnitude of the discrimination parameter indicates how well the bill/item discriminates between persons such that the latent dimension strongly predicts voting. The $\beta_j$ difficulty parameters, by contrast, have the relatively uninteresting interpretation in representing the propensity of a person to vote yes marginal or independent of the latent dimension, though their incorporation is important to allow for the residual propensity to vote Yes to vary by bill.

The crucial difference between this parameterization and the standard IRT model is that the discrimination parameters $\gamma_j$ are unconstrained. In an IRT 2-Pl model the discrimination parameters are normally constrained to be positive so that the ideal points/ability parameters $\alpha_i$ can have the interpretation of reflecting the ability of a students to answer correctly more difficult test questions. In other words, the observed indicators are always positively related to the latent trait. The ideal point model can be considered a generalization of the IRT 2-Pl model because it does not assume that this is the case and in fact can estimate the direction of items from the data. This difference, though it would seem minute, is important when analysts are uncertain of the directional relationship between the data and the latent trait, as often occurs in practice.

This flexibility does, though, create some additional complications. The always-positive constraint in a standard IRT model ensures identification in terms of the rotation of the ideal points, so abandoning that assumption requires more work to identify the model. As has become well-known, constraints must be built in to the IRT ideal point estimation in order to identify a unique rotation of the ideal points [@gelman2005]. Usually the polarity of a subset of ideal points or bills/items is constrained so that persons with high or low scores on the latent variable will map onto positive or negative values in the estimated scale. Then the model can learn the polarity of unconstrained ideal points/items given these a priori constraints. While identification is often considered a hassle, it is in fact a feature, not a bug, as the constraints permit inference on the unconstrained items.

It is important to note that there is nothing about this model that necessarily implies that the latent scale is political ideology. That is a useful interpretation to assign ideal points based on data from the U.S. Congress [@rosenthal2007], although even in that domain the interpretability of ideal points in that sense has been recently questioned [@krehbiel2014]. The ideal point model represents a social choice process where the construct by which people make choices is unobserved. As such, it *can* estimate ideology as a latent construct, but it does not need to. Interpreting what the latent variable represents is a necessary part of any application of the model. What can be said is that it always represents the aggregation of a social choice process with competing alternatives.

In this paper I employ a Bayesian interpretation of this model [@jackman2004] that I estimate using Hamiltonian Markov Chain Monte Carlo with Stan [@carpenter2017]. I am interested in posterior inference on the set of unobserved parameters $\{\alpha_i,\gamma_j,\beta_j\} \in \theta$ that is conditional on the observed data $Y_{ij}$, or $Pr(\theta|Y_{ij})$. To do so, I define independent priors on $\theta$, $Pr(\theta)$, which is multiplied by the likelihood $L(\cdot)$ that is defined by \@ref(eq:basic):

```{=tex}
\begin{equation}
Pr(\theta|Y_{ij}) \propto Pr(\theta)L(Y_{ij}|\theta)
(\#eq:Bayes)
\end{equation}
```
I use the proportional symbol $\propto$ to reflect the fact that the term in the denominator representing the probability of the data $Pr(Y_{ij})$ is normally omitted in computational Bayesian analysis as it is fixed with respect to the parameters [@gelman2013]. We can further specify the priors in terms of the distributions assigned to each parameter in $\theta$:

```{=tex}
\begin{align*}
\alpha_i &\sim N(0,1)\\
\gamma_j &\sim N(0,5)\\
\beta_j &\sim N(0,2)\\
\end{align*}
```
The ideal points are given a tight $N(0,1)$ prior to constrain the scale for identification purposes, while the discrimination and difficulty parameters have weakly informative priors on the logit scale. The effect $\phi$ of a matrix of covariates $X$ measured on the persons $i$ can also be added to this model for ideal points as shown in @gelman2005 and @jackman2004 by including them as hierarchical predictors:

$$
\alpha_i \sim N(X\phi',1)
$$

I also consider the possibility of including hierarchical covariates to predict the discrimination of the $J$ items, which has not been estimated previously in the literature though it is straightforward in a Bayesian framework:

$$
\gamma_j \sim N(X\phi',1)
$$

Given this definition of the model, I next show how this framework can incorporate missing data when not all of $Y_{ij}$ is observed, time-varying inference when $\alpha_i$ is measured at different points in time and the estimation of diverse distributions for $Y_{ij}$ whether singly or jointly.

# Missing Data

Missing data is a tricky problem to address in latent variable models because latent variables are themselves defined as missing data. For this reason, standard imputation techniques cannot be easily applied. Because ideal point models are representations of social aggregation processes, a straightforward approach of imputing data conditional on the model assumes that missingness is orthogonal to observed ideal points [@rubin2002]. In many situations, this assumption may not be realistic when there is reason to believe that social actors, such as legislators, are absent for reasons that are related to their own ideal points.

The most rigorous treatment of the problem of missing data and ideal points can be found in @rosas2015, who put forward a model in which non-ignorable missingness occurs when legislators disagree with the position of their party. Their analysis shows that missingness in ideal point models often reflects strategic behavior and it is unwise to treat it as either missing-at-random (MAR) or missing-conditionally-at-random (MCAR). In this section I define a model for non-ignorable missingness that aims for widespread applicability while taking into account the strategic nature of ideal point actors.

I extend my notation of the outcome $Y_{ij}$ by adding a subscript $r \in \{0,1\}$ for whether person $i$ chose to vote or answer item $j$ ($r=1$) or chose not to vote/answer ($r=0$). We can then add a separate selection model that first estimates $Pr(r=0)$ and deflates the likelihood $L(Y_{ijr}|\theta)$ accordingly:

```{=tex}
\begin{equation}
    L(Y_{ijr}|\alpha_i,\gamma_j,\beta_j,\nu_j,\omega_j) = 
    \prod^{I}_{i=1} \prod^{J}_{j=1}
    \begin{cases}
    \zeta(\alpha_{i}'\nu_j - \omega_j ) & \text{if } r=0, \text{ and} \\
    (1-\zeta({\alpha_{i}'\nu_j - \omega_j}))L(Y_{ijr}|\alpha_i,\gamma_j,\beta_j) & \text{if } r=1
    \end{cases}
(\#eq:inflate2)
\end{equation}
```
I let $\zeta(\cdot)$ stand for the inverse logit function in the equation above. As can be seen, the selection model for absences $\zeta(\alpha_i'\nu_j - \omega_j)$ is very similar to \@ref(eq:basic) except that I have substituted a new set of discrimination $\nu_j$ and difficulty $\omega_j$ parameters. The ideal points $\alpha_i$ enter into both the selection model and the main ideal point model $L(Y_{ijr}|\alpha_i,\gamma_j,\beta_j)$. As such, the selection model is able to inflate or deflate the ideal points by taking into account a first stage process. The inclusion of $\nu_j$ and $\omega_j$ in the first-stage selection model creates a new "missingness" space where person $i$ first chooses whether she will cast a vote, send a tweet or answer a question, as the case may be. Only if the item is close to person $i$ in this missingness space will person $i$ then also decide to participate and provide information $Y_{ij}$ that is informative of their ideal point. This model can be interpreted as a censoring model where persons may choose to self-censor their ideal points.

The general form of the selection model--which is essentially a first-stage ideal point model--allows it to pick up a range of non-ignorable missingness patterns if missingness correlates with ideal points. The model does not make a priori assumptions about exactly why ideal points may determine missingness, and as such the model cannot provide such an interpretation a posteriori without taking time to interpret what the parameters of the selection model reveal about actors' intentions. The discrimination parameter $\nu_j$ is very helpful for this purpose because it will indicate which set of items show correlation between missingness and one of the poles of the latent variable. For example, if the latent variable is constrained positive for liberal Senators, then high positive discrimination would indicate that more liberal Senators tend to be absent on a particular bill. A naive model that assumed that absences were ignorable would miss this pattern and treat these Senators as more moderate based solely on their observed voting record.

This model is also general enough to allow for the possibility that data is actually missing at random at the item level. If the discrimination parameter $\nu_j$ is zero then ideal points do not enter into the equation for missingness and $Pr(r=0)$ is equal to the item-specific intercept $\omega_j$. In the legislative context, this would be the same as estimating the probability of absence by the proportion of legislators who do not show up for particular bill. Including this parameter will also separate that item-specific random missingness from missingness patterns suspiciously associated with ideal points. For this reason, the selection model will perform at least as well as a standard imputation method where the missing ideal points are MCAR conditional on each item.

While this missingness model is readily applicable to a legislative context where legislators may not want to show up on votes depending on what the vote would reveal about their ideal points, it is also useful for other non-ignorable missingness patterns. Twitter data is an excellent example. It is well-established that estimating a latent trait like political polarization will be vastly over-stated if the tendency of Twitter users to select who they choose to follow is not taken into account [@Barbera2015]. The two-stage selection model can be used to estimate the people's propensity to retweet ideological content net of their self-selection into whom they follow on Twitter [@kubinec2018].

# Time-varying Inference

While ideal points are usually conceived of as relatively time-invariant constructs, time-varying ideal point models have been available to researchers for quite some time. The most well-known time-varying version is the DW-NOMINATE model [@rosenthal2007; @armstrong2014; @Caughey2016], which employs a flexible polynomial function of ideal points to permit them to vary semi-parametrically. The other existing approach is the random walk model of @quinn2002 in which ideal points in time $t$ are equal to ideal points in $t-1$ plus a Normally-distributed random jump. This simple model is nonetheless quite helpful at determining the probable trend in ideal points over time because it imposes virtually no structure on the time-series trends.

Both of these approaches share in common an emphasis measuring over-time trends. By contrast, in this paper I also want to consider effects of hierarchical predictors on ideal points that vary over time. Including such predictors would enable scholars to test much more precise questions about how time-varying covariates influence ideal points, especially as political scientists obtain ever-larger and more fine-grained datasets. However, existing methods cannot easily handle such covariates. The DW-NOMINATE approach is designed to produce accurate and relatively non-parametric estimates of legislator's ideal points versus inference on those trends, while the random-walk approach cannot include additional covariates without radically changing the model. Any covariate with a constant effect in a random-walk will push the time series in a constant direction over time (i.e., static drift). It is too simple of a model to separate the effect of a covariate from the time series trend.

To produce a wider range of possibilities to time-varying inference, I introduce two new time series models as alternatives to the random-walk model. I begin by defining the random walk by placing a subscript $t$ on the ideal points $\alpha_i$ and stipulating a relationship in the prior between time points:

\begin{equation}
\alpha_{it} \sim N(\delta_i+ \alpha_{it-1},\sigma_i)
(\#eq:rwc)
\end{equation} We give every person $i$ a separate over-time variance parameter $\sigma_i$ and intercept/offset $\delta_i$. It is possible to identify this model by placing a tight upper bound on the variance parameters and adding a polarity constraint in the intercepts $\delta_i$, though as I discuss later, this approach can still fail. For computational reasons, the implementation of this model in `idealstan` employs a non-centered parameterization [@betancourt2013] that reduces dependence between the variance $\sigma_i$ and the prior value of $\alpha_{it-1}$:

```{=tex}
\begin{equation}
\begin{split}
\epsilon_{it} &\sim N(0,1)\\
\sigma_i &\sim E(1)\\
\alpha_{it} &= \delta_i + \alpha_{it-1} + \sigma_i\epsilon_{it}
\end{split}
(\#eq:rwnc)
\end{equation}
```
The first new time-series model that I consider is an AR(1) or autoregressive parameterization, which can be understood as a generalization of the random-walk model. To do so, we must add a parameter to the model, $\psi_i$ that is constrained to lie in the $(-1,1)$ interval:

```{=tex}
\begin{equation}
\begin{split}
\epsilon_{it} &\sim N(0,1)\\
\sigma_i &\sim E(1)\\
\alpha_{it} &= \delta_i + \psi_i\alpha_{it-1} + \sigma_i\epsilon_{it}
\end{split}
(\#eq:ar1)
\end{equation}
```
If $\psi_i$ lies in the $(-1,1)$ interval, then the series is stationary and will always return or decay to the long-term mean of the series $\delta_i$. By comparison, the random walk model can be thought of as an AR(1) model where $\psi_i$ is fixed at 1. Importantly, the effect of a hierarchical time-varying covariate $X_t'\phi$ is now well-defined due to the addition of the decay factor $\psi_i$ in the model. A covariate $\phi$ now represents the shock of a time-varying covariate $X_t$ on the ideal point series $\alpha_t$ that will decay over time at a rate determined by $\psi_i$. We can even calculate an impulse-response function showing the over-time decay in the effect of $\phi$ as shown in the empirical example section.

In addition to incorporating covariates, the AR(1) time-series model will provide more useful estimates than the random-walk model when there is a prior reason to believe that ideal point trends are stationary over time. This assumption could hold in a setting where shifting ideal points reflect temporary "shocks" as opposed to long-term shifts, such as comparing roll call votes from a single legislative session versus an entire legislator's career. A priori it can be difficult to know which model is a more accurate description of time trends in ideal points; however, fitting both models and comparing predictions can elucidate how each model performs in a given context.

Based on this comparison of the AR(1) and the random walk model, it would seem advantageous to have a model that estimates time-series trends with relatively little structure while still allowing for time-varying covariates. To include such a model, I further define a Gaussian process ideal point model in which the ideal points $\alpha_{it}$ vary in terms of a Gaussian process with a squared-exponential kernel. A Gaussian process (GP) is chosen because it represents a very flexible framework for semi-parametric inference of either spatial or temporal autocorrelation [@rasmussen2006]. In addition, while existing models only incorporate discrete time processes where it is assumed that the time points are equally spaced, a Gaussian process is a "continuous" time process where the time points can be irregularly spaced, such as votes occurring at odd intervals throughout a year. Given the limited application of this model in political science research, let alone ideal point models specifically, I briefly review the notation for a GP before combining it with the ideal point model.

A GP is usually defined as a "distribution over functions" [@rasmussen2006, 13]. We assume that underlying an observed time series $x_t$ is an unobserved function $f(x_t)$. To estimate the most likely values of $f(x_t)$, I can assume $f(x_t)$ is multivariate Normally-distributed where the mean and covariance of this distribution are themselves functions of the mean and covariance of $f(x_t)$:

```{=tex}
\begin{equation}
f(x_t) \sim N(\mu(f(x_t)),\Sigma(f(x_t)))
(\#eq:gp1)
\end{equation}
```
Usually the $\mu(f(x_t))$ function is assumed to be 0 as it does not vary with $t$. Instead, much of the flexibility of the distribution involves specifying a function for $\Sigma(f(x_t))$, the covariance. The function most often used, and incorporated here, is the squared-exponential kernel $k_t(\cdot)$:

```{=tex}
\begin{equation}
k_t(x_t,x_{t'}) = \large \sigma^2_f  e^{\displaystyle -\frac{(x_t - x_{t'})^2}{ 2l^2}} + \sigma^2_{x_t}  
(\#eq:gpcov)
\end{equation}
```
What this function does is convert all the distances in terms of time between $x_t$ and $x_{t'}$ into a positive semi-definite covariance matrix $\Sigma$. We can then sample from this multivariate Normal with covariance $\Sigma$ to estimate a distribution over the possible time-series functions $f(x_t)$. The GP framework is so flexible that it can fit an *infinite* number of basis functions of $x_t$ [@rasmussen2006, 14]. Other semi-parametric functions, such as splines and polynomials, are special cases of the GP for certain values of the hyper parameters used in the covariance function [@rasmussen2006, 137-140]. What is even more appealing is that the GP can handle time-varying covariates $X_t\phi'$ simply by including them in place of $\mu(x_t)$. The multivariate Normal will then sample from the specified covariate matrix while averaging over the possible values of the covariates.

The GP model has another considerable advantage over the random walk and AR(1) models. Because the covariance function accepts the actual values of the time series, it does not have to use consecutive time points. In an AR(1) or random walk model, it is assumed that the time points are discrete and consecutive. For example, in a legislature it would be possible to record the actual day that a bill was voted on rather than simply assign that aggregate the bill to the month-level. In cases in which gaps in the time series are significant, the GP may produce inferences that are more realistic [@tahk2015].

What gives the GP the ability to fit so many different functions are the three hyper-parameters in \@ref(eq:gpcov). $\sigma^2_{if}$ can be referred to as the marginal standard deviation and represents the total amount of variance in $x_t$ explained by the covariance function $k(x_t,x_{t'})$. A higher value for this hyperparameter will result in more bounce in the time series. $\sigma^2_{ix_t}$, on the other hand, represents residual variance in the time series $x_t$ that the covariance function does not fully explain (which could be interpreted as additional measurement error). Finally, the length-scale parameter $l^2_i$ represents a smoothing factor controlling how much different in time are correlated together. A very low length-scale will allow the time-series to cross the origin at a much higher rate. Conversely, higher length-scales result in smoother functions. The three hyper-parameters both overlap and interact, which makes isolating the effect of any one hyper-parameter difficult.

To employ the GP as a time process for ideal points, we simply replace the observed $x_t$ with the latent ideal points $\alpha_{it}$:

```{=tex}
\begin{equation}
\alpha_i \sim N(0,k_t(\alpha_{it},\alpha_{it'}))
(\#eq:gpideal)
\end{equation}
```
This straightforward parameterization shows some of the power of Bayesian modeling. We can include virtually any time process by simply defining the time series over a set of parameters rather than a set of observed data.

These different time series models can be understood in terms of order of complexity. The random walk is the simplest as it only requires one parameter, $\sigma_i$, per person $i$. The AR(1) model is more complex as it introduces an additional parameter to control the decay rate in the series, $\psi_i$. The GP is the most complex as it has three separate parameters per person, $\sigma^2_{if}$, $l_i^2$ and $\sigma^2_{iY}$.

We can also combine these time-series processes with the model of missing data in \@ref(eq:inflate1) to permit inference on ideal points that vary over time and may have non-ignorable missingness:

```{=tex}
\begin{equation}
    L(Y_{ijtr}|\alpha_{it},\gamma_j,\beta_j,\nu_j,\omega_j) = 
    \prod^{T}_{t=1} \prod^{I}_{i=1} \prod^{J}_{j=1}
    \begin{cases}
    \zeta(\alpha_{it}'\nu_j - \omega_j ) & \text{if } r=0, \text{ and} \\
    (1-\zeta({\alpha_{it}'\nu_j - \omega_j}))L(Y_{ijtr}|\alpha_{it},\gamma_j,\beta_j) & \text{if } r=1
    \end{cases}
(\#eq:inflate1)
\end{equation}
```
To change from a random-walk to a different time-series model for $\alpha_{it}$, we can simply change the prior for $\alpha_{it}$ that we include in $Pr(\theta)$ with one of the previously-defined time processes. I do not index the item parameters by $t$ because in many applications of the model, the item parameters only occur at one point in time, such as bills in a legislature. It is possible in some situations, such as students taking a test multiple times, for item parameters to vary over time as well. Inference on these parameters' time-series trends would then involve defining time-series priors over these parameters, whether jointly with $\alpha_{it}$ or separately, which is a possible area for future research.

Identification for models with time-varying $\alpha_{it}$ is more challenging. @quinn2002 addressed this issue for random-walk models by restricting the variance $\sigma_i$ to very low values, generally less than 0.1. I employ this restriction as well for the random walk and AR(1) models, constraining $\sigma_i$ to the interval $(0,0.1)$ for the random walk model and $(0,0.5)$ for the AR(1) model, although I find that even with these very restrictive variances, unidentifiability can still manifest itself for the random-walk model. Time-varying models can have multiple possible rotations of the ideal points at multiple time points, creating many modes and oscillation from one time point to the next. As such, for the random-walk and GP models, I first fit a model with a restricted variance with variational inference, described later in this paper, and then include the differences between the maximum and minimum of two persons' time series as additional prior information. This prevents further oscillation in the ideal points while allowing for slightly more variance in the series. Indeed, fitting an approximate fit of the model first is helpful for the AR(1) model as well as it provides useful starting values for hierarchical parameters.

As a result, the AR(1) model is in fact easier to identify than the random walk. Because of the constraints on the series to return to its over-time mean, all that is usually needed for identification is a polarity constraint on two of the persons' long-term intercepts $\delta_i$. The AR(1) model can generally also permit more variance in $\sigma_i$ than the random walk model without resulting in multi-modality.

In addition to the polarity constraints, The GP model requires further constraints to have global identification. Its greatest strength, flexibility, is also its greatest weakness in an ideal point framework. To prevent multi-modality and oscillation in the series, I had to limit the residual standard deviation parameter $\sigma_{iY}$ to a very small interval of $(0,0.025)$ as the inclusion of *additional* measurement error caused oscillation in the time series. I then assigned a very high log-normal prior to the length-scale $l^2$ equal to three times the mean time point $3\bar{T}$. The marginal standard deviation $\sigma_{if}$ is constrained to the $(0,0.3)$ interval, permitting some inference on the variability of different persons' trends. These priors and constraints are conservative and are designed to permit identification in a wide array of settings. The priors and constraints can be loosened with larger datasets that also have very high levels of over-time variation. As the hyper-parameters are constant with respect to $T$, identification will generally improve with longer time series.

# Diverse Distributions

Up to this point, I have followed convention in assuming that the observed data $Y_{ij}$ is a binary outcome (Bernoulli distribution). Virtually any distribution for $Y_{ij}$ is possible, especially with the flexibility of Bayesian modeling. In this section I incorporate existing distributions used for ideal points while also introducing the reader to distributions, especially ordinal models, that have been rarely employed for this purpose but could have considerable utility. Furthermore, I implement the @kropko2013 method to combine distributions into one joint posterior distribution, permitting models to use mixed datasets, such as both discrete and continuous variables. This method is especially useful for building time-varying indices incorporating measurement indicators that could be binary, ordinal, continuous or count variables.

There are two possible ordinal models that can be implemented in the ideal point framework. The first is known as a rating-scale model in the IRT framework and is structurally similar to the ordered logit model. @imai2016 are the first to implement this model for ideal points, though within an EM algorithm that is unfortunately vulnerable to issues of perfect separation, which occur frequently in legislative data if certain vote categories, such as abstentions, do not occur very often. By contrast, a fully Bayesian approach to ordinal models permits very sparse distributions of responses across persons.

Following the notation I introduced, a rating-scale model for $k \in K$ outcomes can be modeled as follows for observed data:

$$
    L(Y_{ijtk}|\alpha_{it},\gamma_j,\beta_j) =  \prod_{i-1}^{I} \prod_{j=1}^{J} \prod_{t=1}^{T}
    \begin{cases} 
    1 -  \zeta(\gamma_j \alpha_i - \beta_j - c_1) & \text{if } K = 0 \\
    \zeta(\gamma_j \alpha_i - \beta_j - c_{k-1}) - \zeta(\gamma_j \alpha_i - \beta_j - c_{k})       & \text{if } 0 < k < K, \text{ and} \\
    \zeta(\gamma_j \alpha_i - \beta_j - c_{k-1}) - 0 & \text{if } k=K
    \end{cases}
$$

where again $\zeta(\cdot)$ represents the logit function. In this version, each ordinal category $k$ less one is assigned a cutpoint $c_k$. In ideal point terms, these cutpoints divide the ideal point space across categories. A direct appliction is to use this model to include abstentions as a middle category between yea and nay votes. In this three outcome setting, two cutting planes exist in the ideal point space such that one end of the spectrum would vote No, the opposite end of the spectrum would vote Yes, and those in between the cutpoints would vote Abstain. As the polarity of items switches, Abstain will always remain in the center of the ideal point distribution, but the Yes and No positions can flip sides. This simple but efficient model helps to model situations where abstentions are in fact quite important, such as parliamentary systems [@brauninger2016].

A different parameterization is known as the graded response model in IRT. This model allows the cutpoints $c_k$ to vary by bill. As a result, instead of including separate cutpoints, we can change the model by indexing each item-specific intercept for each category $k$:

$$
    L(Y_{ijtk}|\alpha_{it},\gamma_j,\beta_j) =  \prod_{i-1}^{I} \prod_{j=1}^{J} \prod_{t=1}^{T}
    \begin{cases} 
    1 -  \zeta(\gamma_j \alpha_i - \beta_{jk-1}) & \text{if } K = 0 \\
    \zeta(\gamma_j \alpha_i - \beta_{jk-1}) - \zeta(\gamma_j \alpha_i - \beta_{jk-1})       & \text{if } 0 < k < K, \text{ and} \\
    \zeta(\gamma_j \alpha_i - \beta_{jk-1}) - 0 & \text{if } k=K
    \end{cases}
$$

Though not employed in political science to any great extent, this alternative formulation to ordered logit permits more flexible inference on the cutpoints. Allowing cutpoints to vary by item will enable much more accurate prediction of categories with fewer numbers of observations. It also increases information about the items/bills, though of course at a cost of including $K$ additional parameters per item. My intention in including it here is not to suggest that it is a superior model to the standard rating-scale approach, but rather as an alternative that may be useful in some situations. Either model can be profitably used to model rollcall data with middle outcomes like abstentions.

I do not review the distributions for the Poisson count model as that has been widely applied in political science [@slapin2008]. However, I do specify two other distributions that have not been widely used in the ideal point literature: the Normal distribution for continuous and the log-normal distribution for positive-continuous outcomes. While @kropko2013 employed the Normal distribution in a traditional IRT framework, no one has yet used the log-Normal distribution.

For a normal distribution, the ideal point model is used to predict the mean of $Y_{ijt}$ given residual standard deviation $\sigma_N$:

$$
  L(Y_{ijt}|\alpha_{it},\gamma_j,\beta_j) =  \prod_{i-1}^{I} \prod_{j=1}^{J} \prod_{t=1}^{T} \frac{1}{ \sqrt{2\pi \sigma_N^2 }} e^{-\frac{(Y_{ijt}-(\gamma_j\alpha_{it}-\beta_j)^2}{2\sigma_N^2}}
$$

The log-normal distribution is the same distribution except that the domain of the distribution is on $log(Y_{ijt})$:

$$
  L(Y_{ijt}|\alpha_{it},\gamma_j,\beta_j) =  \prod_{i-1}^{I} \prod_{j=1}^{J} \prod_{t=1}^{T} \frac{1}{ \sqrt{2\pi \sigma_N^2 }} e^{-\frac{(log(Y_{ijt})-(\gamma_j\alpha_{it}-\beta_j)^2}{2\sigma_N^2}}
$$

The log-normal parameterization is useful for continuous variables that are only defined on positive real numbers. For variables with substantial skew, such as income, this parameterization has a more useful interpretation than the more general Normal distribution. With both of these distributions, most continuous distributions can be modeled with reasonable accuracy.

The utility of including all of these kinds of distributions, besides granting scholars more options with which data to use for ideal point estimation, is to permit modeling of different distributions in the same data. It is very common to have panel data sets and survey data with mixed data types, such as binary, ordinal, counts and continuous variables. Other than @kropko2013 who worked within a traditional IRT framework (i.e., fixed polarity of items), no one has attempted to implement this kind of joint modeling. It promises to offer scholars the ability to easily build time-varying indices such as those employed by V-DEM [@vdem2017] and the transparency index [@jrv2018]. In addition, these joint distributions incorporate missing-data inference and time-varying inference defined previously in this paper.

It is straightforward to do so using the Bayesian framework defined in this paper. I simply index $Y_{ij}$ by $m$ for the $M$ models that each relate to a distinct outcome. These outcomes could be binary, ordinal, counts, continuous or positive-continuous per the notation presented:

$$
  L(Y_{ijtm}|\alpha_{it},\gamma_j,\beta_j) =  \prod_{i-1}^{I} \prod_{j=1}^{J} \prod_{t=1}^{T} \prod_{m=1}^{M} L_m(\gamma_j\alpha_{it}-\beta_j)
$$

where $L_m$ represents the likelihood function for the $m$th distribution. While I suppressed notation for missing data for the sake of brevity, all of these models can also be estimated marginal of missing data $r$ using the notation presented earlier in addition to incorporating time-series dynamics.

# Big Data Inference

The Achilles heel of Bayesian inference is generalizing to large data sets. Markov chains, which form the core of the estimation engine, cannot be parallelized as each iteration depends on the value of the previous iteration. As such, I present two methods of scaling this model to the ever-increasing datasets that political scientists employ for analysis.

The first method is variational posterior approximation. A variational approximation converts the posterior distribution into a series of factorized distributions for each variable in the posterior [@Grimmer2011; @blei2017]. Though this method is not new, the implementation in Stan that I incorporate as well in `idealstan` is. Variational algorithmic approximations normally need to be derived separately for each model, which dramatically increases the costs of switching to this form of inference. However, Stan has implemented a form of "black box" variational inference where the gradients necessary to perform variational are calculated using an automatic differentiation library [@NIPS2015_5758]. As a result, variational inference is available for *any* of the models presented in this paper (and all future models as well).

Variational inference (VI), though, is not a perfect solution. Because the distribution of each parameter is independent, it ignores inter-parameter correlations, which lowers the uncertainty of estimates. Furthermore, the variational approximation can fail to capture a close summary of the posterior if the algorithm is terminated prematurely [@gelman2018]. This issue can be largely avoided by using a more conservative standard for convergence, and additional statistics can be measured to test for degeneracies in the variational approximation [@gelman2018]. It may even be possible to adjust posterior inference post-hoc by conducting simulation studies on particular models to see how much uncertainty is lost with the VI algorithm.

In addition to these questions about VI's ability to capture the true posterior, it is also has an upper limit on its performance. While replacing MCMC with a much simpler factorized distribution will dramatically lower processing times, for truly enormous datasets it will still slow down. As such, I also examine an emerging method for big-data inference with Bayesian datasets: parallel computation.

Traditionally, parallel computation has had a limited impact on MCMC because Markov chains depend on prior iterations of the algorithm. This dependence between iterations means that the computations cannot be parallelized across iterations. However, recently the Stan team has implemented a way to parallelize the *likelihood* computation within a Markov chain, and in particular the computationally expensive gradients necessary to perform Hamiltonian MCMC. The likelihood can be parallelized so long as parameters are conditionally independent. For a static ideal point model, this means that the likelihood can be parallelized across persons or items. For a dynamic model, parallelization can be performed across persons as ideal points depend on prior values. As this parallelization has yet to be fully released, it will be implemented in the near future as part of the R package `idealstan`, permitting truly massive computational inference of ideal point models where the upper limit is the number of computer cores available.

There is one another means of speeding up computations, and that is to transfer computation to a graphical processing unit (GPU). This method has recently been implemented successfully to speed up MCMC estimation of a static ideal point model [@lewis2018]. Stan has also recently implemented GPU optimization for matrix calculations. This feature will be implemented to speed up GP inference, as GPs require sampling from a multivariate Normal distribution. However, GPU optimization will otherwise be limited as it requires square matrix computation, and the data in `idealstan` is in a "long" form that allows for varying numbers of observations per item and person.

# Empirical Examples

To show how time-varying estimation can increase our ability to learn about ideal points, I fit a time-varying ideal point model with the most commonly used dataset in ideal point analysis: U.S. Congressional roll-call votes. The research question that I consider is that of the effect of economic disruption on political polarization in Congress. Recent research has proposed that changes in economic conditions, whether in terms of economic inequality and trade, may be affecting increasing polarization in the U.S. Congress. It has long been understood that there is an important interaction in terms of how voters perceive economic recession and their willingness to vote for parties in a polarized environment [@alesina1995].

However, we also know that the salience of policy-making varies with economic conditions that correlate with external shocks [@baker2016]. Most of the studies of trade and other shocks on Congress employ outcomes at the year level, whether the unit of analysis are states or Congressional districts. To show the power of the time-series models I employ, I analyze month-level variation in unemployment by Congressional district and its effect on party-level ideal points in the House to see to what extent localized economic shocks either polarize or de-polarize parties.

Furthermore, I interact unemployment with the level that these districts experienced adverse shocks due to exposure to Chinese trade to see whether heightened salience due to recession increases polarization among legislators in those districts even further. It has been shown that exposure to Chinese trade has important consequences for manufacturing across the United States [@autor2013], and also has implications for how legislators vote on trade-related legislation [@hall2015] and foreign policy hositility towards China [@kuk2017]. As such, exposure to trade with China is an important potential mediator of the effect of unemployment on legislative behavior.

The data I collect come from the Bureau of Labor Statistic's Local Area Unemployment Statistics program, which produces unadjusted estimates of monthly unemployment by local-level units. To aggregate these numbers to the Congressional district level, I performed a spatial join by areal interpolation (i.e., weighted by amount of overlap) between the county-level monthly series and Congressional district boundaries from @lewis2013. The resulting dataset represents the average monthly unemployment data that would be most relevant to a particular Congressperson's district. Even with this high level of disaggregation, the dataset has very low missingness (50 district-year-month observations), which I impute non-parametrically using the technique of @StekhovenBuehlmann2012.

I then merge this dataset with a decade-level measure of exposure to Chinese imports from @hall2015 who aggregated information from @autor2013 to produce district-level measures of import exposure per worker. I use this measure as a direct way of determining how strongly Chinese trade has affected a particular district. The measure only has two over-time values, unfortunately, but there is substantial variation across districts. Their measure does not have specific units as it is a weighted multiplication of workers in a district in a given industry and that industry's trade with China, although the measure is scaled to isolate Chinese dependence as opposed to other kinds of trade behavior.

```{r load_cong,include=F}

# UNCOMMENT COMPLETELY TO LOAD FROM RAW DATA
# UNCOMMENT LINES WITH "readRDS" TO RUN FROM PRE-CALCULATED DATA
# THIS SCRIPT WILL TAKE A SIGNIFICANT AMOUNT OF TIME TO RUN & REQUIRE SIGNIFICANT MEMORY

if(create_data) {
  # load county-level unemployment & other data
# countun <- read_delim('data/la_county.txt',delim="\t")
#
# countun <- mutate(countun,series_id=trimws(series_id)) %>%
#   filter(period!="M13")
# saveRDS(countun,'data/countun.rds')

# countun <- readRDS('data/countun.rds')

# # load series indicators
# id_data <- read_tsv('data/la_series.txt')
#
# county_series <- filter(id_data,measure_code %in% c("04","06"),
#                         area_type_code=="F") %>%
#   mutate(series_id=trimws(series_id))
#
# #merge in unemployment data at series level
#
# county_series <- left_join(county_series,countun, by="series_id")
#
# # need to split to create separate columns for unemp/labor force
#
# county_series <- county_series %>%
#   mutate(measure_code=recode(measure_code,`04`="unemp",
#                              `06`="labor_force"),
#          value=as.numeric(value)) %>%
#   select(-series_id,-matches('footnote'),-seasonal,-srd_code,-series_title,-area_type_code) %>%
#   spread(key = "measure_code",value="value")
#

# need FIPS codes

# bls_fips <- get_bls_county() %>%
#   select(-period,
#          -labor_force,
#          -unemployed,
#          -employed,
#          -unemployed_rate) %>%
#   distinct
#
# saveRDS(bls_fips,'data/bls_fips.rds')

# bls_fips <- readRDS('data/bls_fips.rds')
#
# # merge in FIPS codes and drop unnecessary data
#
# county_series <- select(county_series,-begin_year,-begin_period,-end_year,
#                         -end_period) %>%
#   left_join(bls_fips,by="area_code")
#
# # check unemployment rates over time
#
# county_series <- county_series %>%
#   filter(period!="M13") %>%
#   mutate(period=recode(period,M01="January",
#                            M02="February",
#                            M03="March",
#                            M04="April",
#                            M05="May",
#                            M06="June",
#                            M07="July",
#                            M08="August",
#                            M09="September",
#                            M10="October",
#                            M11="November",
#                            M12="December"),
#          date_recode=paste0(year,"-",period,"-1"),
#          date_recode=ymd(date_recode))
#
# # need state labels for FIPS codes
#
# fips_state <- tigris::fips_codes %>%
#   select(state,state_code) %>%
#   distinct
#
# county_series <- left_join(county_series,fips_state,by=c(fips_state='state_code'))

# saveRDS(county_series,'data/county_series.rds')

# county_series <- readRDS('data/county_series.rds')

# Now we want to merge with congressional district

# we need to re-project the data to a common coordinate system to
# do areal interpolation. To do so I use the Albers projection
# as it is supposed to preserve area within the continguous U.S.
# see https://gis.stackexchange.com/questions/141580/which-projection-is-best-for-mapping-the-contiguous-united-states

# albers <- "+proj=aea +lat_1=29.5 +lat_2=45.5 +lat_0=37.5 +lon_0=-96 +x_0=0 +y_0=0 +datum=NAD83 +units=m +no_defs"

# we  need districts for each apportionment (4 going back to the 1980s)
# districts2018 <- st_read('data/congress_shape/districtShapes/districts114.shp') %>% st_transform(albers)
# districts2008 <- st_read('data/congress_shape/districtShapes/districts110.shp') %>% st_transform(albers)
# districts1998 <- st_read('data/congress_shape/districtShapes/districts105.shp') %>% st_transform(albers)
# districts1988 <- st_read('data/congress_shape/districtShapes/districts100.shp') %>% st_transform(albers)
#
# dist_list <- list(d2018=districts2018,
#                   d2008=districts2008,
#                   d1998=districts1998,
#                   d1988=districts1988)
## need to add in fips codes
# fips_all <- tigris::fips_codes
# dist_list <- lapply(dist_list,function(d) {
#   left_join(d,distinct(select(fips_all,state_name,state_code)),by=c(STATENAME='state_name'))
# })
# saveRDS(dist_list,'data/dist_list.rds')

# county_space <- tigris::counties() %>%
#   st_as_sf
# saveRDS(county_space,'data/county_space.rds')


# merge in our county data
# we will do areal-weighted interpolation to aggregate to the district level
# this gets to be too big so we need to do it one state at a time
# need to do the merge three times for the four apportionments:
#   1. 2013-2023 (present)
#   2. 2003 - 2013
#   3. 1993-2003
#   4. 1983 - 1993 (last)

# ids over which to map
# we map over each value individually as areal interpolation doesn't do
# more than one time point/value at a time

# ids <- select(county_series,year,period) %>%
#         distinct

# need 4 county series to merge with distinct districts

# county1 <- filter(county_series,year>2012)
# county2 <- filter(county_series,year<=2012 & year>2002)
# county3 <- filter(county_series,year<=2002 & year>1992)
# county4 <- filter(county_series,year<=1992 & year>1982)
#
# rm(county_series)
#
# all_counties <- list(county1=county1,
#                      county2=county2,
#                      county3=county3,
#                      county4=county4)
# rm(county1,county2,county3,county4)
# # only select vars we need
#
# all_counties <- lapply(all_counties,function(c) {
#   # collapse to county
#   group_by(c,year,period,fips_state,fips_county) %>%
#     summarize(labor_force=mean(labor_force,na.rm=T),
#               unemp=mean(unemp,na.rm=T))
#   })
#
# # interpolate across districts
#
#
# county_space <- readRDS('data/county_space.rds') %>%
#               st_transform(albers) %>%
#             st_buffer(dist = 0) %>%
#   select(STATEFP,COUNTYFP,GEOID,geometry)
#
# dist_list <- readRDS('data/dist_list.rds') %>%
#   lapply(st_buffer,dist=0) %>%
#   lapply(select,DISTRICT,ID,geometry)
#
# # create intersections and save them to save space
#
# int_list <- lapply(names(dist_list),function(d) {
#   aw_intersect(source=county_space,.data=dist_list[[d]],
#                areaVar="area") %>%
#     aw_total(source = county_space, id = GEOID, areaVar = "area", totalVar = "totalArea",
#              type = "extensive", weight = "total") %>%
#     aw_weight(areaVar = "area", totalVar = "totalArea",
#             areaWeight = "areaWeight") %>%
#     saveRDS(paste0('data/',d,'_int.rds'))
# })
#
# all_ints_names <- rev(list.files(path = "data/",pattern="int.rds",full.names = T))
#
# # we can now load up one intersection at a time and average the covariates
#
# over_states <- purrr::pmap(list(all_counties,
#                                 all_ints_names,
#                                 dist_list),function(c,d1,d2) {
#
#   d1 <- readRDS(d1)
#
#   # merge in the covariates we want
#
#   d_join <- left_join(d1,c,by=c(STATEFP="fips_state",
#                                COUNTYFP="fips_county"))
#
#   d_join <- split(d_join,list(d_join$year,
#                                              d_join$period))
#
#   # re-weight covariates
#
#   out_data_labor <- lapply(d_join,
#                            function(this_join) {
#                       out_d <- aw_calculate(.data=this_join,
#                            value=labor_force,
#                            areaWeight="areaWeight") %>%
#                         aw_aggregate(target=d2,tid=ID,interVar=labor_force)
#                       cat(paste0("Now on year ",unique(this_join$year)," and month ",
#                                    unique(this_join$period)),file="output.txt",append=T)
#                       out_d$year <- unique(this_join$year)
#                       out_d$period <- unique(this_join$period)
#                       sf::st_geometry(out_d) <- NULL
#                       return(out_d)
#                            }) %>% bind_rows
#
#   out_data_unemp <- lapply(d_join,
#                            function(this_join) {
#                       out_d <- aw_calculate(.data=this_join,
#                            value=unemp,
#                            areaWeight="areaWeight") %>%
#                         aw_aggregate(target=d2,tid=ID,interVar=unemp)
#                       cat(paste0("Now on year ",unique(this_join$year)," and month ",
#                                    unique(this_join$period)),file="output.txt",append=T)
#                       out_d$year <- unique(this_join$year)
#                       out_d$period <- unique(this_join$period)
#                       sf::st_geometry(out_d) <- NULL
#                       return(out_d)
#                            }) %>% bind_rows
#
#   # merge and output
#
#   out_data_labor <- left_join(out_data_labor,
#                               out_data_unemp,
#                               by=c("DISTRICT",
#                                    "ID",
#                                    "year",
#                                    "period")) %>%
#     mutate(unemp_rate=unemp/labor_force)
#
#   return(out_data_labor)
#   }) %>% bind_rows

# final step: impute missing (only 50 missing month-year values)

# convert our year to a linear time counter so that MissRanger will use it correctly to impute

# over_imp <- over_states %>%
#   mutate(date_recode=ymd(paste0(year,"-",period,"-1")),
#          date_recode=as.numeric(date_recode),
#          row_num=1:n(),
#          ID=factor(ID),
#          DISTRICT=factor(DISTRICT)) %>%
#   select(-year,-period)
#
# over_imp <- missRanger(over_imp,pmm.k=5,num.trees=100,returnOOB = T,seed=666112,verbose=2)
#
# # re-create over_states
#
# over_states <- arrange(over_imp,row_num) %>%
#                   mutate(year=over_states$year,
#                       period=over_states$period)
#
# # check OOB (out of bag prediction error)
#
# attr(over_imp,"oob")

# add to existing data
# need to put IDs back in to over_states

#merge district covariates

# dist_state <- readRDS('data/dist_list.rds') %>%
#   lapply(function(d) st_drop_geometry(d)) %>%
#   lapply(select,ID,
#          DISTRICT,
#          state_code) %>%
#   bind_rows
#
# over_states <- left_join(over_states,
#                          dist_state,
#                          by=c('ID',
#                               'DISTRICT'))
# fips_all <- tigris::fips_codes %>%
#   select(state_code,
#          state) %>%
#   distinct
#
# over_states <- left_join(over_states,
#                          fips_all,
#                          by="state_code") %>%
#   mutate(DISTRICT=as.numeric(as.character(DISTRICT)))
#
# # remove U.S. areas without representation (islands & Puerto Rico)
#
# over_states %>%
#   filter(DISTRICT!='98') %>%
#   saveRDS('data/over_states.rds')
}



over_states <- readRDS('data/over_states.rds')

```

```{r histun, fig.cap="Histogram of District-Level Unemployment"}

out_plot <- over_states %>% 
  ggplot(aes(x=unemp_rate)) +
  geom_histogram() +
  theme(panel.grid=element_blank(),
        panel.background = element_blank()) +
  ylab("") +
  xlab("Unemployment Rates") +
  scale_x_continuous(labels=scales::percent)

print(out_plot)

```

```{r plotcount,fig.cap="Monthly Unemployment Rates by U.S. County, 1990-2018",fig.subcap="Source: Bureau of Labor Statistics",fig.width=6,fig.height=5}

county_series <- readRDS('data/county_series.rds')

county_series %>% 
  mutate(unemp_rate=unemp/labor_force) %>% 
  filter(state %in% c("AL","CA","NY","MI")) %>% 
    distinct %>% 
  ggplot(aes(y=unemp_rate,x=date_recode)) + 
    geom_line(alpha=0.2,aes(group=fips_county)) +
    theme(panel.grid=element_blank(),
          panel.background = element_blank(),
          strip.background = element_blank(),
          axis.text=element_text(face="bold"),
          strip.text = element_text(face="bold")) +
    facet_wrap(~state,scales='free_y',ncol=2) +
    xlab("") +
    ylab("Unemployment Rates") +
    scale_y_continuous(labels=scales::percent)
  
ggsave("month_unemp.png",width=5,height=4)
```

Figure \@ref(fig:histun) shows the overall distribution of district-level unemployment rates from 1990 to 2018 inclusive, with a distinct mode at 5.0%, the so-called natural rate of unemployment. Figure \@ref(fig:plotcount) breaks out the distribution of county-level unemployment rates over time by four states: Alabama, California, Michigan and New York. As can be seen, there is substantial variation both over time and across counties within a state. As such, this series is an excellent data source for showing the utility of time-varying ideal point models. We can also learn empirically from seeing how the effect of these unemployment rates affects legislators in both parties in the House. For the unemployment data itself, we can consider the following hypothesis about the effect of recession on legislators' ideal points:

> H1: When monthly unemployment rates rise, legislators in each party become less polarized.

This hypothesis expresses the idea that in times of recession legislators feel more pressure to work across the aisle to produce bipartisan legislation, which we can measure as the spread of ideal points between both parties. We can test this hypothesis by including unemployment rates as an exogenous covariate in the ideal point model while interacting it with an indicator for each legislator's party ID. The effect of the covariate then averages over the ideal points of each individual legislator for each party.

On the other hand, we might think that economic recessions would have the opposite effect. Perhaps polarization should increase as legislators are dealing with higher numbers of unemployed constituents. Moving towards the ideological extremes could be a way of heading off primary challengers who would seize the initiative to take advantage of incumbent weakness. So we also consider a competing alternative hypothesis:

> H2: When monthly unemployment rates rise, legislators in each party become more polarized.

With the Chinese import shock data, we might surmise that polarization will increase among legislators in districts where Chinese import exposure per worker is very high. This kind of polarization could arise either by increasing divergence on trade-related legislation [@hall2015] or via increased foreign policy hostility to China [@kuk2017].

> H2: When monthly unemployment rates rise in districts with a higher exposure to Chinese import competition, legislators in each party become more polarized.

This hypothesis expresses a conditional relationship that runs in the opposite direction of the first hypothesis. It would seem that, as existing literature has shown, exposure to Chinese import shocks shifts legislators' ideal points on trade policy and increases polarization, that there should be a strongly interactive relationshp between district-level unemployment and Chinese import exposure. When both are high, we would have a strong prior that legislators will polarize away from each other. This hypothesis can be tested with a three-way interaction of party ID, unemployment rates and Chinese shock exposure at the district level.

To test these hypotheses, I fit an AR(1) time-varying ideal point model to the data. Because the AR(1) model is a discrete-time model, I aggregate the date of votes to months. Each legislator's ideal point is estimated separately, along with legislator-specific autoregressive parameters ($\psi_i$), while the covariates $\phi$ average across this time-series variation.

```{r preparecong}

# UNCOMMENT TO RUN FROM SCRATCH

# rm(county_series)
# 
# # need Congress rollcall info
# 
# rollinfo <- read_csv('data/Hall_rollcalls.csv')
# 
# unam_roll <- filter(rollinfo,
#                     yea_count==0|nay_count==0)
# 
# # member votes
# 
# rollcalls <- read_csv('data/Hall_votes.csv') %>% 
#   filter(congress>100)
# 
# #remove unanmous votes
# 
# rollcalls <- anti_join(rollcalls,
#                        unam_roll,
#                        by=c("congress",
#                             "rollnumber"))
# 
# # need member info
# 
# meminfo <- read_csv('data/Hall_members.csv')
# 
# # merge member info with rollcall data
# 
# rollcalls <- left_join(select(rollcalls,-prob,-chamber),
#                           select(meminfo,
#                                  -chamber,
#                                  -occupancy,
#                                  -last_means,
#                                  -bioguide_id,
#                                  -born,
#                                  -(died:nokken_poole_dim2)),
#                           by=c('icpsr','congress'))
# 
# 
# rollcalls <- left_join(rollcalls,
#                         select(rollinfo,
#                                congress,
#                                rollnumber,
#                                date),
#                        by=c("congress",
#                             "rollnumber"))
# 
# 
# # need to recode DISTRICT in over_states from 0 to 1 (at-large is coded as 1 in rollcall data)
# 
# over_states <- mutate(over_states,DISTRICT=if_else(DISTRICT==0,DISTRICT+1,DISTRICT))
# 
# # there were a series of votes held on Jan. 1st, 2013 that really belonged to the old 2012 
# # Congress. To avoid a mismatch, I recode those votes to December 31st, 2012.
# 
# rollcalls <- mutate(rollcalls,
#                     date=if_else(date==ymd('2013-01-01'),date-1,date),
#                     year=year(date),
#                     month=month(date,label=T,abbr=F)) %>% 
#             left_join(over_states,
#                        by=c(district_code="DISTRICT",
#                             state_abbrev="state",
#                             "year",
#                             month="period"))
# 
# # check for missing
# 
# lookat <- filter(rollcalls,district_code!=0,year>1989,year<2019,is.na(unemp_rate))
# 
# # remove remaining missing data (not relevant and prior/post unemployment data is recorded)
# 
# rollcalls <- filter(rollcalls,!is.na(unemp_rate))
# 
# # not present in legislature = missing data
# 
# rollcalls <- mutate(rollcalls,
#                  cast_code=factor(cast_code,exclude=0L),
#                  cast_code=fct_collapse(cast_code,
#                                         Yes=c("1","2","3"),
#                                         Nay=c("4","5","6"),
#                                         Abstention=c("7","8","9")),
#                  cast_code=fct_relevel(cast_code,"Nay","Yes","Abstention"),
#                  party_code=factor(party_code,
#                                    labels=c("D",
#                                             "R",
#                                             "I"))) %>% 
#   distinct
# 
# rollcalls$date_month <- rollcalls$date
# day(rollcalls$date_month) <- 1

# saveRDS(rollcalls,'data/rollcalls.rds')

# remove unnecesssary objects

# rm(over_states,rollinfo,meminfo)

rollcalls <- readRDS('data/rollcalls.rds') 

# use a sample of rollcall data for now

# rollcalls <- filter(rollcalls,bioname %in% c("BARTON, Joe Linus",
#                                              "DUNCAN, John J., Jr.",
#                                              "LEVIN, Sander Martin",
#                                              "NEAL, Richard Edmund")) %>%
#   mutate(party_code=factor(party_code,levels=c("D","R")))

rm(county_series,over_states)

```

```{r runcong1}


if(test) {
  
  # only use a subset of bills from full time period
  
  # unemp1 <- filter(unemp1, (item %in% sample(unique(item),
  #                                 3000)) | item %in% c("105_919","115_1050"))
  
  # use only last congress
  
  unemp1 <- filter(unemp1, congress==115) %>% 
    filter((item %in% sample(unique(item),
                                   200)) | item %in% c("115_588","115_1050"))
  
}

# you had to have voted on at least 10 separate days

unemp1 <- anti_join(unemp1, filter(legis_count, n_votes_nonunam<25),by="bioname") %>% 
  anti_join(filter(num_days,n<10),by="bioname") %>% 
                 id_make(outcome_disc="cast_code",
                         item_id="item",
                  person_id="bioname",
                  group_id="party_code",
                  time_id = "date_month",
                  person_cov = ~unemp_rate*party_code)

# remove original data

# to fit the models, see file to_cluster.R

# spline of degree 

unemp1_fit <- readRDS('data/unempdouble_restrict1_fit.rds')
unemp2_fit <- readRDS('data/unemp2_fit.rds')
unemp3_fit <- readRDS('data/unemp3_fit.rds')

```

```{r fitgp}

unemp_gp_fit <- readRDS('data/unempdouble_restrict_gp_fit.rds')

```

```{r loadothers}

# unemp_ar_fit <- readRDS("data/unemp1_ar_fit.rds")
# unemp_rw_fit <- readRDS("data/unemp1_rw_fit.rds")

unemp_ar_fit <- readRDS("data/unempdouble_restrict1_ar_fit.rds")
unemp_rw_fit <- readRDS("data/unemp1_rw_fit.rds")

```

```{r fitchina, include=F}

rollcalls2 <- rollcalls %>% 
  mutate(decade=case_when(year<2000~1L,
                          year<2010~2L,
                          TRUE~3L)) %>% 
  filter(decade<3)


load('data/andy_hall_jop/fh_final_analysis.rdata')

x <- filter(x,!is.na(x)) %>% 
  distinct

rollcalls2 <- left_join(rollcalls2,select(x,dist,decade,x,z,state),
                       by=c(district_code="dist",
                            "decade",state_abbrev="state"))

# again, just load a fitted model from the cluster

china_fit <- readRDS('data/china_double_restrict_fit1.rds')

```

I first examine the overall distributions of the ideal point scores over time. Figure \@ref(fig:ardistconst) shows the ideal points for two Congressmen who had their ideal points constrained, Joe Linus Barton (a Republican) and Richard Edmund Neal (a Democrat). Unlike with static models, these two were selected not because they are the most ideologically extreme but rather because they served in Congress for nearly the entire sample period. As such, their long vote records help to identify the model over time. Figure \@ref(fig:ardistconst) shows very stable, though not ideologically extreme, trends for them both. Neal is known as a moderate, so it is not as surprising that his ideal point is modestly positive. Interestingly, the variance in their ideal points declines over time, perhaps suggesting gradual hardening of political positions.

```{r calcmargeffs}



```

Based on this analysis of the constraints, we can infer that the positive side of the distribution represents more conservative ideology, while the negative side is more liberal. Using over-time models makes this interpretation more nuanced, of course, as it is possible for the meaning of the poles to change over time. We can plot the full combined distribution of all Congresspeople in Figure \@ref(fig:ardistall). This figure shows a fascinating evolution of the ideal points over time. At the beginning of Barton and Neal's careers, there were substantial numbers of conservative Democrats and liberal Republicans. By the end of their careers, the two poles of more liberal Democrats and more conservative Republicans clearly emerge. We can see how this distribution rotates over time by highlighting the two long-serving Congressmen in Figure \@ref(fig:ardistallconst). During the twenty-five year period in the data, Neal went from a more liberal member of the House to a moderate as more liberal memvbers joined the party, while Barton maintained his position in the middle of the Republican distribution by moving rightward over time.

I next turn to the effect of unemployment on these distributions. One challenge with interpreting covariates of ideal point models is providing a clear explanation of the coefficients in terms of the measurement model. Because the IRT parameterization is non-linear and bi-polar, it is possible but not necesssarily useful to solely interpret the coefficients in terms of unit changes on the probability of the outcome. For this reason, I introduce a new statistic which I call *marginal ideal point effects* that average the covariate values over the model's discrimination scores for each pole of the latent scale. By doing so, we can better capture the effect of the covariate on the latent quantity rather than just on the observed data.

Because the latent scale is bi-directional, there are two sets of marginal effects that I need to calculate. For positive values of the latent scale, I average the effect of the covariates on ideal points $\phi$ across all values of positive discrimination parameters $\gamma_j>0$ for each posterior draw $s \in S$ and each discrimination parameter $j \in J$:

```{=tex}
\begin{equation}
\frac{\partial \phi}{\partial Pr(Y_{ijrt}=1|\gamma_j>0)} = \frac{ \sum_{j=1}^J \Theta(X\phi_s' \gamma_{js})-0.5}{J}
(\#eq:marg2)
\end{equation}
```
In this equation, $\Theta(\cdot)$ represents an inverse link function that maps the ideal point values back to the observed outcome, such as a logit function for the inverse logit link (binary model) and the natural logarithm for the exponential link (Poisson distribution). For binary models with the inverse logit, I include a constant $0.5$ to calculate marginal changes in probability as $Pr(0.5)$ equals 0 on the logit scale. It is important to average this quantity for each posterior draw separately as the parameters are likely correlated in the posterior. I also calculate this statistic for all values of the discrimination parameters less than zero:

```{=tex}
\begin{equation}
\frac{\partial \phi}{\partial Pr(Y_{ijrt}=1|\gamma_j<0)} = \frac{ \sum_{j=1}^J \Theta(X\phi_s' \gamma_{js})-0.5}{J}
(\#eq:marg1)
\end{equation}
```
As a result we have two sets of marginal effects that represent the effect of the covariate weighted by the model's estimate as to whether a given item, in this case a rollcall vote, belongs to either side of the dimension. For example, if we assume that the lower end of the spectrum is liberal, then the marginal effect of $\phi$ for negative discrimination parameters will be weighted by the total amount of "liberalness" in the data. If there are many highly discriminating liberal bills, then the covariate's marginal effects on liberal votes will be much higher than if there are only a few discriminating liberal bills. Another way of conceptualizing this effect is how much a covariate prompts a given person to take a position that is consistent with a given direction of the latent scale.

```{r ardistconst, fig.cap="Over-time Ideal Points for Constrained Legislators by Month, 1990-2018",eval=FALSE}

#unemp1_fit <- readRDS("data/unemp1_run1fit.rds")

# need to calculate ideal points manually b/c of hierarchical covariates
library(posterior)

time_points <- unemp1_fit@time_varying
legis_cov <- unemp1_fit@stan_samples$draws("legis_x") %>% as_draws_matrix()



id_plot_legis_dyn(unemp1_fit,use_ci=F,plot_text = T,include=c("BARTON, Joe Linus",
                                                              "NEAL, Richard Edmund")) +
  scale_color_manual(values=c(R="red",
                              D="blue"),guide="none")

ggsave("constrained_dist.png")

```

```{r ardistall, fig.cap="Over-time Ideal Points for All U.S. House Legislators by Month, 1990-2018",eval=FALSE}

dem_ids <- select(unemp1_fit@score_data@score_matrix,person_id,group_id) %>% 
  distinct %>% filter(group_id %in% c("D","R")) %>% 
  mutate(group_id=factor(group_id))

id_plot_legis_dyn(unemp1_fit,use_ci=F,plot_text = F,person_line_alpha = 0.1,
                  include=dem_ids$person_id) + scale_color_manual(values=c(R="red",
                              D="blue",I="green"),name="")

ggsave("overall_dist.png")

```

```{r ardistallconst, fig.cap="Overlay of Constrained Legislators on Unconstrained Legislators, 1990-2018",eval=FALSE}

dem_ids <- select(unemp1_fit@score_data@score_matrix,person_id,group_id) %>% 
  distinct %>% filter(group_id %in% c("D","R"))

id_plot_legis_dyn(unemp1_fit,use_ci=F,plot_text = F,
                  highlight = c("BARTON, Joe Linus","NEAL, Richard Edmund"),
                  include=dem_ids$person_id)

ggsave("overlay_dist.png")

```

```{r ar1plot,fig.cap="Effect of District-Level Unemployment Rates (AR1 Model) on Legislators' Ideal Points 1990-2018",eval=FALSE}

# load AR(1) fit and plot resulting data

id_plot_cov(unemp1_fit,label_high = "Conservative",
            label_low="Liberal",pred_outcome = "Yes",
            new_cov_names = c(`unemp_rate:party_codeR`="Republican X\nUnemployment",
                              `unemp_rate:party_codeI`="Independent X\nUnemployment",
                              unemp_rate="Unemployment",
                              party_codeR="Republican",
                              party_codeI="Independent",
                              `(Intercept)`="Intercept"),
            recalc_vals = c("Republican X\nUnemployment",
                            "Unemployment",
                            "Combined\nRepublican"),
            filter_cov = "Intercept")

ggsave("overall_eff_ar1.png")

```

```{r gp1plot,fig.cap="Effect of District-Level Unemployment Rates (GP Model) on Legislators' Ideal Points 1990-2018",eval=F,include=F}

# load AR(1) fit and plot resulting data

id_plot_cov(unemp2_fit,label_high = "Conservative",
            label_low="Liberal",pred_outcome = "Yes",
            new_cov_names = c(`unemp_rate:party_codeR`="Republican X\nUnemployment",
                              `unemp_rate:party_codeI`="Independent X\nUnemployment",
                              unemp_rate="Unemployment",
                              party_codeR="Republican",
                              party_codeI="Independent",
                              `(Intercept)`="Intercept"),
            recalc_vals = c("Republican X\nUnemployment",
                            "Unemployment",
                            "Combined\nRepublican"),
            filter_cov = "Intercept")

ggsave("overall_eff_gp1.png")

```

```{r irf_calc,include=F,eval=FALSE}
# need to calculate these in a separate chunk as they produce too much output otherwise
dem_ids <- select(unemp1_fit@score_data@score_matrix,person_id,group_id) %>% 
  distinct %>% filter(group_id=="D")
outplot_dem <- id_plot_irf(unemp1_fit,label_high = "Conservative",
            label_low="Liberal",pred_outcome = "Yes",
            recalc_vals = F,
            line_type=1,
            line_width = .4,
            line_alpha = 0.3,
            time_label= "Months Since Unemployment Rate Increase",
            line_color='black',
            include=dem_ids$person_id,
            cov_name = c("unemp_rate"),
            use_ci=F)

dem_ids <- select(unemp1_fit@score_data@score_matrix,person_id,group_id) %>% 
  distinct %>% filter(group_id=="R")

outplot_rep <- id_plot_irf(unemp1_fit,label_high = "Conservative",
            label_low="Liberal",pred_outcome = "Yes",
            recalc_vals = F,
            line_type=1,
            line_width = .4,
            line_alpha = 0.3,
            time_label= "Months Since Unemployment Rate Increase",
            line_color='black',
            include=dem_ids$person_id,
            cov_name = c("unemp_rate"),
            use_ci=F)

```

```{r chinapl,fig.cap="Interaction of Unemployment Rates and Chinese Import Exposure on Legislative Votes 1990-2010",eval=FALSE}

# load AR(1) fit and plot resulting data

id_plot_cov(china_fit1,label_high = "Conservative",
            label_low="Liberal",pred_outcome = "Yes",
            new_cov_names = c(`unemp_rate:party_codeR`="Republican X\nUnemployment",
                              `unemp_rate:party_codeI`="Independent X\nUnemployment",
                              unemp_rate="Unemployment",
                              x="Import Exposure\nPer Worker",
                              `(Intercept)`="Intercept",
                              `unemp_rate:x`="Unemployment X\nImports",
                              `unemp_rate:party_codeR:x`="Unemployment X\nRepublican\nImports",
                              `unemp_rate:party_codeI:x`="Unemployment X\nIndependent\nImports",
                              `party_codeR:x`="Republican X\nImports",
                              `party_codeI:x`="Independent X\nImports"),
            # recalc_vals = c("Republican X\nUnemployment",
            #                 "Unemployment",
            #                 "Combined\nRepublican"),
            filter_cov = "Intercept") 

ggsave("overall_eff_china.png")

```

The ideal point marginal effects for the unemployment-ideal point interaction model are shown in Figure \@ref(fig:ar1plot). There are two coefficients that capture the interaction: the first is the Combined Republican effect, which represents the interaction term for Republican times Unemployment plus its constituent term, Unemployment. The effect of unemployment for Democrats is equal to the coefficient Unemployment as Democrats are the baseline category.

On the whole, Figure \@ref(fig:ar1plot) shows that legislators in both parties tend to move to the left during economic recessions. This is likely due to pressures to increase redistributive spending during economic downturns. However, because the left-ward movement is greater for Democrats than for Republicans, hypothesis 2 has more support because on the whole Democrats end up farther to the left than Republicans. That is, even as both groups move to the left, Democrats move farther to the left, resulting in a legislature that is on the whole more polarized. However, it should be noted that this effect is slight, and is likely not a large contributing factor of well-known polarization between Democrats and Republicans in recent years.

It is possible with the ideal point marginal effects to make precise statements about how unemployment affects legislative behavior in terms of the latent dimension. The effect magnitudes in the table appear over-stated as a one-unit change would equal a 100% increase in unemployment. For a more realistic 1% increase in unemployment, the resulting propensity to vote change is much smaller. For Republicans, a 1% increase in unemployment at the district level results in approximately a 0.1% decrease in voting for conservative bills. The effect is stronger for liberal bills: for a 1% increase in unemployment, voting for liberal bills increases by 0.18%. Because the values of discrimination do not need to be identical for both sides of the latent variable, the ideal point marginal effects are also not likely to be identical for both poles.

For Democrats, a 1% increase in unemployment is associated with a 0.2% increase in the probability of voting for liberal bills while voting for conservative bills decreases by about 0.18%. While these effects are modest, it is important to note that the underlying data is at the month level, so the cumulative effects of monthly changes in unemployment over time could still be significant.

The advantage in employing these statistics is because ideal point marginal effects fully incorporate the utility of the underlying measurement model. These covariate effects average over measurement uncertainty in the ideological content of bills instead of relying on an arbitrary coding. For these reasons, these covariate effects can be given the interpretation of the marginal effect of unemployment on legislators' voting for conservative or liberal bills while taking into account our uncertainty over how liberal or conservative a particular bill in fact is.

<!-- The GP model covariates have an identical interpretation to the AR(1) covariate effects as they are calculated on the underlying ideal point model which is the same between specifications. Interestingly, Figure \@reg(fig:gp1plot) shows different effects. This semi-parametric specification shows relatively weaker effects, though still in the same direction. MORE HERE ON THE GP MODEL WHEN IT'S DONE. -->

<!-- It might appear at first that we should prefer the GP model's results as it is more "robust" than the AR(1) because it can incorporate a wide array of time-series processes. The GP model is indeed more flexible, but that flexibility comes at a cost. It is harder to give the effects in Figure @ref(fig:gp1plot) a simple interpretation in terms of over-time inference. A hierarchical covariate in a GP model is the effect of unemployment averaged over the $TxT$ covariance matrix for *each person's ideal point trends*, which in this case are party-level ideal points. As such it is difficult to characterize how the covariate interacts with these time-series trends that remain non-transparent. It may be possible to obtain some kind of summary statement of GP time trends from posterior prediction given the estimates of each person's GP parameters, but I leave further decomposing GP results as an exercise for future research.  -->

The advantage of the AR(1) model relative to the more complicated GP model is that it is fairly straightforward to interpret the coefficients. Because the ideal points in this model are stationary, any over-time covariate equals movement away from the time series' long-term average, or what is known in time-series terminology as a "shock." Furthermore, we know how each ideal point time series responds to these shocks from the AR(1) parameters $\psi_i$. A higher magnitude of these parameters means that the ideal points display higher auto-correlation, which in terms of the covariate indicates that the effect of the covariate will have a longer lasting effect over time. The sign of the autocorrelation parameter $\psi_i$ indicates in which direction the legislator moves given a time-series shock. A negative autocorrelation parameter will cause the legislator to oscillate in response to a shock, switching from more conservative to more liberal behavior between time points. A positive autocorrelation parameter means that the legislator responds to time-series shocks by slowly decaying to the long term mean without oscillating.

What these magnitudes and sign changes imply for legislative behavior is a fascinating area for future research. To illustrate how these auto-correlation processes matter, I calculate impulse-response functions for each legislators' ideal points time series with respect to the unemployment covariate. An impulse response function measures the marginal effect of a covariate on a time series over time. For a given covariate $\phi$ and an ideal point time series $\alpha_{it}$, we can define it as follows for a given time point $t$ where the shock $\phi$ occurs:

```{=tex}
\begin{equation}
\frac{\partial \alpha_{xt+1}}{\partial \phi} = \psi_i\phi
\end{equation}
```
We can then iterate over future time periods $t$ by forward-substituting $\alpha_{xt}$ with the values of $\psi_i \phi$ from the prior time period:

```{=tex}
\begin{equation}
\frac{\partial \alpha_{xt+2}}{\partial \phi} = \psi_i(\psi_i\phi)
\end{equation}
```
This recursive algorithm can be run for an infinite number of time periods as the effect will never decay truly to zero, although in general ten time periods is usually more than sufficient to capture the impulse-response function dynamics. I calculate these impulse-response dynamics for the unemployment-party ID interaction on latent score marginal effects for each legislator. Figures \@ref(fig:irfdem) and \@ref(fig:irfrep) show these impulse-response functions for Democrat and Republican legislators respectively. What is interesting to note is that oscillation rather than slow decay is the dominant time-series behavior for legislators in both parties. We also see that there are substantial levels of autocorrelation with effects persisting through the whole 10-month period over which the impulse response functions were calculated.

```{r irfdem,fig.cap="Impulse Response Functions for Unemployment on Democrats' Ideal Points",eval=FALSE}


print(outplot_dem)

ggsave("irf_dem.png")
```

```{r irfrep,fig.cap="Impulse Response Functions for Unemployment on Republicans' Ideal Points",eval=FALSE}

print(outplot_rep)

ggsave("irf_rep.png")
```

To obtain the effects in the covariate plots in Figure \@ref(fig:ar1plot), the covariate effect would need to be averaged over the time-series trends shown in these figures. In other words, while it might seem at first counter-intuitive that legislators would flip from side to side as a result of an unemployment shock, that is because the model estimates that in general these legislators tend to respond to such shocks through oscillation rather than slow decay. A positive effect from Figure \@ref(fig:ar1plot) will result in higher positive oscillations versus negative oscillations over time for these legislators. Again, this difference in time series behavior is substantively interesting, but would require further research to understand the reasons for why legislators respond to shocks in varying ways.

Overall, the results of these models do show that legislative behavior changes during times of higher local unemployment. Furthermore, we can measure these movements in ideal points down to the monthly-level, permitting very precise statements about how legislative behavior changes. In general, the effects of 1% change in district-level unemployment on legislators are modest, though we would expect modest effects as this 1% change represents one-month's worth of unemployment data. Aggregated over time, these effects could grow to be substantial.

Finally I turn to the results of the Chinese import exposure model. The ideal point marginal effects for this model are shown in Figure \@ref(fig:chinapl). Unfortunately, it is not possible to read off the interactive relationship as the main effect is a continuous by continuous by binary variable interaction. The only way to interpret this relationship is by calculating the effect of the interaction for different values of Chinese import exposure per worker and party ID. This plot for the mean posterior values is shown in Figure \@ref(fig:chinaint). There is a strongly non-linear relationship between Chinese import exposure and unemployment. At low rates of Chinese import exposure, unemployment appears to have a weak effect on legislators in either party. At higher levels of import exposure, however, there is a much stronger relationship. In this case, it would appear that the relationship is strongly de-polarizing. Democrats with in districts with high unemployment and high import exposure tend to vote more conservative, while Republicans in similar districts tend to vote more liberal than other members of their party. This suggests that Chinese import exposure is indeed a strong moderating influence on the relationship between unemployment and voting behavior. Unfortunately, it is not possible to do a straightforward nested model comparison between the two-way and three-way interaction models, though further probing of the relationship could yield further insights into why these findings seem to run in opposite directions.

```{r chinaint,fig.cap="Effect of Unemployment Conditional on Chinese Import Exposure per Worker, 1990-2010",eval=FALSE}

# need to extract the values of the covariates

legis_x <- rstan::extract(china_fit1@stan_samples,"legis_x")[[1]]

# let's separate out the different values of the covariates

unemp_rate <- legis_x[,2]
import <- legis_x[,3]
rep_unemp <- legis_x[,4]
unemp_import <- legis_x[,6]
rep_import <- legis_x[,7]
rep_import_unemp <- legis_x[,9]

# now we need make a grid of all values in the data

import_range <- seq(min(china_fit@score_data@score_matrix$x,na.rm=T),
                   max(china_fit@score_data@score_matrix$x,na.rm=T),
                   by=0.1)


# iterate over data and calculate average values of response
# conditional for discrimination of different types

china_pars <- rstan::extract(china_fit@stan_samples)

all_plot_vals_rep <- lapply(import_range, function(i) {
  # returns a data frame of all posterior draws for this particular 
  # data combination
  # iterate over discrimination vectors
  
  this_est <- lapply(1:nrow(china_pars$L_free), function(d) {
    all_discrim <- china_pars$sigma_reg_free[d,]
    pos_discrim <- all_discrim[all_discrim>0]
    neg_discrim <- all_discrim[all_discrim<0]
    pos <- tibble(y_pred_mean=mean(plogis((unemp_rate[d] + rep_unemp[d] +
                                      unemp_import[d]*i +
           rep_import_unemp[d]*i)*pos_discrim)
           - 0.5),
                     import=i,
                     type="Pr(Yes|Conservative)")
    
    neg <- tibble(y_pred_mean=mean(plogis((unemp_rate[d] + rep_unemp[d] +
                                      unemp_import[d]*i +
           rep_import_unemp[d]*i)*neg_discrim)
           - 0.5),
                     import=i,
                     type="Pr(Yes|Liberal)")
    
    combined <- bind_rows(pos,neg)
    combined$iter <- d
    combined$party <- "R"
    combined
  }) %>% bind_rows
  
  this_est
}) %>% bind_rows

all_plot_vals_dem <- lapply(import_range, function(i) {
    
  this_est <- lapply(1:nrow(china_pars$L_free), function(d) {
    all_discrim <- china_pars$sigma_reg_free[d,]
    pos_discrim <- all_discrim[all_discrim>0]
    neg_discrim <- all_discrim[all_discrim<0]
    pos <- tibble(y_pred_mean=mean(plogis((unemp_rate[d] + 
                                      unemp_import[d]*i)*pos_discrim)-0.5),
                     import=i,
                     type="Pr(Yes|Conservative)")
    
    neg <- tibble(y_pred_mean=mean(plogis((unemp_rate[d] + 
                                      unemp_import[d]*i)*neg_discrim)-0.5),
                     import=i,
                     type="Pr(Yes|Liberal)")
    
    combined <- bind_rows(pos,neg)
    combined$iter <- d
    combined$party <- "D"
    combined
  }) %>% bind_rows
  
  this_est
}) %>% bind_rows

# combine dems and reps

combined_all <- bind_rows(all_plot_vals_rep,all_plot_vals_dem)

# plot the bugger

combined_all %>%
  group_by(party,import,type) %>% 
  summarize(y_pred_mean_mean=mean(y_pred_mean),
            y_pred_low=quantile(y_pred_mean,.05),
            y_pred_high=quantile(y_pred_mean,.95)) %>% 
  ungroup %>% 
  ggplot(aes(y=y_pred_mean_mean,x=import)) +
  geom_errorbar(aes(ymin=y_pred_low,
                     ymax=y_pred_high,
                     color=party)) +
  theme(panel.grid = element_blank(),
        panel.background = element_blank(),
        strip.background = element_blank(),
        strip.text = element_text(face="bold")) +
  facet_wrap(~type,scales="free") +
  xlab("Import Exposure per Worker") +
  ylab("Marginal Effect of Unemployment") +
  scale_colour_manual(values=c("D"="blue",
                               "R"="red")) +
  scale_y_continuous(labels=scales::percent)
#   

ggsave("china_int_plot.png")

```

# Discussion

The aim of this paper was to put forward a general framework for ideal point estimation that would both integrate existing approaches while providing new areas for analysis, particularly in missing data and over-time inference. The intention is not to suggest, however, that this framework represents all that can be done with ideal points. This paper's approach is general and designed to fit a wide variety of applications, but investigating the intricacies of social situations will always require more custom modeling. The utility of this study is rather to promote the use of ideal point models as a core part of empirical research in political science, not just for legislative studies or rollcall voting data.

The reason that ideal point models have wider applicability is because social choice processes where individuals decide between competing alternatives happen very often. If these alternatives are polarizing, or represent competing poles, than an ideal point model can help locate people's positions along the axis of competition while also determine the relative weight of the items they are choosing. The ultimate payoff is that this kind of modeling will help political scientists focus on inference on the constructs they care about, which are usually unobservable, such as partisanship, ethnic identity, corruption, and political ideology. Instead of being forced into arbitrary measurement decisions over the available indicators, political scientists can maximize the information in their data by collapsing multiple indicators to a single dimension.

As a result, this paper largely sidesteps the debate about how to determine when ideal points represent political ideology. Defining a the latent construct can only be done in the context of a specific empirical application. The political ideology interpretation is most useful for American politics where legislators have considerable latitude in voting behavior, but this does not mean that ideal point representations of other polities or social situations are not as useful. So long as a social choice process is involved, ideal point models can extract useful information from noisy data.

# Conclusion

In this paper, I presented a generalization of the increasingly popular ideal point model in the Bayesian IRT framework. I extended the model with new modes of missing-data, time-varying and joint distribution inference. Rather than focus on a single empirical application, the contribution of this paper is an analytical tool that can extend the domain and applicability of ideal point models across the discipline as all of these models are available in a single R package, `idealstan`. Crucially, all of these methods build on each other so that missing data can be incorporated in time-varying models or in joint distributions and vice versa.

For missing-data inference, I implement a two-stage selection model in which social actors first decide whether to signal their ideal points by providing observed data or to censor their ideal points by choosing not to provide any observable data. This model is flexible enough to adjust ideal points whenever missing data is correlated with either pole of the ideal point spectrum, and has wider applicability than just legislative contexts.

I implemented two new time-varying models for ideal points, the AR(1) stationary model and the Gaussian process model. The AR(1) model imposes more structure on the time-series trends in ideal points, but also has more interpretable coefficients than the Gaussian process model. The Gaussian process model, on the other hand, is able to fit a wide variety of time-series trends. Both models can fit time-varying covariates to predict ideal points, permitting new forms of precise inference on what changes ideal points.

All of these models are available in an R package called `idealstan`. This package is designed to automate the sometimes arduous process of preparing data for ideal point modeling, including identifying parameters. Furthermore, the use of a single R package for all models enables researchers to compare different models on the same data with ease. Big data alternatives to standard Bayesian inference are also incorporated in the package to enable estimation of the ever larger data sets available to political scientists.

<!-- With the Chinese import shock data, we can consider the following hypothesis: -->

<!-- > H2: When monthly unemployment rates rise in districts with a higher exposure to Chinese import competition, legislators in each party become more polarized. -->

<!-- This hypothesis expresses a conditional relationship that runs in the opposite direction of the first hypothesis. It would seem that, as existing literature has shown, exposure to Chinese import shocks shifts legislators' ideal points on trade policy and increases polarization, that there should be a strongly interactive relationshp between district-level unemployment and Chinese import exposure. When both are high, we would have a strong prior that legislators will polarize away from each other. This hypothesis can be tested with a three-way interaction of party ID, unemployment rates and Chinese shock exposure at the district level. -->

<!-- Finally we can also consider the following hypothesis: -->

<!-- > H3: When monthly unemployment rates rise in districts with a higher exposure to Chinese import competition, legislators in each party become more anti-trade. -->

<!-- Instead of testing overall polarization as H2 proposes, we can see if the movement in ideal points is concentrated on bills on trade policy by adding an indicator for trade-related legislation to the ideal point model and interacting it with party ID, unemployment, and China import shock exposure. -->

# References
