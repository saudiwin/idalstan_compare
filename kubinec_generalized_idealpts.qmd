---
title: 'Generalized Ideal Point Models for Robust Measurement with Dirty Data in the Social Sciences'
author: 
  - name: "Robert Kubinec"
    affiliation: University of South Carolina
    email: rkubinec@mailbox.sc.edu
    corresponding: true
date: last-modified
execute: 
  cache: true
  echo: false
  warning: false
  error: false
date-format: long
format: 
  wordcount-pdf: default
linestretch: 1.5
abstract: 'This paper presents a measurement tool for diverse social science data that leverages advances in Bayesian computation to develop a model that can handle mixed distributions, novel time series processes, parallelization for large datasets, and a two-stage adjustment for non-ignorable missing data. Additionally, I introduce a new estimand for the effect of covariates on latent scales, ideal point marginal effects, which reveals how an external covariate affects observable indicators via the mediation of the latent variable. I apply this model to rollcall data from the U.S. House and responses from a voter-candidate survey in Japan. For surveys, the incorporation of non-ignorable missing data can significantly change estimated latent positions of respondents. For legislative data, new time series models enable the estimation of monthly changes in Congressperson ideological trajectories from 1990 to 2018. All models are implemented in the R package `idealstan`, which is built on the Stan framework for Hamiltonian Markov Chain Monte Carlo inference. The supplementary information for this article can be accessed from [this link.](https://github.com/saudiwin/idalstan_compare/blob/c012171a02a77742a1e017800d8994b1ebf4a8db/kubinec_SI.pdf)^[I thank attendees of the 2019 Political Methodology Conference for valuable feedback. I thank the New York University Abu Dhabi High Performance Computing Center, the Ookami Center at Stony Brook University, and the University of South Carolina High Performance Computing Cluster for providing computational resources. At present, the R package idealstan is available via a Github repository at \url{https://github.com/saudiwin/idealstan}. A reproducible version of this paper, kubinec_generalized_idealpts.qmd, is available at a Github repository at \url{https://github.com/saudiwin/idalstan_compare}. Complete monthly ideal point scores for U.S. legislators in the House are available at [this link](https://www.dropbox.com/scl/fo/04zptnyt1fbvax22mim2e/ADWliFrLKQUFxeEP9PiRZUI?rlkey=bpcjzmrsyzdxb4h9n2z93zmeb&st=kpd2lpmf&dl=0).] WORD COUNT: {{< words-body >}}'
bibliography: "BibTexDatabase.bib"
fig-cap-location: top
filters:
  - authors-block
---

```{r setup, include=FALSE}

require(dplyr)
require(ggplot2)
require(tidyr)
require(readr)
require(lubridate)
require(idealstan)
require(forcats)
library(patchwork)
library(posterior)
library(marginaleffects)
library(tinytable)

# set to true to create all analyses from fitted idealstan models
# requires significant time and RAM (especially the latter)
run_all <- F

# see file to_cluster.R for the fitting of the actual idealstan models

# load the datas

rollcalls <- readRDS('data/rollcalls.rds')

unemp1 <- rollcalls %>% 
  select(cast_code,rollnumber,congress,
         bioname,party_code,date_month,unemp_rate) %>% 
  mutate(item=paste0(congress,"_",rollnumber),
         cast_code=recode_factor(cast_code,Abstention=NA_character_),
         cast_code=as.numeric(cast_code)-1,
         bioname=factor(bioname),
         unemp_rate=100*unemp_rate,
         bioname=relevel(bioname,"DeFAZIO, Peter Anthony")) %>% 
  distinct %>% 
  filter(party_code %in% c("R","D")) %>% 
  mutate(party_code=factor(party_code))

# drop legislators who vote on fewer than 25 unanimous bills

check_bills <- group_by(unemp1,item,party_code,cast_code) %>% count %>% 
  group_by(item,party_code) %>% 
  summarize(prop=n[cast_code==1] / sum(n),
            n_vote=sum(n)) %>% 
  ungroup %>% 
  mutate(prop=ifelse(prop==.5,
                          sample(c(.49,.51),1),
                     prop),
    util_func=(1 / sqrt((.5 - prop)^2))*n_vote) %>% 
  arrange(desc(util_func))

legis_count <- group_by(unemp1, item) %>% 
  mutate(unan=all(cast_code[!is.na(cast_code)]==1) || all(cast_code[!is.na(cast_code)]==0)) %>% 
  group_by(bioname) %>% 
  summarize(n_votes_nonunam=length(unique(item[!unan])))

# check number of days in legislature

num_days <- distinct(unemp1,bioname,date_month) %>% 
  count(bioname)

```

\newpage

This paper takes generalizes the ideal point model from political science to address long-standing issues like non-ignorable missing data while also extending the domain of the model to cover new areas of measurement with diverse distributions and time processes. The goal of this framework is to make the ideal point model a tool for measurement of latent concepts across the social sciences--whether the origin of the data is from social media, surveys, legislatures, bureaucratic agencies, or social networks. To accomplish this task, the paper implements the ideal point model using Stan, a probabilistic programming language for Bayesian inference, that allows for a much wider range of time-series processes, data distributions, and novel ways of incorporating missing data. In addition, the R package `idealstan`, which implements the models, exploits advances in big data Bayesian computation with Stan to permit the robust estimation of latent concepts from much larger datasets than was previously possible with Markov Chain Monte Carlo inference.

While ideal point models were conceived as a way to measure ideology in the U.S. Congress, this powerful measurement tool has wide applicability as a general tool for measuring latent traits from noisy data where there is at least some prior information about the nature of the latent trait. For example, political scientists have recently applied the canonical ideal point model to new datasets and empirical challenges, such as Twitter data [@barbera2015; @kubinec2018], the United Nations Security Council [@baileyEstimatingDynamicState2017], state capacity [@hansonLeviathansLatentDimensions2021], democracy [@coppedgeMethodologyVarietiesDemocracy2019; @pemsteinDemocraticCompromiseLatent2017], campaign finance and lobbying [@bonica2014; @kimMappingPoliticalCommunities2017], and legislative speech [@lauderdaleMeasuringPoliticalPositions2016; @kimEstimatingSpatialPreferences2018]. Through these applications, it has become apparent that the ideal point model has utility for a considerable number of measurement problems in the social sciences so long as available data are imperfect proxies of the concept of interest but the concept has a meaningful theoretical definition a priori (i.e., the measurement exercise is confirmatory rather than exploratory). To encourage further usage of this model, and to help address difficult measurement problems across the social sciences, this paper takes advantage of recent advances in Bayesian computation to broaden the implementation of the standard ideal point model to address challenges related to non-ignorable missing data [@rosasNoNewsNews2015], mixed outcome distributions [@imaiFastEstimationIdeal2016], robust inference for time-varying data [@reunig2019; @caugheyDynamicEstimationLatent2015], as well as new estimands to better understand the effect of external covariates on ideal points [@jackman2004; @bafumiPracticalIssuesImplementing2005].

As such, the contribution of this model and its implementation in `idealstan` is two-fold: 1) the approach integrates advances from diverse parameterizations of the ideal point model into a single framework, and 2) long-standing issues like non-ignorable missing data and big data computation for Bayesian modeling are addressed within this same framework. As such, this parameterization of the model provides applied researchers a Swiss Army knife approach to measurement problems: given a reasonable amount of prior information about the latent trait, virtually any dataset, including those of significant size, can be estimated with robust results. The focus on Bayesian inference ensures that the latent trait is captured with model-based uncertainty; i.e., no assumptions are made about the parametric form of the sampling distribution of the latent trait. The aim of this framework is to shift heavy duty measurement models from the domain of experts with significant background designing customized models to one where applied researchers can estimate latent traits and use them in a variety of downstream analyses [@claassen2020].

This modeling approach not only integrates existing advances into a single framework but also offers novel derivations for time series, outcome distributions, and missing data adjustment. For example, I show in this paper that combining advances in Bayesian computation with flexible time processes in the form of splines can solve a particularly difficult problem of scaling sparse data with measurement error, such as that available from finely grained time series in social media and other digital sources. While existing applications of dynamic ideal points are generally at a relatively high level of aggregation, such as the Congressional session [@carrollComparingNOMINATEIDEAL2009; @shor2022] or the judicial session [@martinDynamicIdealPoint2002], social media data like Twitter/X and Bluesky can be available in minute by minute increments [@kubinecWhenGroupsFall2021; @barbera2015; @eadyNewsSharingSocial2024; @gopal2024], not to mention the wealth of revealed preference information available in TikTok [@widholmRightWingWaveTikTok2024] and YouTube [@laiEstimatingIdeologyPolitical2024]. However, existing time-series methods for ideal points are difficult to employ with sparse data, and for that reason I present splines as a way of estimating restricted polynomials that can separate time trends from measurement noise. To apply the method, I estimate monthly time-varying ideal points for Congresspeople in the House in from 1990 to 2018, which are the first such granular measures available. I also compare multiple time-series processes, including random walks, AR(1), Gaussian processes, and a variety of splines for the 115th Congress to help researchers understand what kind of time series processes are appropriate for the specific latent concepts they are trying to estimate.

I also show in this paper how Bayesian post-estimation [@gill2020] enables the calculation of a new quantity of interest from ideal point models: *ideal point marginal effects*. I define an ideal point marginal effect as the change in the outcome for a given one unit-change in a covariate that influences a person's ideal point. This quantity, which is estimated separately for each item (i.e., each bill or vote) in the data, enables us to understand how an external covariate can influence a person's decision-making while taking into account the relative latent position of the items on which they are taking positions. I illustrate this method with an analysis of the association between monthly district-level unemployment rates and monthly U.S. Congressional ideal points for the 115th House, which shows that Democrats were more likely to moderate their positions when district-level unemployment was high while Republicans showed no legislative responsiveness to unemployment.

The other specific advance is to offer a general purpose method to accomodate non-ignorable missing data in latent variable estimation. I implement a two-stage adjustment that creates a "missingness" space in which actors first choose whether to answer an item, and then conditional on that choice, provide observed responses to the item. While relatively simple, this model can capture a diverse array of missing data patterns that might bias naive ideal point estimates. I show how this problem matters both in legislatures like the U.S. Congress when legislators have a reason to be strategically absent, but even more in ideal point estimation from survey data where missingness rates can be quite high and ignorability is a strong assumption.

All the models in this paper can be fit with the R package `idealstan`, which employs the probabilistic programming language Stan to allow for a wide variety of models to be fit within one joint framework. The model can be identified through assigning informative priors to items as in @morucciMeasurementThatMatches2024, although `idealstan` also makes use of a Generalized Beta distribution for these parameters, which solves the well-known scaling issue of identifying latent traits. Furthermore, the application of Hamiltonian Markov Chain Monte Carlo (HMC) and its associated diagnostics permits very robust inference of difficult posterior geometry, ensuring that measurement inferences are stable [@CarpenterGelmanHoffmanEtAl2017]. Finally, the implementation makes use of novel computational methods to parallelize HMC gradient calculations so that much bigger models can be fit with full Bayesian inference than was previously possible. This new parameterization of ideal points that combines missing-data inference, time-varying inference and inference over joint distributions provides scholars with a multipurpose tool for tackling difficult measurement challenges in both traditional and nontraditional datasets.

# Latent Variables as Statistical Deduction

> "There is nothing more deceptive than an obvious fact."

> -- Sherlock Holmes

The traditional presentation of applied statistics in political science presents models as estimating unbiased quantities given a recipe list of datasets. Of course, the datasets that social scientists produce rarely fit these clean molds, which leads to numerous statistical fixes to correct for poorly-fitting models, such as zero-inflated Poisson models in the case of counts [@lambertZeroinflatedPoissonRegression1992] and other more generic fixes like clustered standard errors [@abadieWhenShouldYou2023]. What all of these approaches have in common is a focus on adjusting for measurement issues in the dependent variable, while the data are assumed to represent known facts.

Measurement models flip this perspective on its head. Rather than assuming that only the outcome needs a statistical distribution, a measurement model implements a statistical distribution *for the data*. Making this mental switch enables us to think of the data as only one set of observed indicators generated by a latent, unseen construct. Using a single indicator—as is often done in applied analyses—amounts to the very strong assumption that an indicator is a perfect stand-in for concept to be measured; i.e., that measurement error is zero.

In the social sciences, measurement error is almost always non-trivial. For example, voting records, which are one of the most directly observable forms of political data, are nonetheless riddled with inaccurate records and duplicates [@ansolabehereQualityVoterRegistration2010]. Measurement error is more commonly recognized with higher-level concepts like democracy that have led to ongoing scholarly debates [@littleMeasuringDemocraticBacksliding2024; @millerHowLittleMengs2024; @knutsenConceptualMeasurementIssues2024]. However, virtually all social science variables have at least some measurement error because collecting data on humans, who have agency that allows them to hide or misreport information, is never a trivial matter. 

While many scholars recognize these structural limitations, and measurement models have a long history in statistics [@flake2019], it can be relatively difficult to apply measurement models to real-world datasets, especially when the tool needs to be able to incorporate relevant theoretical priors about the latent trait. Building a theoretically-valid measurement model can be a very useful contribution to our ability to analyze concepts, such as the influential work on legislative ideology in the United States [@poole1997; @jackman2004], but deriving an appropriate model can involve extensive technical work to incorporate theoretical priors [@caugheyDynamicEstimationLatent2015]. The model I present in this paper, which I shall refer to as `idealstan` as that is the name of the associated software package, aims to contribute a general purpose measurement model that applies to diverse datasets and problems while also permitting the incorporation of theory into measurement choices. By doing so, I hope to mainstream relatively complex Bayesian measurement models that have often taken months or years to derive and permit them to be rapidly deployed to diverse phenomena.

<!--# The simplest measurement model is the econometric errors-in-variables model where the data were generated by a Normal distribution with a pre-specified standard deviation [@durbin1954]. This model assumes that any noise in the data is evenly distributed across all the data points, and its only effect on the final statistical estimation is to increase the uncertainty of estimates. There are many more measurement models available, however, all of which can be combined with traditional statistical estimation techniques or employed as the primary model of interest.  -->

<!--# It is possible to define measurement models in terms of the structure they impose on the distribution underlying the data. As mentioned previously, the errors-in-variables model may be the simplest such distribution in which relatively little is known or assumed about noise in the data. Most other measurement models assume that the data are generated by a reduced-form expression. On one end of the spectrum, in cases where there is little prior information about what underlies the data, reduction methods like factor analysis [@harman1960], principal components analysis [@hotelling1933] and correspondence analysis [@greenacre2017; @barbera2015] can compress the columns of a data matrix into an eigenvalue-based summary of the variation within the matrix. This kind of information can be thought of a generalization of a correlation matrix of the data. -->

<!--# On the other end of the spectrum are methods that impose a very specific structure on the data distribution. Structural equation modeling is arguably the most demanding as researchers can stipulate a very precise process through which latent variables are related to each other, permitting complex unobserved feedback networks [@ullman2012]. There are other models involving latent variable analysis in time-series that impose a very specific functional form, such as the hidden Markov model [@rabiner1989] and the state-space model [@roesser1975], in which the data were generated by an unobserved time-varying construct. The number of latent variable models with a precise structure can be almost endless as it can even include two-stage models like the Heckman selection model [@heckman1985]. -->

To derive the model, I begin with the cannonical formulation of the ideal point model. The reason that this measurement framework is built on the ideal point model is because this formulation fits somewhere in the middle in the spectrum of measurement models in terms of open-ended exploration, as in the well-known factor analysis and PCA models [@harman1960; @hotelling1933; @barbera2015], with the ability to incorporate some of the theoretical structure of confirmatory methods by building in prior constraints and information about the relationship between indicators and the latent trait [@ullman2012; @armstrong2014]. In other words, the ideal point model is flexible enough to handle a wide array of latent variables but also has enough structure to incorporate crucial prior information. To show why this is the case, I first define the model in its original context and then show how it can be generalized to other situations in the social sciences.

Before beginning the review notation, I first note that I will only consider the case of a 1-dimensional latent variable. The reasons for this focus is largely practical; analyzing multiple-dimensional latent variables would complicate the syntax without adding any useful new features as multi-dimensional ideal point analysis has already been extensively explored [@poole2005; @armstrong2014]. Furthermore, in social science settings, the dimensionality of latent variables is usually quite low and adding in additional dimensions can lead to overly-complex models that are difficult to interpret [@morucciModelComplexitySupervised2024], especially in the case of time-varying latent variables. In any case, all of the analyses that I present below can be straightforwardly modified to accommodate additional noncompensatory dimensions, and for that reason I leave that extension for future research.

Formally, an ideal point model for any person $i$ and any vote $j$ is defined as the difference in utilities between the person's ideal point $\alpha_i$ and the vote's Yes position, $\beta_{jY}$ and No position, $\beta_{jN}$, such that $i$ always votes for the bill if:

$$
\sqrt{(\alpha_i - \beta_{jY})^2} > \sqrt{(\alpha_i - \beta_{jN})^2} 
$$

That is, if $\alpha_i$ is closer in Euclidean space to the Yes position than the No position. While @enelow198 originally defined this model in deterministic terms, most statistical applications add in vote-specific error terms to make the decision stochastic. The two main statistical parameterizations of this model, the DW-NOMINATE family [@poole2008] and the Bayesian IRT family [@jackman2004], differ primarily in the distribution they assign to that error term [@carrollComparingNOMINATEIDEAL2009]. While an argument can be made for the preference of one over the other [@carroll2013], or even to use a fully non-parametric utility function [@tahk2018; @duck-mayr2020], I concentrate on the standard IRT formulation due to its flexibility and parsimony.

@jackman2004 showed that a certain IRT model, the 2-Pl family, was equivalent to the ideal point model if the actual Yes and No positions of the bill were left unidentified. Instead, this parameterization can estimate the ideal points $\alpha_i$ and a midpoint where the person $i$ is indifferent to voting on the bill $j$. The IRT 2-Pl model in terms of ideal points $\alpha_i$, or what are called ability parameters in the IRT literature, discrimination parameters $\gamma_j$ and difficulty parameters $\beta_j$, is as follows for a binary observed vote $Y_{ij}$:

$$
Pr(Y_{ij}=1) = \prod^{I}_{i=1} \prod^{J}_{j=1} logit^{-1}(\gamma_j \alpha_i - \beta_j)
$$ {#eq-basic}

The sign of the discrimination parameter $\gamma_j$ indicates the "polarity" of the bill/item, or whether voting yes indicates one is more liberal or conservative in the Congressional case. The absolute magnitude of the discrimination parameter indicates how well the bill (item) discriminates between persons such that the latent dimension strongly predicts voting. The $\beta_j$ difficulty parameters, by contrast, have the relatively uninteresting interpretation in representing the propensity of a person to vote yes marginal or independent of the latent dimension, though their incorporation is important to allow for the residual propensity to vote Yes to vary by bill.

<!--# The crucial difference between this parameterization and the standard IRT model is that the discrimination parameters $\gamma_j$ are unconstrained. In an IRT 2-Pl model the discrimination parameters are normally constrained to be positive so that the ideal points/ability parameters $\alpha_i$ can have the interpretation of reflecting the ability of a students to answer correctly more difficult test questions. In other words, the observed indicators are always positively related to the latent trait. The ideal point model can be considered a generalization of the IRT 2-Pl model because it does not assume that this is the case and in fact can estimate the direction of items from the data. This difference, though it would seem minute, is important when analysts are uncertain of the directional relationship between the data and the latent trait, as often occurs in practice.  -->

The ability to allow for contrasting polarity at the item level does create some additional complications. The always-positive discrimination constraint in a standard IRT model ensures identification in terms of the rotation of the ideal points, so abandoning that assumption requires more work to identify the model. As has become well-known, constraints must be built in to the IRT ideal point estimation in order to identify a unique rotation of the ideal points [@bafumiPracticalIssuesImplementing2005]. As I discuss, the best forms of identification make use of theoretical priors so that latent concepts match closely what the researcher knows about the concept they are trying to measure [@morucciMeasurementThatMatches2024]. For these reasons, identification is not a "bug" but rather a "feature" of the ideal point model as it allows the researcher to estimate a meaningful latent trait while still permitting a substantial number of parameters to be estimated freely. By contrast, imposing a positivity constraint on all items in an IRT model can be a very strong assumption that can result in misleading latent variable estimation—unless of course the particular empirical context justifies the assumption, such as in the canonical student testing application of IRT. Given that the traditional IRT has the positivity constraint as its only main difference from the ideal point formulation, all of the analyses that follow can also be estimated with that constraint in place.

It is important to note that there is nothing about this model that necessarily implies that the latent scale is political ideology. That is a useful interpretation to assign ideal points based on data from the U.S. Congress [@poole2007] or state legislatures [@shor2022]. The ideal point model represents a social choice process where the construct by which people make choices is unobserved. As such, it *can* estimate ideology as a latent construct, but it does not need to. Interpreting what the latent variable represents is a necessary part of any application of the model. What can be said is that it can flexibly represent the outcome of a variety of social processes so long as there is a unique rotation, or equivalently, sign identification of the indicators/items.

In this paper I employ a Bayesian interpretation of this model [@jackman2004] that I estimate using Hamiltonian Markov Chain Monte Carlo with Stan [@CarpenterGelmanHoffmanEtAl2017]. I am interested in posterior inference on a set of unobserved parameters $\{\alpha_i,\gamma_j,\beta_j\} \in \theta$ that is conditional on the observed data $Y_{ij}$, or $Pr(\theta|Y_{ij})$. To do so, I define independent priors on $\theta$, $Pr(\theta)$, which is multiplied by the likelihood $L(\cdot)$ that is defined by @eq-Bayes:[^1]

[^1]: I use the proportional symbol $\propto$ to reflect the fact that the term in the denominator representing the probability of the data $Pr(Y_{ij})$ is normally omitted in computational Bayesian analysis as it is fixed with respect to the parameters [@gelman2013].

$$
Pr(\theta|Y_{ij}) \propto Pr(\theta)L(Y_{ij}|\theta)
$$ {#eq-Bayes}

My presentation of the Bayesian ideal point model departs somewhat from conventional parameterizations in order to be more flexible and to incorporate recent advances in measurement modeling. I start by specifying the priors in terms of the distributions assigned to each parameter in $\theta$:

\begin{align}
\alpha_i &\sim \text{Normal}(0,3)\\
\gamma_j &\sim \text{GeneralizedBeta}(2,2)\\
\beta_j &\sim \text{Normal}(0,3)
\end{align}\label{eq-genprior}

The ideal points $\alpha_i$ and the discrimination parameters $\beta_j$ are given a loose $N(0,3)$ prior that is weakly informative for most scales and is only needed to make the posterior proper. Identification of the posterior, which is an important issue as these types of measurement models can have multiple possible solutions with equal likelihood [@bafumiPracticalIssuesImplementing2005], happens via the Generalized Beta prior on the discrimination parameters $\gamma_j$:

$$
f(x; \alpha, \beta, a, b) = \frac{(x - a)^{\alpha - 1} (b - x)^{\beta - 1}}{(b - a)^{\alpha + \beta - 1} B(\alpha, \beta)}
$$ {#eq-genbeta}

where:

\begin{itemize}
    \item $\alpha > 0$ and $\beta > 0$ are the shape and scale parameters, analogous to the prior sample size,
    \item $a = -1$ and $b = 1$ define the support of the distribution,
    \item $B(\alpha, \beta) = \int_0^1 t^{\alpha - 1} (1 - t)^{\beta - 1} \, dt$ is the Beta function.
\end{itemize}

The reason I use this distribution for the items is to resolve two problems at once. First, the restrictions on the range of the $\gamma_j$ parameters solve the problem of identifying a scale for the latent variable because the item discrimination parameters must stay within this range. The Generalized Beta distribution further allows this range to be both positive and negative, which is required for the ideal point model to estimate latent variables with bi-polarity (such as both conservative and liberal ideology). To identify the rotation of the ideal points, specific items can be given a very tight prior, such as:

$$
\gamma_{j=k} \sim \text{GeneralizedBeta}(10,1000)
$$ for a parameter with the majority its support around the point -0.98 and

$$
\gamma_{j=k'} \sim \text{GeneralizedBeta}(1000,10)
$$

for a parameter with the majority of its support around the point +0.98. The shape and scale parameters of the Generalized Beta have an intuitive interpretation as they represent the prior observed negative and positive observations for a given proportion. As a result, increasing both the scale and shape proportionally will preserve the sample expected value but reduce uncertainty around the expected value of the distribution. With larger datasets, increasing both the shape and scale to larger values, such as $(20000, 200)$, is necessary as the strength of the prior is relative to the size of the data as in @eq-Bayes. To fit a traditional IRT model, on the other hand, all this is necessary is to modify the lower bound ($a$) of the Generalized Beta distribution to 0 rather than -1.

Constraining the discrimination parameters builds on work that shows that identification of the items (i.e., votes) in an ideal point model has superior theoretical properties as it allows for the ideal points to float freely, which are usually the main subject of interest in terms of measurement [@morucciMeasurementThatMatches2024]. Furthermore, in a manner similar to @poole1997, I implicitly scale the discrimination parameters to the the open interval $(-1,1)$ by using the Generalized Beta distribution as described previously. I assign weakly informative values of +2 to each of the distribution's parameters (scale and shape) so that values of 0, or the midpoint of the distribution, are weakly preferred over the end points. With this prior, no further work is necessary to fix a scale and all other parameters can have weakly informative priors. In fact, if there is enough data, the discrimination parameters can have the uninformative Generalized Beta prior of +1, +1 so that all unconstrained discrimination parameters $\gamma_j$ are equally likely in the $(-1, 1)$ interval.

The effect $\phi$ of a matrix of covariates $X$ measured on the persons $i$ can also be added to this model for ideal points as shown in @gelman2005 and @jackman2004 by including them as hierarchical predictors:

$$
\alpha_i \sim N(X\phi',3)
$$ {#eq-hier}

Given this definition of the model, I next show how this framework can incorporate missing data when not all of $Y_{ij}$ is observed, time-varying inference when $\alpha_i$ is measured at different points in time and the estimation of diverse distributions for $Y_{ij}$ whether singly or jointly.

## Missing Data

Missing data is a tricky problem to address in latent variable models because latent variables are themselves defined as missing data. For this reason, standard imputation techniques cannot be easily applied. Because ideal point models are representations of social aggregation processes, a straightforward approach of imputing data conditional on the model assumes that missingness is orthogonal to observed ideal points [@rubin2002]. In many situations, this assumption may not be realistic when there is reason to believe that social actors, such as legislators, do not respond to stimuli out of a desire to avoid revealing their latent trait. In this section I define a model for non-ignorable missingness in ideal point estimates that aims for widespread applicability.

I extend my notation of the outcome $Y_{ij}$ by adding a subscript $r \in \{0,1\}$ for whether person $i$ chose to vote or answer item $j$ ($r=1$) or chose not to answer ($r=0$). We can then add a separate selection model that first estimates $Pr(r=0)$ and deflates the likelihood $L(Y_{ijr}|\theta)$ accordingly:

$$
    L(Y_{ijr}|\alpha_i,\gamma_j,\beta_j,\nu_j,\omega_j) = 
    \prod^{I}_{i=1} \prod^{J}_{j=1}
    \begin{cases}
    \zeta(\alpha_{i}'\nu_j - \omega_j ) & \text{if } r=0, \text{ and} \\
    (1-\zeta({\alpha_{i}'\nu_j - \omega_j}))L(Y_{ijr}|\alpha_i,\gamma_j,\beta_j) & \text{if } r=1
    \end{cases}
$$ {#eq-inflate2}

I let $\zeta(\cdot)$ stand for the inverse logit function in the equation above. As can be seen, the selection model for missing data $\zeta(\alpha_i'\nu_j - \omega_j)$ is very similar to @eq-basic except that I have substituted a new set of discrimination $\nu_j$ and difficulty $\omega_j$ parameters. The ideal points $\alpha_i$ enter into both the selection model and the main ideal point model $L(Y_{ijr}|\alpha_i,\gamma_j,\beta_j)$. As such, the selection model is able to inflate or deflate the ideal points by taking into account a first stage process. The inclusion of $\nu_j$ and $\omega_j$ in the first-stage selection model creates a new "missingness" space where person $i$ first chooses whether she will cast a vote, send a tweet or answer a survey question, as the case may be. Only if the item is close to person $i$ in this missingness space will person $i$ then also decide to participate and provide information $Y_{ij}$ that is informative of their ideal point. This model can be interpreted as a censoring model where persons may choose to self-censor their ideal points.

The general form of the selection model--which is essentially a first-stage ideal point model--allows it to pick up a range of non-ignorable missingness patterns if missingness correlates with ideal points. The model does not make a priori assumptions about exactly why ideal points may determine missingness, and as such the model cannot provide such an interpretation a posteriori without taking time to interpret what the parameters of the selection model reveal about actors' intentions. The missingness discrimination parameter $\nu_j$ is very helpful for this purpose because it will indicate which set of items show correlation between missingness and one of the poles of the latent variable. For example, if the latent variable is constrained positive for liberal Senators, then high positive discrimination would indicate that more liberal Senators tend to be absent on a particular bill. A naive model that assumed that absences were ignorable would miss this pattern and treat these Senators as more moderate based solely on their observed voting record.

This model is also general enough to allow for the possibility that data is actually missing at random at the item level. If the discrimination parameter $\nu_j$ is zero then ideal points do not enter into the equation for missingness and $Pr(r=0)$ is equal to the item-specific intercept $\omega_j$. In the legislative context, this would be the same as estimating the probability of absence by the proportion of legislators who do not show up for particular bill. Including this parameter will also separate that item-specific random missingness from missingness patterns suspiciously associated with ideal points. For this reason, the selection model will perform at least as well as a standard imputation method where the missing ideal points are MCAR conditional on each item.

While this missingness model is readily applicable to a legislative context where legislators may not want to show up on votes depending on what the vote would reveal about their ideal points, it is also useful for other non-ignorable missingness patterns. Twitter data is an excellent example. It is well-established that estimating a latent trait like political polarization will be vastly over-stated if the tendency of Twitter (now known as X) users to select who they choose to follow is not taken into account [@Barbera2015]. The two-stage selection model can be used to estimate the people's propensity to retweet ideological content net of their self-selection into whom they follow on social media [@kubinecWhenGroupsFall2021]. For survey data, the two-stage model will take into account that respondents with high or low values on the latent trait could be more or less likely to provide an answer on a given question, and then backwards infer their true ideal point given those varying missingness rates by question.

This model does not generally require more identifying information than previously stipulated (i.e., at least some observed item discrimination $\gamma_j$ pinned to high or low values). However, it does raise model complexity, can require more computation and can in some cases require more pinned item discrimination parameters. However, the missingness parameters themselves can have similar weakly informative priors without causing any serious problems with identification:

\begin{align}
\nu_j &\sim \text{GeneralizedBeta}(2,2)\\
\omega_j &\sim \text{Normal}(0,3)
\end{align}\label{eq-missprior}

## Time-varying Inference

The model specified in @eq-Bayes is a static model: it estimates one ideal point per person for the entire sample. Increasingly, time-varying estimation is useful as the scale and complexity of datasets also increases. In this section, I show how `idealstan` can be extended to allow for a variety of time-varying processes of the person ideal points $\alpha_i$. Importantly, these models are all jointly estimated with the missing-data two-stage model mentioned previously.

The most well-known time-varying ideal point estimates come from the DW-NOMINATE model [@rosenthal2007; @armstrong2014; @Caughey2016], which employs a linear time trend to allow for variation in legislator ideology from one session of Congress to the next. The other widely-used approach is the random walk model of @quinn2002 in which ideal points in time $t$ are equal to ideal points in $t-1$ plus a Normally-distributed random jump. This much more flexible model is quite helpful at determining the probable trend in ideal points over time because it imposes virtually no structure on the time-series trends, although as I discuss below, it adds a considerable greater demand in terms of finding a unique identified solution. There are extensions of the random walk, such as employing a Student T's distribution to allow for more stable patterns over time [@reuning2019], though these approaches have the same difficulties with identification.

<!--# Both of these approaches share in common an emphasis measuring over-time trends. By contrast, in this paper I also want to consider effects of hierarchical predictors on ideal points that vary over time. Including such predictors would enable scholars to test much more precise questions about how time-varying covariates influence ideal points, especially as political scientists obtain ever-larger and more fine-grained datasets. However, existing methods cannot easily handle such covariates. The DW-NOMINATE approach is designed to produce accurate and relatively non-parametric estimates of legislator's ideal points versus inference on those trends, while the random-walk approach cannot include additional covariates without radically changing the model. Any covariate with a constant effect in a random-walk will push the time series in a constant direction over time (i.e., static drift). It is too simple of a model to separate the effect of a covariate from the time series trend.  -->

To produce a wider range of possibilities to time-varying inference, I introduce three alternative time series models: AR(1) processes, Gaussian processes, and basis splines. I begin by defining the random walk by placing a subscript $t$ on the ideal points $\alpha_i$ and stipulating a relationship in the prior between time points:

$$
\alpha_{it} \sim N(\delta_i+ \alpha_{it-1},\sigma_i)
$$ {#eq-rwc}

We give every person $i$ a separate over-time variance parameter $\sigma_i$ and intercept/offset $\delta_i$. For computational reasons, the implementation of this model in `idealstan` employs a non-centered parameterization [@betancourt2013] that reduces dependence between the variance $\sigma_i$ and the prior value of $\alpha_{it-1}$:

$$
\begin{split}
\epsilon_{it} &\sim N(0,1)\\
\sigma_i &\sim E(1)\\
\alpha_{it} &= \delta_i + \alpha_{it-1} + \sigma_i\epsilon_{it}
\end{split}
$$ {#eq-rwnc}

The first new time-series model that I consider is an AR(1) or autoregressive parameterization, which can be understood as a generalization of the random-walk model. To do so, we must add a parameter to the model, $\psi_i$ that is constrained to lie in the $(-1,1)$ interval:

$$
\begin{split}
\epsilon_{it} &\sim N(0,1)\\
\sigma_i &\sim E(1)\\
\alpha_{it} &= \delta_i + \psi_i\alpha_{it-1} + \sigma_i\epsilon_{it}
\end{split}
$$ {#eq-ar1}

If $\psi_i$ lies in the $(-1,1)$ interval, then the series is stationary and will always return or decay to the long-term mean of the series $\delta_i$. By comparison, the random walk model can be thought of as an AR(1) model where $\psi_i$ is fixed at 1. This type of time series model implies that a given person or legislator has a stable over-time average ideal point and that movement away from that over-time average is some kind of exogenous shock that will eventually decay.

Both the AR(1) model and the random walk are models of discrete time, or a set number of time points. The next two time series processes that I consider are both continuous models of time; given any set of time points, they are capable of producing smooth interpolations between observed time points. The first such time process I consider is a Gaussian process ideal point model in which the ideal points $\alpha_{it}$ vary in terms of a Gaussian process with a squared-exponential kernel. A Gaussian process (GP) is chosen because it represents the most flexible framework possible for semi-parametric inference of either spatial or temporal autocorrelation [@rasmussen2006]. Given the limited application of this model in political science research, let alone ideal point models specifically, I briefly review the notation for a GP before combining it with the ideal point model.

A GP is usually defined as a "distribution over functions" [@rasmussen2006, 13]. We assume that underlying an observed time series $x_t$ is an unobserved function $f(x_t)$. To estimate the most likely values of $f(x_t)$, I can assume $f(x_t)$ is multivariate Normally-distributed where the mean and covariance of this distribution are themselves functions of the mean and covariance of $f(x_t)$:

$$
f(x_t) \sim N(\mu(f(x_t)),\Sigma(f(x_t)))
$$ {#eq-gp1}

Usually the $\mu(f(x_t))$ function is assumed to be 0 as it does not vary with $t$. Instead, much of the flexibility of the distribution involves specifying a function for $\Sigma(f(x_t))$, the covariance. The function most often used, and incorporated here, is the squared-exponential kernel $k_t(\cdot)$:

$$
k_t(x_t,x_{t'}) = \large \sigma^2_f  e^{\displaystyle -\frac{(x_t - x_{t'})^2}{ 2l^2}} + \sigma^2_{x_t}  
$$ {#eq-gpcov}

What this function does is convert all the distances in terms of time between $x_t$ and $x_{t'}$ into a positive semi-definite covariance matrix $\Sigma$. We can then sample from this multivariate Normal with covariance $\Sigma$ to estimate a distribution over the possible time-series functions $f(x_t)$. The GP framework is so flexible that it can fit an *infinite* number of basis functions of $x_t$ [@rasmussen2006, 14]. Other semi-parametric functions, such as splines and polynomials, are special cases of the GP for certain values of the hyper parameters used in the covariance function [@rasmussen2006, 137-140]. What is even more appealing is that the GP can handle time-varying covariates $X_t\phi'$ simply by including them in place of $\mu(x_t)$. The multivariate Normal will then sample from the specified covariate matrix while averaging over the possible values of the covariates.

What gives the GP the ability to fit so many different functions are the three hyper-parameters in @eq-gpcov. $\sigma^2_{if}$ can be referred to as the marginal standard deviation and represents the total amount of variance in $x_t$ explained by the covariance function $k(x_t,x_{t'})$. A higher value for this hyperparameter will result in more bounce in the time series. $\sigma^2_{ix_t}$, on the other hand, represents residual variance in the time series $x_t$ that the covariance function does not fully explain (which could be interpreted as additional measurement error). Finally, the length-scale parameter $l^2_i$ represents a smoothing factor controlling how much different in time are correlated together. A very low length-scale will allow the time-series to cross the origin at a much higher rate. Conversely, higher length-scales result in smoother functions. The three hyper-parameters both overlap and interact, which makes isolating the effect of any one hyper-parameter difficult.

To employ the GP as a time process for ideal points, we simply replace the observed $x_t$ with the latent ideal points $\alpha_{it}$:

$$
\alpha_i \sim N(0,k_t(\alpha_{it},\alpha_{it'}))
$$ {#eq-gpideal}

This straightforward parameterization shows some of the power of Bayesian modeling. We can include virtually any time process by simply defining the time series over a set of parameters rather than a set of observed data. Of course, whether this model can be identified in a latent variable model with measurement error is a different question. Because of the GP's complexity, I turn next to consider a different option, basis splines, that permit more control over the flexibility of the time-series process.

Splines are a method of constructing semi-parametric time-series by combining polynomial functions of time [@perperoglouReviewSplineFunction2019]. While there are many different kinds, I consider here basis splines due to their ability to fit into the latent variable framework. A basis spline separates a polynomial function of time into individual basis functions that the space of all possible polynomial functions for a given polynomial degree [@deboorCalculatingBsplines1972; @gordonBSPLINECURVESSURFACES1974]. Because each of the individual functions are relatively simple, this construction allows for a wide variety of polynomial functions to be derived while also enabling efficient sampling.

One central feature of splines is that they are polynomial functions defined over a series of sequential partitions of a given time series known as knots. A spline of a given order is a polynomial function of a given degree (by convention, one less than the order) that is defined for each sequential partition given by the location of the knots. Each piecewise polynomial function must begin and end at the same point of each sequential partition, ensuring a smooth combined function.

The presentation below follows that of @kharratzadeh2017. Given a spline order $d$ and a given number $s$ of sequential knots $q$, we can define a basis matrix $B_{q,d}$ as a function of continuous time points $t \in \mathbb{R}|t_{min} < t < T$:

$$
B_{s,d}(t) = \omega_{s,d} B_{s,d}(t) + (1 - \omega_{s+1,d}B_{s+1,d-1}(t))
$$ {#eq-bfunc}

where the function $\omega(\cdot)$ can be defined as:

$$
\omega_{s,d} = \begin{cases} 
\frac{t-q_s}{q_{s+d-1}-q_s}, & \text{if } q_s \neq q_{s+d-1} \\
0, & \text{otherwise}
\end{cases}
$$

The resulting basis matrix $B_{s,d}(t)$ is a set of vectors of number of columns $T$ and number rows equal to the order of the spline. The polynomial function of $t$ can then be generated by multiplying the basis matrix by a vector of spline coefficients $A_i$ :

$$
S_{q,d}(t) = B_{s,d}(t)A_i
$$ {#eq-splinefunc}

where the length of spline coefficient vector $A_i$ is equal to the number of knots plus the degree of the spline. While in practice $t$ is in a set of discrete time points, defining the function over a continuous interval permits interpolation of the time series, a convenient feature of the spline technique.

We can then sample the spline coefficients by defining a loose prior distribution for $A_i$ as a function of a scaling parameter $\tau$ and a location parameter $A'_i$:

\begin{align}
A'_i &\sim N(0,1)\\
\tau &\sim E(1)\\
A_i &= A_i'\tau
\end{align} \label{eq-splineprior}

The adjustment with $A_i'$ and $\tau$ is to permit non-centered sampling which is more efficient.

The main advantage of using basis splines as I demonstrate in the empirical application is that it is possible to fine-tune the complexity of the time series function, which is valuable as it allows for theoretical priors to govern the amount of possible over-time variation in the latent trait. Increasing either the degree of the polynomial or the number of knots increases the number of coefficients per person by one. In cases of high sparsity, a low number of knots (or even no knots at all) and a low degree polynomial can be used to find a stable latent variable estimation even with very limited data per time point.

As a result, these different time series models can be understood in terms of order of complexity. Except for splines, all of the time series models require at least one parameter per time point $t$. The random walk is the simplest of these models as it only requires one additional parameter, $\sigma_i$, per person $i$. The AR(1) model is more complex as it introduces an additional parameter to control the decay rate in the series, $\psi_i$. The GP is the most complex as it has three separate parameters per person, $\sigma^2_{if}$, $l_i^2$ and $\sigma^2_{iY}$, in addition to one estimate per time point $t$. For these reasons, the GP is the most demanding model to fit for a latent variable model as there needs to be a substantial amount of data per time point to estimate a flexible time trend while also separating our measurement noise.

For these reasons, I argue that lower-dimensional time series methods like splines are most useful in situations where data are relatively sparse per time point. As the ideal point model is a measurement model, it can be difficult to separate measurement noise from the additional noise arising from flexible time series processes.

Having defined these time series models, I know show that it is possible to combine these time-series processes with the model of missing data in @eq-inflate1 to permit inference on ideal points that vary over time and may have non-ignorable missingness:

$$
    L(Y_{ijtr}|\alpha_{it},\gamma_j,\beta_j,\nu_j,\omega_j) = 
    \prod^{T}_{t=1} \prod^{I}_{i=1} \prod^{J}_{j=1}
    \begin{cases}
    \zeta(\alpha_{it}'\nu_j - \omega_j ) & \text{if } r=0, \text{ and} \\
    (1-\zeta({\alpha_{it}'\nu_j - \omega_j}))L(Y_{ijtr}|\alpha_{it},\gamma_j,\beta_j) & \text{if } r=1
    \end{cases}
$$ {#eq-inflate1}

To change from a random-walk to a different time-series model for $\alpha_{it}$, we can simply change the prior for $\alpha_{it}$ that we include in $Pr(\theta)$ with one of the previously-defined time processes. I do not index the item parameters by $t$ because in many applications of the model, the item parameters only occur at one point in time, such as bills in a legislature. It is possible in some situations, such as students taking a test multiple times, for item parameters to vary over time as well. Inference on these parameters' time-series trends would then involve defining time-series priors over these parameters, whether jointly with $\alpha_{it}$ or separately, which is a possible area for future research.

As I have mentioned, identification for models with time-varying $\alpha_{it}$ is a challenging endeavor without extensive discussion in the literature. @quinn2002 addressed this issue for random-walk models by restricting the variance $\sigma_i$ to values strictly between 0 and 0.1. This hard-coded constraint forces the time series to change little over time, though it also limits inference as the time series can only differ so much from each other. By contrast, I impose generally diffuse priors, or priors strong enough to impose some scaling identifiability, without forcing hard constraints on variance or other parameters. The Generalized Beta prior on the item discrimination parameters $\gamma_j$ is able to enforce a fixed scale for the latent trait while informative priors on pinned items is sufficient to identify a particular mode.

However, if the data are very sparse in terms of having very few items per time point, then identification can be a serious challenge as pinning the discrimination parameter of a particular item may not constrain the time-series process sufficiently. This identification issue is particularly problematic when items are only observed in single time points as is the case with legislative data (rollcall votes are not repeated). In these cases, the spline function becomes particularly useful, as I show later, because it can rule out edge modes where latent traits shift and oscillate radically, such as a legislator moving from conservative to liberal across consecutive time points.

## Diverse Distributions

Up to this point, I have followed convention in assuming that the observed data $Y_{ij}$ is a binary outcome (Bernoulli distribution). However, this is a limitation which unnecessarily restricts ideal point models to binary data. There are noted exceptions, such as the Poisson "Wordfish" model [@slapin2008] and @imaiFastEstimationIdeal2016, who implement EM-based estimation for ordinal outcomes. Within the Bayesian framework I have proposed, and combined with the extraordinary flexibility of Stan's HMC algorithm, virtually any distribution for $Y_{ij}$ is possible and it is even possible to let the distribution for $Y_{ij}$ to vary at the item level. For an introduction to this type of joint modeling of distributions in a Bayesian framework, see @kropko2013. These types of mixed outcomes are especially important for survey data in which questions could have binary, ordinal, proportions, counts or continuous response distributions.

To use a different distribution, the likelihood function $L(\cdot)$ in @eq-inflate1 can be modified with any probability density or mass function that properly integrates or sums to 1 where $m$ is an index for all $M$ distributions used for the mixed outcome $Y_{ijtm}$:

$$
  L(Y_{ijtm}|\alpha_{it},\gamma_j,\beta_j) =  \prod_{i-1}^{I} \prod_{j=1}^{J} \prod_{t=1}^{T} \prod_{m=1}^{M} L_m(\gamma_j\alpha_{it}-\beta_j)
$$ {#eq-likem}

where $L_m$ represents the likelihood function for the $m$th distribution. While I suppressed notation for missing data for the sake of brevity, all of these models can also be estimated marginal of missing data $r$ using the notation presented earlier in addition to incorporating time-series dynamics.

The software package `idealstan` implements such likelihoods for the Normal distribution, the log-Normal distribution, the Poisson distribution, ordinal outcomes for both rating scale and graded response formulations, and the ordered Beta distribution for bounded continuous responses like percentages and proportions [@kubinecOrderedBetaRegression2022]. Furthermore, because of the modular nature of Stan's probabilistic program engine, these distributions can be mixed at the item level and can be jointly estimated with all of the time series processes and missing-data adjustments discussed previously. For reasons of brevity, I refer the reader to the first section of the supplementary information for the full derivation of these additional likelihoods.

## Identification and Starting Values

While identification is achievable in statistical terms by putting tight priors on item discrimination parameters (and indeed is an advisable practice to ensure the latent trait accords with theoretical priors [@morucciMeasurementThatMatches2024]), it is not necessarily sufficient to achieve identification in terms of estimation. Estimating time-varying latent variables in particular is very difficult to achieve. As mentioned previously, @quinn2002 used a very restrictive bound on over-time variance to identify their estimates of Supreme Court justices. In order to estimate DW-NOMINATE using optimization methods, Poole and Rosenthal had to undergo a laborious process of choosing starting values for legislator ideal points, estimating dynamic scores, and then reviewing the scores to see if they resembled their theoretical priors [@carrollMeasuringBiasUncertainty2009]. Similarly, @barbera2015 scores were only possible to derive by using starting values for HMC with Stan that accorded with theoretical expectations about ideal point distributions.

While identifying the sign and rotation of ideal points with pinned discrimination parameters is sufficient for static models, high-dimensional time-varying latent variable model posteriors can be difficult for HMC to adequately explore even though it is the most robust and stable MCMC algorithm available [@CarpenterGelmanHoffmanEtAl2017]. For high-dimensional models, such as latent variables with hundreds of time points, HMC can end up in lower-probability modes that may have the opposite polarity of what the identification restrictions require. Specifying starting values that are closer to the posterior mode can help HMC converge and explore the posterior for very difficult problems. However, specifying starting values a priori can either be extremely tedious as the DW-NOMINATE procedure mentioned previously or it can result in latent variables that depend on arbitrarily chosen starting values that can mask convergence issues with HMC and result in fragile inferences.

To address this problem, `idealstan` implements a novel method for identifying starting values for high-dimensional HMC using what is known as the Pathfinder algorithm [@zhangPathfinderParallelQuasiNewton2021]. The Pathfinder algorithm is a form of variational inference [@goplerudMultinomialFrameworkIdeal2019; @Grimmer2011] that approximates the joint posterior by minimizing the Kullback-Leibler divergence with a form of Newton optimization. While this approximation is unlikely to capture the correct form of the joint posterior distribution, it is remarkably robust considering its relative simplicity [@zhangPathfinderParallelQuasiNewton2021]. Importantly, Pathfinder integrates with the Stan framework so that Pathfinder can first be used to identify starting values for the HMC algorithm, which is particularly important for high-dimensional time-varying models. These starting values are optimal in the sense that they should be reasonably close to the posterior mode, and they also do not reflect researcher discretion, which makes them a more robust choice for measurement exercises as they are in principle replicable.

This method of "mode-finding" works so well that in fact it can even be used in place of the identification restrictions on item parameters mentioned previously, but it should be noted that by doing so, the ideal point model is no longer a confirmatory method and instead should be used in an exploratory context in which the derived latent variable may or may not have any clear empirical meaning. For all the empirical examples shown later, Pathfinder was used to ensure convergence of the HMC chains to the optimal mode conditional on informative identification conditions.

## Big Data Inference

The Achilles heel of Bayesian inference is generalizing to large data sets. Markov chains, which form the core of the estimation engine, cannot be easily parallelized as each iteration depends on the value of the previous iteration. However, recently the Stan team has implemented a way to parallelize the *posterior gradient* computation within a Markov chain, which are necessary and very expensive to calculate the iteration trajectories of Hamiltonian MCMC. Any model likelihood can be parallelized in terms of its gradient calculations so long as the parameters are conditionally independent within discrete subsets or "shards" of the data. For a static ideal point model, this means that the likelihood can be parallelized across subsets of persons or items. For a dynamic model, parallelization can be performed across persons as the ideal point for a given person at time $t$ depends on the value of the ideal point in $t-1$, which would break conditional independence of parameters if the data were subset by item.

This type of parallelization of the gradient calculations is implemented for all of the models previously described. This allows for the computation of Bayesian ideal points for datasets that were previously intractable due to the length of time required. However, it should be noted that this type of parallelization does have significant overhead as not all of the computations of the Markov Chain are parallelizable, leading to significant information that must be shared across parallel threads. Furthermore, this type of parallelization is only possible using in-memory parallel techniques, which restricts the application to single nodes on a computer cluster.

Even with these restrictions, though, this type of computation greatly expands the possibilities for Bayesian inference on big datasets, reducing the time required for model convergence by several orders of magnitude depending on the number of cores available. In the supplementary information section 3 I show a simulation of `idealstan` models of varying sizes and the associated speedups from using multiple cores. Moving from a single core to four cores, which is available on many laptops, can cut estimation time for a medium-sized dataset from approximately 20 minutes to approximately 7 minutes. With larger numbers of cores (up to the limit of what is available in single machines in conventional computing clusters), run times can be reduced to as low as a single minute for datasets of up to 100,000 observations.

## Ideal Point Marginal Effects

Having specified the types of models being proposed, I next turn to a new post-estimation quantity that helps to both clarify what ideal points substantively mean. Scholars want to identify latent traits because they want to understand what factors might predict the latent trait. Most analyses with latent variables use an estimate in a separate regression model, potentially with the method of composition to propagate measurement uncertainty [@caugheySubstanceChangeCongressional2016].

However, it is also possible to jointly estimate the covariate as a hierarchical predictor in the model, as noted previously. While this type of analysis has been available for some time, the benefit of performing this kind of joint estimation, as opposed to using a summary measure in a downstream analysis. Ideal point marginal effects help clarify what can be understood by directly predicting the latent trait. In particular, ideal point marginal effects can reveal the relationship between the covariate and the observed indicators while taking into account the mediation of the latent variable—that is, the effect of the covariate on outcomes that is weighted by the relevance of the outcomes to the latent variable.

To define this estimand, I return to the notation used previously and take the derivative of @eq-likem. For a given ideal point $\alpha_{it}$, the marginal effect of a covariate $x$ on an outcome $Y_{ijt}$ with a likelihood function $L_m(\cdot)$ is equivalent to:

$$
\frac{\partial Y_{ijtm}}{\partial x} \left( L_m(\gamma_j(\alpha_{it} + \phi x) - \beta_j) \right) = \phi \gamma_j L_m'(\gamma_j(\alpha_{it} + \phi x) - \beta_j)
$$ {#eq-deriv}

which follows directly from the chain rule.

What is important is that the quantity in @eq-deriv can be decomposed into three constituent parts: the derivative of the likelihood for the given model $L_m'(\cdot)$, the value of the discrimination parameter $\gamma_j$, and the value of the coefficient of the covariate, $\phi$. Multiplying these terms together will obtain the simultaneous change in $Y_{ijtm}$ for a one-unit change in the covariate $x$.

From the notation it is clear that this formula permits the marginalization of marginal effects across models $M$. However, doing so could result in marginal effect estimates that are difficult to interpret as it would involve averaging over the different scales of the mixed outcome $Y_{ijtm}$, such as a marginal effect that is 40% the change in a count variable and 60% change in the probability of an ordinal outcome. For practical purposes, then, I assume that ideal point marginal effects should be reported separately for each distribution $m \in M$, and potentially each item $j \in J$, for a given mixed outcome.

There are two methods to calculate these types of marginal effects. The first would be to analytically calculate these quantities for each specification. While this is possible to do, it is quite complicated as different linear model specifications, such as including a quadratic term for $x$, would necessitate a separate analytical derivative. A much more flexible method is to adopt numerical differentiation instead.

As @arel-bundockMarginaleffectsMarginalEffects2022 and @leeperPackageMargins2017 describe, numerical differentiation can calculate marginal effects through a simple formula. For a given covariate $x$ and likelihood function $L_m(\cdot)$, we can calculate $L_m'(x)$ by calculating two points on the function $L_m(x)$ that are only a very small value $\epsilon$ apart:

$$
L_m'(x) = lim_{\epsilon\rightarrow0}\frac{L_m(x + \epsilon) - L_m(x-\epsilon)}{2\epsilon}
$$ {#eq-numderiv} We can apply the formula in @eq-numderiv to our derivative specification in @eq-deriv to calculate ideal point marginal effects for a covariate $x$ separately for each item $j \in \{1, 2, ... J\}$:

$$
\frac{\partial_j Y_{itj=jm}}{\partial_j x} \left( L_m(\gamma_j(\alpha_{it} + \phi x) - \beta_j) \right) = lim_{\epsilon\rightarrow0}\frac{L_m(\gamma_j(\alpha_{it} + \phi (x + \epsilon)) - \beta_j) - L_m(\gamma_j(\alpha_{it} + \phi (x-\epsilon)) - \beta_j)}{2\epsilon}
$$ {#eq-idealderiv}

Given these item-specific marginal effects, we can also calculate a sample-average ideal point marginal effect of $x$ by averaging over all items $j$:

$$
\frac{\partial Y_{itjm}}{\partial x} \left( L_m(\gamma_j(\alpha_{it} + \phi x) - \beta_j) \right) = \frac{\sum_{j=1}^J \frac{\partial_j Y_{itj=jm}}{\partial_j x}}{J}
$$ {#eq-avgmarg}

However, this sample-average quantity is less interesting as it averages over item discrimination (and any mixed models), which is one of the most useful aspects of this quantity of interest. An ideal point marginal effect of a covariate $x$ defined for persons $i$ and item $j$ is the change in that item given its relative polarity and the relative ideal point of the person. As I demonstrate in the empirical section, the disaggregated item-specific marginal effect intuitively captures the latent variable information encoded in the parameters in a way that makes the relationship clear between the ideal point covariate and the indicators for the latent trait.

The power of this method of numerical differentiation comes by enabling a variety of linear model specifications. For example, to calculate these marginal effects when including a squared term for $x$, we simply extend the formula by including this term in the limit,

\begin{equation}
\frac{\partial_j Y_{itj=jm}}{\partial_j x} \left( L_m(\gamma_j(\alpha_{it} + \phi_1 x + \phi_2 x^2) - \beta_j) \right) = lim_{\epsilon\rightarrow0}\frac{\begin{split} &L_m(\gamma_j(\alpha_{it} + \phi_1 (x + \epsilon) + \phi_2((x + \epsilon)^2)) - \beta_j)\\ - &L_m(\gamma_j(\alpha_{it} + \phi_1 (x-\epsilon) + \phi_2((x - \epsilon)^2)) - \beta_j)\end{split}}{2\epsilon}
\end{equation}

and so on for any differentiable function of $x$.

Finally, it is important to note that because all the parameters in @eq-idealderiv have posterior distributions, it is trivial to obtain uncertainty in these estimates by iterating @eq-idealderiv over the number of posterior draws and using empirical quantiles for uncertainty intervals. The main computational cost is calculating model predictions over $x + \epsilon$ and $x - \epsilon$, although this computation is embarrassingly parallel in the items $j$ and thus not a constraint given adequate computational resources.

# Empirical Examples

In this section I present original empirical findings with the `idealstan` model while also showing how the model can be best applied to empirical data. In addition, I show the conditions under which `idealstan`'s innovations, especially the missing-data adjustment and varying time-series processes, can affect inferences that scholars make. Given that comparison to existing models is helpful for establishing model validity, I first discuss an example drawn from Japanese survey data in @imaiFastEstimationIdeal2016 and then a comprehensive dataset of rollcall votes from the U.S. House from 1990 to 2018.

## Missing Data in Voter-Candidate Surveys

Examining survey data illustrates how `idealstan` can be applied to diverse forms of social science data. This analysis will also illustrate why the missing-data adjustment mechanism is even more important for survey data than legislative data because missingness rates on surveys can be quite high and there is often only a single measurement per respondent. As a result, non-ignorable missing data can significantly affect the estimation of respondent-level latent traits like ideology or pro-democratic attitudes.

To show this adjustment in practice, I re-analyzed estimates of survey respondent ideology in @imaiFastEstimationIdeal2016 based on a joint legislator-voter survey in Japan known as the UTokyo-Asahi Survey (for more information, see @hiranoPolicyPositionsMixed2011). The @imaiFastEstimationIdeal2016 method, known as emIRT, is able to scale diverse types of data very quickly using an expectation-maximization (EM) algorithm, but it is not able to incorporate missing data. They use this method to scale 19,443 responses with 98 survey questions in a very short amount of time (approximately two minutes). While `idealstan` can also estimate models of this size, it requires a much longer period of approximately six hours on a cluster node with 16 cores.[^2] For that reason, `idealstan` should not be seen as a replacement for emIRT given that only emIRT can accommodate the largest datasets, such as those with tens of millions of observations.[^3] However, for this analysis and others, while estimation takes longer than emIRT, in a measurement context models are often run once and then the output is further analyzed, which makes the estimation time a (more or less) fixed cost for a given study.

[^2]: These speeds are based on a 2021 ARM-based server with access to 48 cores. The actual speed of the MCMC HMC algorithm depends on the number of cores available and the type of cores available. Generally speaking, the parallelization algorithm performs well with CPU cores with large and fast caches to speed up data transfer. Modern CPUs of the ARM variety in Apple machines perform particularly well with this algorithm and can reduce computation time considerably. While very large models require the use of high-performance machine with dozens of cores, even quite large models can be fit with a laptop that has a modern processor and multiple available cores.

[^3]: It is possible for `idealstan` to also estimate such large datasets if more complicated forms of parallelization are used such as the message-passing interface (MPI) that can parallelize across nodes. However, while such models are tractable with `idealstan`, they would require significant technical work to implement on high-performance computing clusters, and as such I do not consider them further in this article.

I first reproduce their analysis and compare it to two estimates from similar ordinal IRT models, one using the parameterization in this paper with missing data and the other with only observed data (no adjustment). @fig-asahi plots the emIRT estimates from @imaiFastEstimationIdeal2016 against an `idealstan` model with only observed data and an `idealstan` model that uses the missing data adjustment. The same data was used in both models along with pinned survey item discrimination parameters based on the emIRT fit. As can be seen, when using only observed data, `idealstan` estimates are almost identical to those of emIRT (correlation of 0.99). However, with missing data, the estimates notably diverge (correlation of 0.86). In particular, emIRT tends to record respondents at the negative end of the scale as having less dispersion than `idealstan`.

Substantively, the ends of this latent scale correspond to a long-standing cleavage in Japanese politics: support for Japan's constitution, especially its prohibition on aggressive military, versus a strident Japanese nationalism that seeks to change these post-war institutions [@hirano2011]. The emIRT model, along with the `idealstan` model, estimates this cleavage as the first dimension given a question about whether foreigners should be eligible for permanent residency being pinned towards negative values (indicating liberal cosmpolitanism) and a question about whether Japan should revise its constitution pinned towards positive values (strident nationalism). As a result, the stronger discrimination at the negative end of the scale in the missing data model in @fig-asahi implies that respondents who held more liberal views were hesitant to answer some questions, and by inferring that this missingness could be correlated with their ideal point, `idealstan` is able to estimate that a subset of respondents are more liberal than they would appear to be if missing data were completely missing at random.

```{r}
#| fig-cap: "Comparison of Imai et al. emIRT Estimates to Bayesian IRT with and Without Misssing Data"
#| label: fig-asahi

# load results based on external script utokyo_asahi_comparison.R
# run that script first to reproduce these results

combine_tibble <- readRDS("data/combine_tibble_emIRT.rds")

combine_tibble %>% 
  ggplot(aes(y=em_estimate,x=estimate)) +
  geom_point(colour="black",alpha=0.5) +
  labs(y="emIRT Ideal Points",x="idealstan Ideal Points",
       caption=stringr::str_wrap("Plot shows survey respondent ideal point estimates from the idealstan package for an IRT ideal point model with only observed data from the Asahi Todai survey and an idealstan model that incorporates non-ignorable missing data. These ideal point estimates are plotted against emIRT estimates, which also only use observed data.")) +
  ggthemes::theme_clean() +
  facet_wrap(~model) +
  stat_smooth(method="lm")

```

To understand the drivers of this change in the ideal point estimates, in @fig-itemasahi I plot the item missingness discrimination parameters from the `idealstan` model with missing data adjustment against the percent of missing data for each survey item. As can be seen, in this survey missing data rates are relatively low, with the highest rate at 15% and many questions with as little as 5% or less missing data. Furthermore, there is a weak relationship between the amount of missing data and missing data discrimination: a question can have only a little missing data, yet the missingness patterns can be strongly correlated with the latent trait. For example, the question `defense`, which asked respondents whether Japan "should have the right to preemptive defense when it feel there is a legitimate threat" [@hirano2011, p. 27], has only 3% missing data but has a missingness discrimination parameter $\nu_j$ of approximately -0.85. In other words, relatively few respondents did not answer this question, but for those who did not answer, they tended to be quite liberal (according to the scale).

```{r}
#| label: fig-itemasahi
#| fig-cap: Item Discrimination Parameters Versus Percent Missing Data in Survey Item

miss_data <- readRDS("data/miss_data_emIRT.rds")

miss_data %>% 
  filter(!(item_id %in% c("Q4_22_12","Q5_22_12"))) %>% 
  ggplot(aes(y=prop_miss,x=`Posterior Median`,label=item_id)) +
  geom_text(check_overlap=TRUE,colour="black") +
  ggthemes::theme_clean() +
  scale_y_continuous(labels=scales::percent) +
  labs(y="Proportion Missing in Survey Item",x="Item Missingness Discrimination",
       caption=stringr::str_wrap("Plot shows proportion of missing data at the question/item level for the Asahi Todai survey on Japanese political issues versus the size of the discrimination parameter for missing data at the item level from the idealstan IRT ideal point model. Large values imply that missingness is non-ignorable (i.e., correlated with the latent trait being estimated)."))

```

It is beyond the scope of this paper to discuss fully the reasons why liberal cosmopolitan voters and candidates in Japan may be less likely to answer questions related to Japanese militarism and nationalism, especially when more nationalist respondents do not appear to have the same kind of proclivity. More nationalist respondents have ideal points that are roughly the same regardless of whether missing data is modeled directly or not per @fig-asahi. While missing data rates in this survey are quite low, the strong missingness discrimination of certain questions has an outsize impact on the negative end of the scale. It would seem that in this example the missing data adjustment uncovers fascinating patterns in how politics intersects with survey question responses in a way that both corrects for potential bias in the estimated scale and opens up potential new empirical findings.

For these reasons, the missing data adjustment presented in this paper, while relatively simple, can have serious ramifications for measurement when missing data exists and there is reason to believe that missingness is correlated with the latent trait. In the next section, I look at missingness in a much "harder" case of the U.S. Congress where absences by legislators are often plausibly ignorable due to issues like poor health. However, as I show, incorporating missing data adjustment, especially in combination with robust time series processes, can reveal useful patterns of behavior that observed data models tend to obscure.

## Time Processes, Missing Data and Covariates in The U.S. Congress

In this section, I combine the missing-data adjustment along with the new time-series methods in `idealstan` to show how these innovations can affect our knowledge of legislative behavior. To do so, I fit time-varying ideal point models to estimate monthly legislator ideal points for all Congresspeople in the U.S. House. While my total dataset encompasses the period 1990 to 2018, I focus first on the 115th Congress (2016 to 2018) as it permits easier comparison of the different time series methods, especially as the most complicated time series methods are simply not tractable for the full dataset.

The estimation of ideal point marginal effects, which will be defined at the legislator level, allows me to answer a research question about the role of changes in district-level unemployment on political polarization in Congress. Recent research has proposed that changes in economic conditions, whether in terms of economic inequality and trade, may be affecting increasing polarization in the U.S. Congress. It has long been understood that there is an important interaction in terms of how voters perceive economic recession and their willingness to vote for parties in a polarized environment [@alesina1995].

However, we also know that the salience of policy-making varies with economic conditions that correlate with external shocks [@baker2016]. Most of the studies of trade and other shocks on Congress employ outcomes at the year level, whether the unit of analysis are states or Congressional districts. To show the power of the time-series models I employ, I analyze month-level variation in unemployment by Congressional district and its association with legislator ideal points in the House to see to what extent localized economic shocks are associated with either polarizing or de-polarizing voting behavior.

<!--# Furthermore, I interact unemployment with the level that these districts experienced adverse shocks due to exposure to Chinese trade to see whether heightened salience due to recession increases polarization among legislators in those districts even further. It has been shown that exposure to Chinese trade has important consequences for manufacturing across the United States [@autor2013], and also has implications for how legislators vote on trade-related legislation [@hall2015] and foreign policy hositility towards China [@kuk2017]. As such, exposure to trade with China is an important potential mediator of the effect of unemployment on legislative behavior.  -->

The data I collect come from the Bureau of Labor Statistic's Local Area Unemployment Statistics program, which produces unadjusted estimates of monthly unemployment by local-level units. To aggregate these numbers to the Congressional district level, I performed a spatial join by areal interpolation (i.e., weighted by amount of overlap) between the county-level monthly series and Congressional district boundaries from @lewis2013. The resulting dataset represents the average monthly unemployment data that would be most relevant to a particular Congressperson's district. Even with this high level of disaggregation, the dataset has very low missingness (50 district-year-month observations), which I impute non-parametrically using the technique of @StekhovenBuehlmann2012.

<!--# I then merge this dataset with a decade-level measure of exposure to Chinese imports from @hall2015 who aggregated information from @autor2013 to produce district-level measures of import exposure per worker. I use this measure as a direct way of determining how strongly Chinese trade has affected a particular district. The measure only has two over-time values, unfortunately, but there is substantial variation across districts. Their measure does not have specific units as it is a weighted multiplication of workers in a district in a given industry and that industry's trade with China, although the measure is scaled to isolate Chinese dependence as opposed to other kinds of trade behavior.  -->

```{r histun}
#| label: fig-hist
#| fig-cap: "Histogram of District-Level Unemployment"

over_states <- readRDS('data/over_states.rds')

out_plot <- over_states %>% 
  ggplot(aes(x=unemp_rate)) +
  geom_histogram() +
  theme(panel.grid=element_blank(),
        panel.background = element_blank()) +
  ylab("") +
  xlab("Unemployment Rates") +
  scale_x_continuous(labels=scales::percent)

print(out_plot)

```

```{r plotcount}
#| label: fig-plotcount
#| fig-cap: Monthly Unemployment Rates by U.S. County, 1990-2018

county_series <- readRDS('data/county_series.rds')

county_series %>% 
  mutate(unemp_rate=unemp/labor_force) %>% 
  filter(state %in% c("AL","CA","NY","MI")) %>% 
    distinct %>% 
  ggplot(aes(y=unemp_rate,x=date_recode)) + 
    geom_line(alpha=0.2,aes(group=fips_county)) +
    theme(panel.grid=element_blank(),
          panel.background = element_blank(),
          strip.background = element_blank(),
          axis.text=element_text(face="bold"),
          strip.text = element_text(face="bold")) +
    facet_wrap(~state,scales='free_y',ncol=2) +
    xlab("") +
    ylab("Unemployment Rates") +
    scale_y_continuous(labels=scales::percent)
  
ggsave("month_unemp.png",width=5,height=4)
```

@fig-hist shows the overall distribution of district-level unemployment rates from 1990 to 2018 inclusive, with a distinct mode at 5.0%, the so-called natural rate of unemployment. @fig-plotcount breaks out the distribution of county-level unemployment rates over time by four states: Alabama, California, Michigan and New York. As can be seen, there is substantial variation both over time and across counties within a state. As such, this series is an excellent data source for showing the utility of time-varying ideal point models. We can also learn empirically from seeing how unemployment rates are differentially associated with legislator polarization in both parties in the House. With the `idealstan` model, we can consider the following hypothesis about the relationship between localized recession and legislators' ideal points:

> H1: When monthly unemployment rates rise, legislator voting behavior in each party become less polarized.

This hypothesis expresses the idea that in times of recession legislators should feel more pressure to work across the aisle to produce bipartisan legislation, which we can measure as the spread of ideal points between both parties. We can test this hypothesis by including unemployment rates as an exogenous covariate in the ideal point model while interacting it with an indicator for each legislator's party ID. The effect of the covariate then is associated with voting behavior differentially depending on the legislator's ideal point, party membership and the relative level of discrimination (polarization) of the vote.

To test this hypothesis, I fit each of the described time-series models separately to rollcall vote data from the 115th Congress both with and without the missing-data 1st stage adjustment. To identify the model, I pin the discrimination parameters $\gamma_j$ for three party-line votes in which all Republicans voted for a measure as +.997, and three Democrat party-line votes in which all Democrats voted against a measure as -.997. Utilizing the same pinned parameters for all models allows me to compare their relative ability to obtain identification of a single rotation of the ideal points.

For the spline specifications, I consider splines of degrees 2, 3 and 4 while using only one knot (i.e., one polynomial function for the whole time period). As I show later, when modeling multiple Congressional sessions, using knots for presidential administrations allows for period effects in the time series. With only one Congressional session, however, a single polynomial function is sufficient as it would not be expected for a member of Congress to switch their ideological position in a noticeable way more than once or perhaps twice per term.

As a result, I do find that the simpler time series models, i.e. the splines, are more stable and easier to identify with this dataset with the same six restrictions on item discriminations. The other more complicated time-series models are largely identified because of the use of Pathfinder to identify starting values in a single mode. While this is a valid form of measurement inference, as I show below, these more flexible models are clearly not appropriate for a dataset with many sparse time points as they permit too much over-time variation.

```{r loadmodels,include=FALSE}

if(run_all) {
  
unemp1_fit <- readRDS('data/1151_12_1_fit.rds')
unemp2_fit <- readRDS('data/unemp1151_12_2_fit.rds')
unemp3_fit <- readRDS('data/unemp1151_12_3_fit.rds')

unemp1_fitm <- readRDS('data/1152_12_1_fit.rds')
unemp2_fitm <- readRDS('data/unemp1152_12_2_fit.rds')
unemp3_fitm <- readRDS('data/unemp1152_12_3_fit.rds')

unemp_gp_fit <- readRDS('data/unemp115_1_12__gp_fit.rds')
unemp_ar_fit <- readRDS("data/unemp115_1_12_1_ar_fit.rds")
unemp_rw_fit <- readRDS("data/unemp115_1_12_1_rw_fit.rds")

unemp_gp_fitm <- readRDS('data/unemp115_2_12__gp_fit.rds')
unemp_ar_fitm <- readRDS("data/unemp115_2_12_1_ar_fit.rds")
unemp_rw_fitm <- readRDS("data/unemp115_2_12_1_rw_fit.rds")

rhats1 <- id_plot_rhats(unemp1_fit) + ggtitle("Spline 2nd Degree") + labs(caption="",x="") + xlim(c(0.95, 2))

saveRDS(rhats1, "data/rhats1.rds")

rhats2 <- id_plot_rhats(unemp2_fit) + ggtitle("Spline 3rd Degree") + labs(caption="",y="",x="") + xlim(c(0.95, 2))

saveRDS(rhats2, "data/rhats2.rds")

rhats3 <- id_plot_rhats(unemp3_fit) + ggtitle("Spline 4th Degree") + labs(caption="",y="",x="") + xlim(c(0.95, 2))

saveRDS(rhats3, "data/rhats3.rds")

rhatgp <- id_plot_rhats(unemp_gp_fit) + ggtitle("Gaussian Process") + labs(caption="") + xlim(c(0.95, 2))

saveRDS(rhatgp, "data/rhatgp.rds")

rhatar <- id_plot_rhats(unemp_ar_fit) + ggtitle("AR(1)") + labs(caption="",y="") + xlim(c(0.95, 2))

saveRDS(rhatar, "data/rhatar.rds")

rhatrw <- id_plot_rhats(unemp_rw_fit) + ggtitle("Random Walk") + labs(caption="",y="") + xlim(c(0.95, 2))

saveRDS(rhatrw, "data/rhatrw.rds")

un1 <- id_plot_legis_dyn(unemp1_fit,use_ci=F,plot_text = F,person_line_alpha = 0.1) + scale_color_manual(values=c(R="red",
                              D="blue"),na.translate=F,name="") + ggtitle("Spline of 2nd Degree") + labs(y="") +
  scale_x_date(guide = guide_axis(n.dodge = 2))
  
  saveRDS(un1, "data/un1.rds")

un2 <- id_plot_legis_dyn(unemp2_fit,use_ci=F,plot_text = F,person_line_alpha = 0.1) + scale_color_manual(values=c(R="red",
                              D="blue"),na.translate=F,name="") + ggtitle("Spline of 3rd Degree") + labs(y="") +
  scale_x_date(guide = guide_axis(n.dodge = 2))

  saveRDS(un2, "data/un2.rds")

un3 <- id_plot_legis_dyn(unemp3_fit,use_ci=F,plot_text = F,person_line_alpha = 0.1) + scale_color_manual(values=c(R="red",
                              D="blue"),na.translate=F,name="") + ggtitle("Spline of 4th Degree") + labs(y="") +
  scale_x_date(guide = guide_axis(n.dodge = 2))

  saveRDS(un3, "data/un3.rds")

gp <- id_plot_legis_dyn(unemp_gp_fit,use_ci=F,plot_text = F,person_line_alpha = 0.1) + scale_color_manual(values=c(R="red",
                              D="blue"),na.translate=F,name="") + ggtitle("Gaussian Process") + labs(y="") +
  scale_x_date(guide = guide_axis(n.dodge = 2))

  saveRDS(gp, "data/gp.rds")

ar <- id_plot_legis_dyn(unemp_ar_fit,use_ci=F,plot_text = F,person_line_alpha = 0.1) + scale_color_manual(values=c(R="red",
                              D="blue"),na.translate=F,name="") + ggtitle("AR(1) Process") + labs(y="") +
  scale_x_date(guide = guide_axis(n.dodge = 2))

  saveRDS(ar, "data/ar.rds")

rw <- id_plot_legis_dyn(unemp_rw_fit,use_ci=F,plot_text = F,person_line_alpha = 0.1) + scale_color_manual(values=c(R="red",
                              D="blue"),na.translate=F,name="") + ggtitle("Random Walk") +
  scale_x_date(guide = guide_axis(n.dodge = 2)) + labs(y="")

  saveRDS(rw, "data/rw.rds")
  
un1m <- id_plot_legis_dyn(unemp1_fitm,use_ci=F,plot_text = F,person_line_alpha = 0.1) + scale_color_manual(values=c(R="red",
                              D="blue"),na.translate=F,name="") + ggtitle("Spline of 2nd Degree") + labs(y="") +
  scale_x_date(guide = guide_axis(n.dodge = 2))
  
  saveRDS(un1m, "data/un1m.rds")

un2m <- id_plot_legis_dyn(unemp2_fitm,use_ci=F,plot_text = F,person_line_alpha = 0.1) + scale_color_manual(values=c(R="red",
                              D="blue"),na.translate=F,name="") + ggtitle("Spline of 3rd Degree") + labs(y="") +
  scale_x_date(guide = guide_axis(n.dodge = 2))

  saveRDS(un2m, "data/un2m.rds")

un3m <- id_plot_legis_dyn(unemp3_fitm,use_ci=F,plot_text = F,person_line_alpha = 0.1) + scale_color_manual(values=c(R="red",
                              D="blue"),na.translate=F,name="") + ggtitle("Spline of 4th Degree") + labs(y="") +
  scale_x_date(guide = guide_axis(n.dodge = 2))

  saveRDS(un3m, "data/un3m.rds")

gpm <- id_plot_legis_dyn(unemp_gp_fitm,use_ci=F,plot_text = F,person_line_alpha = 0.1) + scale_color_manual(values=c(R="red",
                              D="blue"),na.translate=F,name="") + ggtitle("Gaussian Process") + labs(y="") +
  scale_x_date(guide = guide_axis(n.dodge = 2))

  saveRDS(gpm, "data/gpm.rds")

arm <- id_plot_legis_dyn(unemp_ar_fitm,use_ci=F,plot_text = F,person_line_alpha = 0.1) + scale_color_manual(values=c(R="red",
                              D="blue"),na.translate=F,name="") + ggtitle("AR(1) Process") + labs(y="") +
  scale_x_date(guide = guide_axis(n.dodge = 2))

  saveRDS(arm, "data/arm.rds")

rwm <- id_plot_legis_dyn(unemp_rw_fitm,use_ci=F,plot_text = F,person_line_alpha = 0.1) + scale_color_manual(values=c(R="red",
                              D="blue"),na.translate=F,name="") + ggtitle("Random Walk") +
  scale_x_date(guide = guide_axis(n.dodge = 2)) + labs(y="")

  saveRDS(rwm, "data/rwm.rds")
  
un1_a <- id_plot_legis_dyn(unemp1_fit,include="AMASH, Justin",
                         use_ci=T,plot_text = F) + scale_color_manual(values=c(R="red",
                              D="blue"),na.translate=F,name="") + ggtitle("Spline of 2nd Degree") + labs(y="") +
  scale_x_date(guide = guide_axis(n.dodge = 2))
  
  saveRDS(un1_a, "data/un1_a.rds")

un2_a <- id_plot_legis_dyn(unemp2_fit,use_ci=T,include="AMASH, Justin",plot_text = F) + scale_color_manual(values=c(R="red",
                              D="blue"),na.translate=F,name="") + ggtitle("Spline of 3rd Degree") + labs(y="") +
  scale_x_date(guide = guide_axis(n.dodge = 2))
 
  saveRDS(un2_a, "data/un2_a.rds")

un3_a <- id_plot_legis_dyn(unemp3_fit,use_ci=T,include="AMASH, Justin",plot_text = F) + scale_color_manual(values=c(R="red",
                              D="blue"),na.translate=F,name="") + ggtitle("Spline of 4th Degree") + labs(y="") +
  scale_x_date(guide = guide_axis(n.dodge = 2))
  
  saveRDS(un3_a, "data/un3_a.rds")

gp_a <- id_plot_legis_dyn(unemp_gp_fit,use_ci=T,include="AMASH, Justin",plot_text = F) + scale_color_manual(values=c(R="red",
                              D="blue"),na.translate=F,name="") + ggtitle("Gaussian Process") + labs(y="") +
  scale_x_date(guide = guide_axis(n.dodge = 2))

  saveRDS(gp_a, "data/gp_a.rds")

ar_a <- id_plot_legis_dyn(unemp_ar_fit,use_ci=T,include="AMASH, Justin",plot_text = F) + scale_color_manual(values=c(R="red",
                              D="blue"),na.translate=F,name="") + ggtitle("AR(1) Process") + labs(y="") +
  scale_x_date(guide = guide_axis(n.dodge = 2))

  saveRDS(ar_a, "data/ar_a.rds")

rw_a <- id_plot_legis_dyn(unemp_rw_fit,use_ci=T,include="AMASH, Justin",plot_text = F) + scale_color_manual(values=c(R="red",
                              D="blue"),na.translate=F,name="") + ggtitle("Random Walk") +
  scale_x_date(guide = guide_axis(n.dodge = 2)) + labs(y="")

  saveRDS(rw_a, "data/rw_a.rds")
  
  un1_am <- id_plot_legis_dyn(unemp1_fitm,include="AMASH, Justin",
                         use_ci=T,plot_text = F) + scale_color_manual(values=c(R="red",
                              D="blue"),na.translate=F,name="") + ggtitle("Spline of 2nd Degree") + labs(y="") +
  scale_x_date(guide = guide_axis(n.dodge = 2))
  
  saveRDS(un1_am, "data/un1_am.rds")

un2_am <- id_plot_legis_dyn(unemp2_fitm,use_ci=T,include="AMASH, Justin",plot_text = F) + scale_color_manual(values=c(R="red",
                              D="blue"),na.translate=F,name="") + ggtitle("Spline of 3rd Degree") + labs(y="") +
  scale_x_date(guide = guide_axis(n.dodge = 2))
 
  saveRDS(un2_am, "data/un2_am.rds")

un3_am <- id_plot_legis_dyn(unemp3_fitm,use_ci=T,include="AMASH, Justin",plot_text = F) + scale_color_manual(values=c(R="red",
                              D="blue"),na.translate=F,name="") + ggtitle("Spline of 4th Degree") + labs(y="") +
  scale_x_date(guide = guide_axis(n.dodge = 2))
  
  saveRDS(un3_am, "data/un3_am.rds")

gp_am <- id_plot_legis_dyn(unemp_gp_fitm,use_ci=T,include="AMASH, Justin",plot_text = F) + scale_color_manual(values=c(R="red",
                              D="blue"),na.translate=F,name="") + ggtitle("Gaussian Process") + labs(y="") +
  scale_x_date(guide = guide_axis(n.dodge = 2))

  saveRDS(gp_am, "data/gp_am.rds")

ar_am <- id_plot_legis_dyn(unemp_ar_fitm,use_ci=T,include="AMASH, Justin",plot_text = F) + scale_color_manual(values=c(R="red",
                              D="blue"),na.translate=F,name="") + ggtitle("AR(1) Process") + labs(y="") +
  scale_x_date(guide = guide_axis(n.dodge = 2))

  saveRDS(ar_am, "data/ar_am.rds")

rw_am <- id_plot_legis_dyn(unemp_rw_fitm,use_ci=T,include="AMASH, Justin",plot_text = F) + scale_color_manual(values=c(R="red",
                              D="blue"),na.translate=F,name="") + ggtitle("Random Walk") +
  scale_x_date(guide = guide_axis(n.dodge = 2)) + labs(y="")

  saveRDS(rw_am, "data/rw_am.rds")
  
# Now do Tim Walz  
  
un1_tw <- id_plot_legis_dyn(unemp1_fit,include="WALZ, Tim",
                         use_ci=T,plot_text = F) + scale_color_manual(values=c(R="red",
                              D="blue"),na.translate=F,name="") + ggtitle("Spline of 2nd Degree") + labs(y="") +
  scale_x_date(guide = guide_axis(n.dodge = 2))
  
  saveRDS(un1_tw, "data/un1_tw.rds")

un2_tw <- id_plot_legis_dyn(unemp2_fit,use_ci=T,include="WALZ, Tim",plot_text = F) + scale_color_manual(values=c(R="red",
                              D="blue"),na.translate=F,name="") + ggtitle("Spline of 3rd Degree") + labs(y="") +
  scale_x_date(guide = guide_axis(n.dodge = 2))
 
  saveRDS(un2_tw, "data/un2_tw.rds")

un3_tw <- id_plot_legis_dyn(unemp3_fit,use_ci=T,include="WALZ, Tim",plot_text = F) + scale_color_manual(values=c(R="red",
                              D="blue"),na.translate=F,name="") + ggtitle("Spline of 4th Degree") + labs(y="") +
  scale_x_date(guide = guide_axis(n.dodge = 2))
  
  saveRDS(un3_tw, "data/un3_tw.rds")

gp_tw <- id_plot_legis_dyn(unemp_gp_fit,use_ci=T,include="WALZ, Tim",plot_text = F) + scale_color_manual(values=c(R="red",
                              D="blue"),na.translate=F,name="") + ggtitle("Gaussian Process") + labs(y="") +
  scale_x_date(guide = guide_axis(n.dodge = 2))

  saveRDS(gp_tw, "data/gp_tw.rds")

ar_tw <- id_plot_legis_dyn(unemp_ar_fit,use_ci=T,include="WALZ, Tim",plot_text = F) + scale_color_manual(values=c(R="red",
                              D="blue"),na.translate=F,name="") + ggtitle("AR(1) Process") + labs(y="") +
  scale_x_date(guide = guide_axis(n.dodge = 2))

  saveRDS(ar_tw, "data/ar_tw.rds")

rw_tw <- id_plot_legis_dyn(unemp_rw_fit,use_ci=T,include="WALZ, Tim",plot_text = F) + scale_color_manual(values=c(R="red",
                              D="blue"),na.translate=F,name="") + ggtitle("Random Walk") +
  scale_x_date(guide = guide_axis(n.dodge = 2)) + labs(y="")

  saveRDS(rw_tw, "data/rw_tw.rds")
  
# Tim Walz with missing data
  
un1_twm <- id_plot_legis_dyn(unemp1_fitm,include="WALZ, Tim",
                         use_ci=T,plot_text = F) + scale_color_manual(values=c(R="red",
                              D="blue"),na.translate=F,name="") + ggtitle("Spline of 2nd Degree") + labs(y="") +
  scale_x_date(guide = guide_axis(n.dodge = 2))
  
  saveRDS(un1_twm, "data/un1_twm.rds")

un2_twm <- id_plot_legis_dyn(unemp2_fitm,use_ci=T,include="WALZ, Tim",plot_text = F) + scale_color_manual(values=c(R="red",
                              D="blue"),na.translate=F,name="") + ggtitle("Spline of 3rd Degree") + labs(y="") +
  scale_x_date(guide = guide_axis(n.dodge = 2))
 
  saveRDS(un2_twm, "data/un2_twm.rds")

un3_twm <- id_plot_legis_dyn(unemp3_fitm,use_ci=T,include="WALZ, Tim",plot_text = F) + scale_color_manual(values=c(R="red",
                              D="blue"),na.translate=F,name="") + ggtitle("Spline of 4th Degree") + labs(y="") +
  scale_x_date(guide = guide_axis(n.dodge = 2))
  
  saveRDS(un3_twm, "data/un3_twm.rds")

gp_twm <- id_plot_legis_dyn(unemp_gp_fitm,use_ci=T,include="WALZ, Tim",plot_text = F) + scale_color_manual(values=c(R="red",
                              D="blue"),na.translate=F,name="") + ggtitle("Gaussian Process") + labs(y="") +
  scale_x_date(guide = guide_axis(n.dodge = 2))

  saveRDS(gp_twm, "data/gp_twm.rds")

ar_twm <- id_plot_legis_dyn(unemp_ar_fitm,use_ci=T,include="WALZ, Tim",plot_text = F) + scale_color_manual(values=c(R="red",
                              D="blue"),na.translate=F,name="") + ggtitle("AR(1) Process") + labs(y="") +
  scale_x_date(guide = guide_axis(n.dodge = 2))

  saveRDS(ar_twm, "data/ar_twm.rds")

rw_twm <- id_plot_legis_dyn(unemp_rw_fitm,use_ci=T,include="WALZ, Tim",plot_text = F) + scale_color_manual(values=c(R="red",
                              D="blue"),na.translate=F,name="") + ggtitle("Random Walk") +
  scale_x_date(guide = guide_axis(n.dodge = 2)) + labs(y="")

  saveRDS(rw_twm, "data/rw_twm.rds")
  
# need to export covariates, anything else we might need
  
s1_cov <- unemp1_fit@stan_samples$draws("legis_x") %>% 
  summarize_draws %>% 
  mutate(Model="Spline 2nd Degree",Missingness="Unmodeled")
s2_cov <- unemp2_fit@stan_samples$draws("legis_x") %>% 
  summarize_draws %>% 
  mutate(Model="Spline 3rd Degree",Missingness="Unmodeled")
s3_cov <- unemp3_fit@stan_samples$draws("legis_x") %>% 
  summarize_draws %>% 
  mutate(Model="Spline 4th Degree",Missingness="Unmodeled")
sgp_cov <- unemp_gp_fit@stan_samples$draws("legis_x") %>% 
  summarize_draws %>% 
  mutate(Model="Gaussian Process",Missingness="Unmodeled")
sar_cov <- unemp_ar_fit@stan_samples$draws("legis_x") %>% 
  summarize_draws %>% 
  mutate(Model="AR(1)",Missingness="Unmodeled")
srw_cov <- unemp_rw_fit@stan_samples$draws("legis_x") %>% 
  summarize_draws %>% 
  mutate(Model="Random Walk",Missingness="Unmodeled")

s1_covm <- unemp1_fitm@stan_samples$draws("legis_x") %>% 
  summarize_draws %>% 
  mutate(Model="Spline 2nd Degree",Missingness="Modeled")
s2_covm <- unemp2_fitm@stan_samples$draws("legis_x") %>% 
  summarize_draws %>% 
  mutate(Model="Spline 3rd Degree",Missingness="Modeled")
s3_covm <- unemp3_fitm@stan_samples$draws("legis_x") %>% 
  summarize_draws %>% 
  mutate(Model="Spline 4th Degree",Missingness="Modeled")
sgp_covm <- unemp_gp_fitm@stan_samples$draws("legis_x") %>% 
  summarize_draws %>% 
  mutate(Model="Gaussian Process",Missingness="Modeled")
sar_covm <- unemp_ar_fitm@stan_samples$draws("legis_x") %>% 
  summarize_draws %>% 
  mutate(Model="AR(1)",Missingness="Modeled")
srw_covm <- unemp_rw_fitm@stan_samples$draws("legis_x") %>% 
  summarize_draws %>% 
  mutate(Model="Random Walk",Missingness="Modeled")

cov_combined <- bind_rows(s1_cov, s2_cov, s3_cov,
                          sgp_cov, sar_cov, srw_cov,
                          s1_covm, s2_covm, s3_covm,
                          sgp_covm, sar_covm, srw_covm)

saveRDS(cov_combined, "data/cov_combined.rds")

rm(list=ls()[grepl(x=ls(),pattern = "unemp")])
  
  
}

```

I next examine the overall distributions of the ideal point scores over time for each model in @fig-alldist when employing only observed votes (i.e., ignoring absences and abstentions). To help with plotting these distributions, uncertainty intervals are removed and instead I plot the average of posterior draws for each legislator ideal point $\alpha_i$ separately by party. As can be seen, the different methods have differing interpretations of ideal point trajectories, although all models in general show relative stability over the monthly periods of the 115th Congress. The main difference is that the more complex models, including the AR(1), random walk, and GP, show much higher levels of over-time variation for some legislators. Based on the short time period of the 115th Congress, and the fact that we are attempting to study legislator ideology, we should be skeptical of these rapid trajectories as reflecting either noise in the data or other aspects of legislative behavior (such as the scheduling of votes) that are polluting our estimate of the latent trait. The spline-based models, by contrast, show change over time for some legislators but change is relatively smooth, i.e., ideology does not change rapidly from to rollcall to rollcall as we would expect.

```{r dists}
#| label: fig-alldist
#| fig-cap: "Comparison of Time-varying Ideal Point Methods for the 115th Congress with Observed Votes"
#| message: false
#| warning: false
#| fig-height: 6
#| fig-width: 6

  un1 <- readRDS("data/un1.rds")
  un2 <- readRDS("data/un2.rds")
  un3 <- readRDS("data/un3.rds")
  rw <- readRDS("data/rw.rds")
  ar <- readRDS("data/ar.rds")
  gp <- readRDS("data/gp.rds")

un1 + un2 + un3 + ar + gp + rw + plot_layout(nrow=2,
                                             guides="collect") + 
  plot_annotation(caption="Posterior average values shown for 115th Congress legislator ideal points.\nItem discrimination for Republican party-line votes was constrained to be positive.") & theme(legend.position = "top",
                                                                                                                                                                                                    plot.title=element_text(size=9))


```

```{r distsamash1}
#| label: fig-amashdist
#| fig-cap: "Comparison of Justin Amash Time-varying Ideal Points for the 115th Congress with Observed Votes"
#| message: false
#| warning: false
#| fig-height: 6
#| fig-width: 6

  un1_a <- readRDS("data/un1_a.rds")
  un2_a <- readRDS("data/un2_a.rds")
  un3_a <- readRDS("data/un3_a.rds")
  gp_a <- readRDS("data/gp_a.rds")
  rw_a <- readRDS("data/rw_a.rds")
  ar_a <- readRDS("data/ar_a.rds")

un1_a + un2_a + un3_a + ar_a + gp_a + rw_a + plot_layout(nrow=2,
                                             guides="collect") + 
  plot_annotation(caption="Posterior average values shown for 115th Congress legislator ideal points.\nItem discrimination for Republican party-line votes was constrained to be positive.") & theme(legend.position = "top",
                                                                                                                                                                                                    plot.title=element_text(size=9))

```

Even with the relative stability in @fig-alldist, there are some legislators whose ideal points do change over time. In @fig-amashdist, I plot the monthly ideal point trajectory for Justin Amash, a Republican Congressman who famously became embroiled in disputes with then-President Donald Trump over foreign policy. As @fig-amashdist shows, Justin Amash shows strong movement in a positive or conservative direction in the middle of the Congressional session. This seems to correspond with his increasing disputes with the Trump White House over foreign policy in particular. Amash's criticism escalated after Trump had a closed door meeting with Russian president Vladimir Putin on July 16, 2016.[^4] Per the plots in @fig-amashdist, this event coincided with noticeable movement in a conservative direction. All of the models show this movement, although the splines show a more gradual movement, as might be expected, while the more complex time series like GPs show a more sudden jump. While there might be some reason to prefer the more complex time series models for Amash's trajectory, it is also true that the splines, despite being much simpler, are able to capture the approximate same trajectory while using fewer parameters and avoiding the implausible jumps from one month to the next.

[^4]: For more information, see \url{https://thehill.com/homenews/house/397787-gop-lawmaker-trump-went-out-of-his-way-to-appear-subordinate-at-putin-press/}.

```{r distscompwalz}
#| include: false


  un1_tw <- readRDS("data/un1_tw.rds")
  un2_tw <- readRDS("data/un2_tw.rds")
  un3_tw <- readRDS("data/un3_tw.rds")
  gp_tw <- readRDS("data/gp_tw.rds")
  rw_tw <- readRDS("data/rw_tw.rds")
  ar_tw <- readRDS("data/ar_tw.rds")



```

```{r distsamash2}
#| label: fig-amashdist1
#| fig-cap: "Comparison of Justin Amash Time-varying Ideal Points for the 115th Congress with Missing Votes"
#| message: false
#| warning: false
#| fig-height: 6
#| fig-width: 6
#| include: false

  un1_am <- readRDS("data/un1_am.rds")
  un2_am <- readRDS("data/un2_am.rds")
  un3_am <- readRDS("data/un3_am.rds")
  gp_am <- readRDS("data/gp_am.rds")
  rw_am <- readRDS("data/rw_am.rds")
  ar_am <- readRDS("data/ar_am.rds")

un1_am + un2_am + un3_am + ar_am + gp_am + rw_am + plot_layout(nrow=2,
                                             guides="collect") + 
  plot_annotation(caption="Posterior average values shown for 115th Congress legislator ideal points.\nItem discrimination for Republican party-line votes was constrained to be positive.") & theme(legend.position = "top",
                                                                                                                                                                                                    plot.title=element_text(size=9))

```

We can next turn to estimation of legislator ideal points for the 115th Congress when incorporating strategic abstention. To provide context of abstention in the U.S. House, Figures 3 and 4 provide descriptive information about absence rates in the U.S. House from 1990 to 2018 measured for each rollcall vote. The figures reveal that while absence rates are stable over time, they tend to increase towards the end of the legislative session. This within-session variation in absence rates implies that a time-varying model that incorporates this missing data mechanism—i.e. to discover whether legislators tend to show up for rollcall votes that are more or less polarized—could likewise capture new insights into the study of strategic legislative absence [@cloléry2023; @battaglini2023; @rothenberg2000].

@fig-alldist2 shows that there is justification for this belief in the 115th Congress. The spline models show interesting heterogeneity at the end of the session. In general, the more complex models show even more over-time variation, though the movements are so large as to be implausible and are another reason that simpler splines should be preferred given the sparsity of the data. There is also some evidence of increasing overlap in the distributions between parties at the end of the session which is not apparent in the observed data distribution in @fig-alldist. This could be evidence that at the end of the session there is increased opportunity for deal-making.

We can examine then-Representative Tim Walz, who in 2018 announced his run to be governor of Minnesota, as a legislator whose absences may have been strategic. I first plot in @fig-walzdist1 the monthly ideal points for Walz with only observed data and in @fig-walzdist2 the ideal points for Walz when incorporating a missing-data adjustment. In a similar manner to Amash, Walz shows a noticeable change in ideal point trajectory towards the end of the session, but in this case the movement is only apparent in the missing-data model in @fig-walzdist2. Furthermore, the models in @fig-walzdist2 show his ideal point moving in a clear negative or liberal direction. Once it became more costly for Walz to show up to vote, his pattern of voting became more similar to those who were on the left wing of the party. This behavior could correspond to Walz's need to appear more liberal in order to win the Democratic primary in the gubernatorial race, which occurred on August 15, 2018.

```{r dists2}
#| label: fig-alldist2
#| fig-cap: "Comparison of Time-varying Ideal Point Methods for the 115th Congress with Missing Votes"
#| message: false
#| warning: false
#| fig-height: 6
#| fig-width: 6

  un1m <- readRDS("data/un1m.rds")
  un2m <- readRDS("data/un2m.rds")
  un3m <- readRDS("data/un3m.rds")
  rwm <- readRDS("data/rwm.rds")
  arm <- readRDS("data/arm.rds")
  gpm <- readRDS("data/gpm.rds")

un1m + un2m + un3m + arm + gpm + rwm + plot_layout(nrow=2,
                                             guides="collect") + 
  plot_annotation(caption="Posterior average values shown for 115th Congress legislator ideal points.\nItem discrimination for Republican party-line votes was constrained to be positive.") & theme(legend.position = "top",
                                                                                                                                                                                                    plot.title=element_text(size=9))

```

```{r distswalz1}
#| label: fig-walzdist1
#| fig-cap: "Comparison of Tim Walz Time-varying Ideal Points for the 115th Congress with Observed Votes"
#| message: false
#| warning: false
#| fig-height: 6
#| fig-width: 6

  un1_tw <- readRDS("data/un1_tw.rds")
  un2_tw <- readRDS("data/un2_tw.rds")
  un3_tw <- readRDS("data/un3_tw.rds")
  gp_tw <- readRDS("data/gp_tw.rds")
  rw_tw <- readRDS("data/rw_tw.rds")
  ar_tw <- readRDS("data/ar_tw.rds")

un1_tw + un2_tw + un3_tw + ar_tw + gp_tw + rw_tw + plot_layout(nrow=2,
                                             guides="collect") + 
  plot_annotation(caption="Posterior average values shown for 115th Congress legislator ideal points.\nItem discrimination for Republican party-line votes was constrained to be positive.") & theme(legend.position = "top",
                                                                                                                                                                                                    plot.title=element_text(size=9))

```

```{r distswalz2}
#| label: fig-walzdist2
#| fig-cap: "Comparison of Tim Walz Time-varying Ideal Points for the 115th Congress with Missing Votes"
#| message: false
#| warning: false
#| fig-height: 6
#| fig-width: 6

  un1_twm <- readRDS("data/un1_twm.rds")
  un2_twm <- readRDS("data/un2_twm.rds")
  un3_twm <- readRDS("data/un3_twm.rds")
  gp_twm <- readRDS("data/gp_twm.rds")
  rw_twm <- readRDS("data/rw_twm.rds")
  ar_twm <- readRDS("data/ar_twm.rds")

un1_twm + un2_twm + un3_twm + ar_twm + gp_twm + rw_twm + plot_layout(nrow=2,
                                             guides="collect") + 
  plot_annotation(caption="Posterior average values shown for 115th Congress legislator ideal points.\nItem discrimination for Republican party-line votes was constrained to be positive.") & theme(legend.position = "top",
                                                                                                                                                                                                    plot.title=element_text(size=9))

```

I next look at the coefficients $\phi$ that relate the legislator-level covariates to the ideal points scores. These values are shown in @fig-coef for the interaction between membership in the GOP and the monthly unemployment rates. As can be seen, the constituent term for Unemployment, which is equivalent to the effect of Unemployment for Democrats, is consistently positive while the effect for the GOP X Unemployment interaction term has the opposite sign and approximately equal magnitude. Substantively, this means that Democratic legislators exhibit de-polarizing behavior when district-level unemployment rates rise because negative values of the scale indicate liberal ideology. For Republican legislators, however, the relationship is a null relationship. Because the constituent term for Unemployment and the interaction term are of equal and opposite signs, the combined effect is equal to zero for GOP legislators [@brambor2006].

Interestingly, the coefficients are much larger for the spline-based models, which shows another drawback of using time-series models that are over-parameterized. It would appear that the more complex models may have absorbed the unemployment variation, implying that the legislator ideal points changed solely due to autocorrelation rather than in reference to district-level unemployment. Again, this artifact of time series processes is why a model needs to be chosen that accords with theoretical priors about the latent trait as it can have downstream consequences for inference.

```{r legisx}
#| label: fig-coef
#| tbl-cap: Coefficients of GOP and District Unemployment Interaction by Model Type ($\phi$)
#| cache: true

library(tinytable)
library(dplyr)

readRDS("data/cov_combined.rds") %>% 
  bind_rows %>% 
  mutate(variable=recode(variable,
                         `legis_x[1]`="Unemployment",
                         `legis_x[2]`="GOP",
                         `legis_x[3]`="UnemploymentXGOP")) %>% 
  filter(variable %in% c("Unemployment",
                         "UnemploymentXGOP")) %>% 
  select(Model,Variable="variable",
         Missingness,
         `Posterior Median`="median",
         `5% Quantile`="q5",
         `95% Quantile`="q95") %>% 
  ggplot(aes(y=`Posterior Median`,x=Model)) +
  geom_pointrange(aes(ymin=`5% Quantile`, 
                      ymax=`95% Quantile`,
                      linetype=Missingness),
                  position=position_dodge(0.5),colour="black") +
  facet_wrap(~Variable,nrow=2) +
  geom_hline(yintercept=0,linetype=3,colour="black") +
  ggthemes::theme_clean() +
  labs(caption=stringr::str_wrap("Plot shows hierarchical covariates that predict ideal points. Estimates are organized by the type of latent time process and by whether missing data was incorporated into the model. Coefficient estimates are available as a table in the supplementary information.",width=55)) +
  coord_flip()
  # arrange(Variable,Model) %>% 
  # filter(Variable %in% c("Unemployment",
  #                        "GOP",
  #                        "UnemploymentXGOP")) %>% 
  # tt(digits=3,
  #    notes="Plot shows hierarchical legislator-levle covariates for different ideal point models, including time functions and whether or not the model accounted for missingness.") %>% 
  # style_tt(i=1:36,
  #          fontsize=.7)

```

We can next examine the ideal point marginal effects of unemployment to better interpret the association in the context of the outcome. Given that we are including an interaction, we need to consider the marginal effect of unemployment for Democrats and Republicans separately. I show item-level marginal effects in @fig-allplot from the second-degree spline model with observed data given the minimal differences across the different spline models for this covariate. Each rollcall vote in @fig-allplot is colored by its relative level of discrimination $\gamma_j$, where higher values signify more polarizing votes.

As can be seen, we do see a relationship between district-level unemployment and legislator voting in the 115th Congress--but only for Democratic legislators. For bills that are the most polarizing in the liberal (negative) direction, the probability of a Democratic legislator voting for one of these bills decreases by approximately 8% as district unemployment increases by 1%. Because item (vote) discrimination does not need to be symmetric, the relationship is slightly stronger for very conservative (positive) votes: a 1% increase in district unemployment is associated with an approximately 10% increase in the probability of voting for Democrats. Again, for Republicans, there does not appear to be any association as the two coefficients for Unemployment cancel out for this subgroup, resulting in zero marginal change.

```{r loadmarg}
#| include: false

by_party <- readRDS("data/by_party.rds")

```

```{r calcmargeffs}
#| label: fig-allplot
#| fig-cap: "Item-level Ideal Point Marginal Effects of Monthly District Unemployment on Legislator Ideal Points"
#| out-width: 100%
#| fig-height: 6

c1 <-   by_party %>% 
    mutate(group_id=factor(group_id,levels=c("D","R"),
                           labels=c("Democrats", "Republicans"))) %>% 
    #filter(!(item_orig %in% c("115_1050","115_588"))) %>% 
    ggplot(aes(y=mean_est,
               x=reorder(item_id,mean_est))) +
    geom_linerange(aes(ymin=low_est,
                       ymax=high_est,
                       colour=`median`)) +
    facet_wrap(~group_id) +
    scale_colour_viridis_c(name="Discrimination") +
    coord_flip() +
    labs(y="Marginal Change in Probability of Voting",
         x="Rollcalls",
         caption="Marginal effect of unemployment on voting on a specific rollcall in the 115th Congress.\nEach rollcall uncertainty interval is colored by the average posterior discrimination of that rollcall.\nMore positive rollcalls tend to be more conservative votes,\nwhile more negative rollcalls tend to be more liberal votes.") +
    geom_hline(yintercept=0,linetype=2) +
    ggthemes::theme_tufte() +
    theme(axis.text.y=element_blank(),
          axis.ticks.y=element_blank(),
          axis.line = element_blank(),
          panel.grid=element_blank(),
          legend.position = "bottom")

ggsave("idealpt_marg_eff.png",plot=c1,width=6,height=4)
  
print(c1)
```

```{r calcmargeffsp,include=FALSE, eval=FALSE}
#| label: fig-allplotp
#| fig-cap: "Comparison of Item-level Ideal Point Marginal Effects of Unemployment on Legislator Ideal Points and Party Moderation"

by_party %>% 
    ungroup %>% 
    mutate(group_id=factor(group_id,levels=c("D","R"),
                           labels=c("Democrats", "Republicans")),
           item_rank=rank(mean_est)) %>% 
    filter(!(item_orig %in% c("115_1050","115_588"))) %>% 
    ggplot(aes(y=mean_est,
               x=item_rank)) +
    geom_ribbon(aes(ymin=low_est,
                    ymax=high_est,
                    fill=group_id),alpha=0.5) +
    #facet_wrap(~group_id) +
    ggthemes::theme_tufte() + 
    scale_fill_manual(values=c(Republicans="red",
                              Democrats="blue"),name="") +
    coord_flip() +
    labs(y="Marginal Change in Probability of Voting",
         x="Rollcalls") +
    geom_hline(yintercept=0,linetype=2) +
    ggdark::dark_mode() +
    theme(axis.text.y=element_blank(),
          axis.ticks.y=element_blank()) +
    ggtitle("Marginal Effect of District Monthly Unemployment on Rollcall Votes in 115th Congress",
            subtitle="Marginal Effect of Unemployment Mediated by Legislator Ideal Point and Bill Discrimination")

```

Overall, the results of these models do show that legislative behavior changes during times of higher local unemployment. Furthermore, we can measure these movements in ideal points down to the monthly-level, permitting very precise statements about how legislative behavior changes even if we do not have a causally-identified design at present. In general, the association of a 1% change in district-level unemployment with legislator ideal points are modest, though we would expect modest effects as this 1% change represents one-month's worth of unemployment data. Aggregated over time, these associations could grow to be substantial.

## Full Trajectories of U.S. House from 1990 to 2018

In this section, I apply the spline-based models to all House sessions from 1990 to 2018. The complete distributions for all legislators for both observed-data and missing-data models are shown in @fig-fulldist. These plots show a similar stability of over-time trends comparable to the much shorter time series in @fig-alldist, which indicates that Congressional ideal points do not change very rapidly. There does not appear to be significantly more or less movement as the degree of the spline increases. There is some additional movement in the missing-data models, though not of a very extreme nature.

![Monthly Ideal Points for U.S. House Legislators from 1990 to 2018](full_dist.pdf){#fig-fulldist}

In @fig-sumbigdist I show the party-level ideal points for both Democrats and Republicans over the same time period. To derive these scores, the individual-level ideal point trajectories are first aggregated to the group level, and then averages are taken of all posterior draws. This method preserves uncertainty in the posterior expectation, although it the uncertainty intervals are not shown to make the differences between models more apparent.

```{r}
#| fig-cap: Monthly Party-level Ideal Points for the U.S. House
#| label: fig-sumbigdist
#| fig-height: 5
#| fig-width: 6

# load all these distributions, plot them 

all_group_big <- lapply(list.files("data/","idealpts",full.names = T), 
                        readRDS) %>% 
  bind_rows(.id="model") %>% 
  mutate(Spline=case_match(as.character(model),
                          c("1","2")~"2°",
                          c("3","4")~"3°",
                          c("5","6")~"4°"),
         Type=case_match(model,
                          c("2","4","6")~"With Abstentions",
                          c("1","3","5")~"Only Observed\nVotes"))

all_group_big %>% 
  ggplot(aes(y=mean_est,x=Time_Point)) +
  geom_line(aes(linetype=Spline,colour=Group)) +
  facet_wrap(~Type,nrow=2) + 
  ggthemes::theme_clean() +
  scale_color_manual(values=c(R="red",
                              D="blue"),na.translate=F,name="") + 
  guides(color="none",
         linetype=guide_legend(override.aes = list(color = "black"))) + 
  scale_y_continuous(labels=c("More\nLiberal","-5","0","5","More\nConservative"),breaks=c(-10,-5,0,5,10)) +
  labs(y="Ideal Point",x="",
       caption=stringr::str_wrap("Plot shows party-average monthly ideal points for the U.S. House from 1990 to 2018 that are aggregated from individual legislator trajectories. Ideal point is derived from a Bayesian IRT model fit with idealstan. Missing-data models account for strategic abstention while observed models only use recorded votes. Each line is an estimate from a different type of spline model in terms of degrees with knots for each presidential administration. Discrete jumps in the time series show periods when new legislators entered.")) 



```

What is clear is that this plot departs noticeably from the DW-NOMINATE scores that generally show a GOP that is moving away from Democrats since 1990 [@carroll2009]. Instead, it would appear that Democrats were moving away from Republicans after 1990. This polarization reached a peak in 2012, and then moderated until the end of 2018. The missing-data models that incorporate strategic abstention show much greater moderation, which could be due to either increasing strategic abstention or the artificial end of the time series in 2018 (or a combination of both factors). However, despite these differences, both the observed data and missing-data models show a similar trajectory. Furthermore, there are very minor differences between the different spline models in terms of degrees and the flexibility of ideal points over time.

It is worth considering the differences between these scores and those of DW-NOMINATE. DW-NOMINATE party-level ideal points are based on legislator ideal points that vary linearly and are estimated once for each Congressional session. Furthermore, these ideal point trajectories are scaled to fit within a unit hypersphere for identification purposes [@carroll2009]. As such, there is considerable stretching of the ideal points to fit within these constraints. By contrast, in the idealstan model, constraints are placed on the item (vote) parameters while the ideal points are allowed to float with a weakly informative prior. As such, there is reason to believe that the trajectories in @fig-sumbigdist are closer to the actual political trajectories of the parties.

There are obviously many interesting hypotheses that can follow from these new party-level and individual-level measures for the U.S. House, and I leave these questions for scholars to pursue as these estimates will be available for further study.

```{=html}
<!--# 

The aim of this paper was to put forward a general framework for ideal point estimation that would both integrate existing approaches while providing new areas for analysis, particularly in missing data and over-time inference. The intention is not to suggest, however, that this framework represents all that can be done with ideal points. This paper's approach is general and designed to fit a wide variety of applications, but investigating the intricacies of social situations will always require more custom modeling. The utility of this study is rather to promote the use of ideal point models as a core part of empirical research in political science, not just for legislative studies or rollcall voting data.

The reason that ideal point models have wider applicability is because social choice processes where individuals decide between competing alternatives happen very often. If these alternatives are polarizing, or represent competing poles, than an ideal point model can help locate people's positions along the axis of competition while also determine the relative weight of the items they are choosing. The ultimate payoff is that this kind of modeling will help political scientists focus on inference on the constructs they care about, which are usually unobservable, such as partisanship, ethnic identity, corruption, and political ideology. Instead of being forced into arbitrary measurement decisions over the available indicators, political scientists can maximize the information in their data by collapsing multiple indicators to a single dimension.

As a result, this paper largely sidesteps the debate about how to determine when ideal points represent political ideology. Defining a the latent construct can only be done in the context of a specific empirical application. The political ideology interpretation is most useful for American politics where legislators have considerable latitude in voting behavior, but this does not mean that ideal point representations of other polities or social situations are not as useful. So long as a social choice process is involved, ideal point models can extract useful information from noisy data.  -->
```

# Conclusion

In this paper, I presented a generalization of the increasingly popular ideal point model in the Bayesian IRT framework. I extended the model with new modes of missing-data, time-varying processes, joint distributions, and quantities of interest. the contribution of this paper is an analytical tool that can extend the domain and applicability of ideal point models across the discipline as all of these models are available in a single R package, `idealstan`. Crucially, all of these methods build on each other so that missing data can be incorporated in time-varying models or in joint distributions and vice versa.

All of these models are available in an R package called `idealstan`. This package is designed to automate the sometimes arduous process of preparing data for ideal point modeling, including identifying parameters. Furthermore, the use of a single R package for all models enables researchers to compare different models on the same data with ease. Parallelization of standard Bayesian inference is also incorporated in the package to enable estimation of the ever larger data sets available to political scientists.

<!-- With the Chinese import shock data, we can consider the following hypothesis: -->

<!-- > H2: When monthly unemployment rates rise in districts with a higher exposure to Chinese import competition, legislators in each party become more polarized. -->

<!-- This hypothesis expresses a conditional relationship that runs in the opposite direction of the first hypothesis. It would seem that, as existing literature has shown, exposure to Chinese import shocks shifts legislators' ideal points on trade policy and increases polarization, that there should be a strongly interactive relationshp between district-level unemployment and Chinese import exposure. When both are high, we would have a strong prior that legislators will polarize away from each other. This hypothesis can be tested with a three-way interaction of party ID, unemployment rates and Chinese shock exposure at the district level. -->

<!-- Finally we can also consider the following hypothesis: -->

<!-- > H3: When monthly unemployment rates rise in districts with a higher exposure to Chinese import competition, legislators in each party become more anti-trade. -->

<!-- Instead of testing overall polarization as H2 proposes, we can see if the movement in ideal points is concentrated on bills on trade policy by adding an indicator for trade-related legislation to the ideal point model and interacting it with party ID, unemployment, and China import shock exposure. -->

# References
