---
title: 'Generalized Ideal Point Models for Robust Measurement with Dirty Data in the Social Sciences'
author: 
  - name: "Robert Kubinec"
    affiliation: New York University Abu Dhabi
    email: rmk7@nyu.edu
    corresponding: true
date: last-modified
execute: 
  cache: true
  echo: false
  warning: false
  error: false
date-format: long
format: pdf
linestretch: 1.5
abstract: 'This  paper presents a Bayesian version of the ideal point model that employs advances in computation and probabilistic programming to permit inference given non-ignorable missing data, diverse distributions, time-series processes, and hierarchical covariates. Furthermore, to better understand how external covariates affect ideal points, I put forward a new item-level quantity of interest, ideal point marginal effects, that are defined as the percent change in supporting an item (bill) for a one-unit change in a person-specific covariate. With this package, I provide the first monthly ideal point estimates for legislators in the U.S. House from 1990 to 2018 that include district-level unemployment as an external covariate. I further demonstrate the utility of the package with a study of voting behavior in the 115th Congress of the United States that reveals how the choice different time series processes affects inference and under what conditions abstention from votes is likely due to strategic behavior. All of these models are available via the R package `idealstan` with Hamiltonian Markov Chain Monte Carlo in Stan.'
bibliography: "BibTexDatabase.bib"
fig-cap-location: top
filters:
  - authors-block
---

```{r setup, include=FALSE}

require(dplyr)
require(ggplot2)
require(tidyr)
require(readr)
require(blscrapeR)
require(lubridate)
require(tigris)
require(sf)
require(areal)
require(missRanger)
require(idealstan)
require(forcats)
library(patchwork)
library(posterior)
library(marginaleffects)

# set to true to create all analyses from fitted idealstan models
# requires significant time and RAM (especially the latter)
run_all <- T

# see file to_cluster.R for the fitting of the actual idealstan models

# load the datas

rollcalls <- readRDS('data/rollcalls.rds')

unemp1 <- rollcalls %>% 
  select(cast_code,rollnumber,congress,
         bioname,party_code,date_month,unemp_rate) %>% 
  mutate(item=paste0(congress,"_",rollnumber),
         cast_code=recode_factor(cast_code,Abstention=NA_character_),
         cast_code=as.numeric(cast_code)-1,
         bioname=factor(bioname),
         unemp_rate=100*unemp_rate,
         bioname=relevel(bioname,"DeFAZIO, Peter Anthony")) %>% 
  distinct %>% 
  filter(party_code %in% c("R","D")) %>% 
  mutate(party_code=factor(party_code))

# drop legislators who vote on fewer than 25 unanimous bills

check_bills <- group_by(unemp1,item,party_code,cast_code) %>% count %>% 
  group_by(item,party_code) %>% 
  summarize(prop=n[cast_code==1] / sum(n),
            n_vote=sum(n)) %>% 
  ungroup %>% 
  mutate(prop=ifelse(prop==.5,
                          sample(c(.49,.51),1),
                     prop),
    util_func=(1 / sqrt((.5 - prop)^2))*n_vote) %>% 
  arrange(desc(util_func))

legis_count <- group_by(unemp1, item) %>% 
  mutate(unan=all(cast_code[!is.na(cast_code)]==1) || all(cast_code[!is.na(cast_code)]==0)) %>% 
  group_by(bioname) %>% 
  summarize(n_votes_nonunam=length(unique(item[!unan])))

# check number of days in legislature

num_days <- distinct(unemp1,bioname,date_month) %>% 
  count(bioname)

```

\newpage

This paper takes the well-established ideal point model from political science and generalizes to address long-standing issues while also extending the domain of the model to cover new areas of measurement with diverse and noisy data. The goal of this new implementation is to make the ideal point model a general tool for measurement of hidden concepts across the social sciences whether the origin of the data is from social media, surveys, legislatures, bureaucratic agencies, or social networks. To accomplish this task, the paper implements the ideal point model using Stan, a probabilistic programming language for Bayesian inference, that allows for a much wider range of time-series processes, data distributions, and novel ways of incorporating missing data. In addition, the open source R package which implements the models exploits advances in big data Bayesian computation with Stan to permit the robust estimation of latent concepts from much larger datasets than was previously possible.

While ideal point models were conceived as a way to measure ideology in the U.S. Congress, this powerful measurement tool has wide applicability as a general tool for analyzing social data with substantial measurement error. For example, political scientists have recently applied the canonical ideal point model to new datasets and empirical challenges, such as Twitter data [@barbera2015; @kubinec2018], the United Nations Security Council [@bailey2017], state capacity [@hanson2021], democracy [@coppedge2019; @pemstein2017], campaign finance data [@bonica2014] and legislative speech [@lauderdale2016]. Through these applications, it has become apparent that the ideal point model has utility for a considerable number of measurement problems in the social sciences so long as available data are imperfect proxies of the concept of interest. To encourage further usage of this model, and to help address difficult measurement problems across the social sciences, this paper takes advantage of recent advances in Bayesian computation to broaden the implementation of the standard ideal point model to address challenges related to non-ignorable missing data, robust inference for time-varying data, as well as new estimands to better understand the effect of external covariates on ideal points [@martin2002; @martin2011a; @barbera2015; @kropko2013; @carroll2009; @bafumi2005; @jackman2004; @imai2016a; @lewis2018; @goplerud2019; @caughey2018].

I show in this paper that combining advances in Bayesian computation with flexible time processes in the form of splines can solve a particularly difficult problem of scaling sparse data with measurement error, such as that available from finely grained time series in social media and other digital sources. While existing applications of dynamic ideal points are generally at a relatively high level of aggregation, such as the Congressional session [@carroll2009] or the judicial session [@martin2002], social media data like Twitter/X and Bluesky can be available in minute by minute increments [@kubinec2018; @eady2024], not to mention the wealth of revealed preference information available in TikTok, Youtube, and other sources. However, existing time-series methods for ideal points are difficult to employ with sparse data, and for that reason I present splines as a way of estimating restricted polynomials that can separate time trends from measurement noise. To apply the method, I estimate monthly time-varying ideal points for Congresspeople in the House in from 1990 to 2018, which are the first such granular measures available. I also compare multiple time-series processes, including random walks, AR(1), and Gaussian processes versus splines for the 115th Congress to show how modeling choices for time can lead to different conclusions about the nature of time-varying ideal point measures.

Finally, I also show in this paper how Bayesian post-estimation [@gill2020] enables the calculation of a new quantity of interest from ideal point models: *ideal point marginal effects*. I define an ideal point marginal effect as the unit change in the set of items for a unit change in a covariate that is mediated by a given person's ideal point. This quantity, which is estimated separately for each item (i.e., each bill or vote) in the data, enables us to understand how an external covariate can influence a person's decision-making while taking into account the relative latent position of the items on which they are taking positions. I illustrate this method with an analysis of the association between monthly district-level unemployment rates and monthly Congressional ideal points for both the full sample and for the 115th Congress.

All the models in this paper can be fit with the R package `idealstan`, which employs the probabilistic programming language Stan to allow for a wide variety of models to be fit within one joint framework. Furthermore, the application of Hamiltonian Monte Carlo and its associated diagnostics permits very robust inference of difficult posterior geometry, ensuring that measurement inferences are stable [@carpenter2017]. This implementation makes use of novel computational methods to parallelize MCMC calculations so that much bigger models can be fit with full Bayesian inference than was previously possible. As I discuss, even larger models can be fit with approximate inference methods, though only for limited applications. This new parameterization of ideal points that combines missing-data inference, time-varying inference and inference over joint distributions provides scholars with a multipurpose tool for tackling difficult measurement challenges in both traditional and nontraditional datasets.

# Latent Variables as Statistical Deduction

> "There is nothing more deceptive than an obvious fact."

> -- Sherlock Holmes

The traditional presentation of applied statistics in political science presents models as estimating unbiased quantities given a recipe list of datasets. Of course, the datasets that political scientists produce rarely fit these clean molds, which leads to numerous statistical fixes to correct for poorly-fitting models, such as zero-inflated Poisson models in the case of counts and other more generic fixes like clustered standard errors. What all of these approaches have in common is a focus on adjusting for measurement issues in the dependent variable, while the data are assumed to represent known facts.

Measurement models flip this perspective on its head. Rather than assuming that only the outcome needs a statistical distribution, a measurement model implements a statistical distribution *for the data*. Making this mental switch enables us to think of the data as only one set of observed indicators generated by a latent, unseen construct. In other words, the data are measured *with error*, where the exact errors are defined by a particular statistical distribution.

<!--# The simplest measurement model is the econometric errors-in-variables model where the data were generated by a Normal distribution with a pre-specified standard deviation [@durbin1954]. This model assumes that any noise in the data is evenly distributed across all the data points, and its only effect on the final statistical estimation is to increase the uncertainty of estimates. There are many more measurement models available, however, all of which can be combined with traditional statistical estimation techniques or employed as the primary model of interest.  -->

<!--# It is possible to define measurement models in terms of the structure they impose on the distribution underlying the data. As mentioned previously, the errors-in-variables model may be the simplest such distribution in which relatively little is known or assumed about noise in the data. Most other measurement models assume that the data are generated by a reduced-form expression. On one end of the spectrum, in cases where there is little prior information about what underlies the data, reduction methods like factor analysis [@harman1960], principal components analysis [@hotelling1933] and correspondence analysis [@greenacre2017; @barbera2015] can compress the columns of a data matrix into an eigenvalue-based summary of the variation within the matrix. This kind of information can be thought of a generalization of a correlation matrix of the data. -->

<!--# On the other end of the spectrum are methods that impose a very specific structure on the data distribution. Structural equation modeling is arguably the most demanding as researchers can stipulate a very precise process through which latent variables are related to each other, permitting complex unobserved feedback networks [@ullman2012]. There are other models involving latent variable analysis in time-series that impose a very specific functional form, such as the hidden Markov model [@rabiner1989] and the state-space model [@roesser1975], in which the data were generated by an unobserved time-varying construct. The number of latent variable models with a precise structure can be almost endless as it can even include two-stage models like the Heckman selection model [@heckman1985]. -->

The ideal point model, and its close kin, the item-response theory framework, fit somewhere in the middle in the spectrum of measurement models as they attempt to balance both open-ended exploration, as in the well-known factor analysis and PCA models [@harman1960; @hotelling1933; @barbera2015], with the ability to incorporate some of the theoretical structure of confirmatory methods [@ullman2012]. Formally, an ideal point model for any person $i$ and any vote $j$ is defined as the difference in utilities between the person's ideal point $\alpha_i$ and the vote's Yes position, $\beta_{jY}$ and No position, $\beta_{jN}$, such that $i$ always votes for the bill if:

$$
\sqrt{(\alpha_i - \beta_{jY})^2} > \sqrt{(\alpha_i - \beta_{jN})^2} 
$$

That is, if $\alpha_i$ is closer in Euclidean space to the Yes position than the No position. While @enelow198 originally defined this model in deterministic terms, most statistical applications add in vote-specific error terms to make the decision stochastic. The two main statistical parameterizations of this model, the DW-NOMINATE family [@poole2008] and the Bayesian IRT family [@jackman2004], differ primarily in the distribution they assign to that error term [@carroll2009]. While an argument can be made for the preference of one over the other [@carroll2013], or even to use a fully non-parametric estimator [@tahk2018], I concentrate on the IRT formulation due to its ease of implementation in a Bayesian framework and its connection to the IRT literature.

@jackman2004 showed that a certain IRT model, the 2-Pl family, was equivalent to the ideal point model if the actual Yes and No positions of the bill were left unidentified. Instead, this parameterization can estimate the ideal points $\alpha_i$ and a midpoint where the person $i$ is indifferent to voting on the bill $j$. The IRT 2-Pl model in terms of ideal points $\alpha_i$, or what are called ability parameters in the IRT literature, discrimination parameters $\gamma_j$ and difficulty parameters $\beta_j$, is as follows for a binary observed vote $Y_{ij}$:

$$
Pr(Y_{ij}=1) = \prod^{I}_{i=1} \prod^{J}_{j=1} logit^{-1}(\gamma_j \alpha_i - \beta_j)
$$ {#eq-basic}

The sign of the discrimination parameter $\gamma_j$ indicates the "polarity" of the bill/item, or whether voting yes indicates one is more liberal or conservative in the Congressional case. The absolute magnitude of the discrimination parameter indicates how well the bill/item discriminates between persons such that the latent dimension strongly predicts voting. The $\beta_j$ difficulty parameters, by contrast, have the relatively uninteresting interpretation in representing the propensity of a person to vote yes marginal or independent of the latent dimension, though their incorporation is important to allow for the residual propensity to vote Yes to vary by bill.

<!--# The crucial difference between this parameterization and the standard IRT model is that the discrimination parameters $\gamma_j$ are unconstrained. In an IRT 2-Pl model the discrimination parameters are normally constrained to be positive so that the ideal points/ability parameters $\alpha_i$ can have the interpretation of reflecting the ability of a students to answer correctly more difficult test questions. In other words, the observed indicators are always positively related to the latent trait. The ideal point model can be considered a generalization of the IRT 2-Pl model because it does not assume that this is the case and in fact can estimate the direction of items from the data. This difference, though it would seem minute, is important when analysts are uncertain of the directional relationship between the data and the latent trait, as often occurs in practice.  -->

The ability to allow for contrasting polarity at the item level does create some additional complications. The always-positive discrimination constraint in a standard IRT model ensures identification in terms of the rotation of the ideal points, so abandoning that assumption requires more work to identify the model. As has become well-known, constraints must be built in to the IRT ideal point estimation in order to identify a unique rotation of the ideal points [@gelman2005]. As I discuss, there is much less information about the identification of a unique rotation for time-varying ideal point models, which this paper will seek to provide additional guidance on.

It is important to note that there is nothing about this model that necessarily implies that the latent scale is political ideology. That is a useful interpretation to assign ideal points based on data from the U.S. Congress [@rosenthal2007]. The ideal point model represents a social choice process where the construct by which people make choices is unobserved. As such, it *can* estimate ideology as a latent construct, but it does not need to. Interpreting what the latent variable represents is a necessary part of any application of the model. What can be said is that it can flexibly represent the outcome of a variety of social processes with considerable flexibility.

In this paper I employ a Bayesian interpretation of this model [@jackman2004] that I estimate using Hamiltonian Markov Chain Monte Carlo with Stan [@carpenter2017]. I am interested in posterior inference on a set of unobserved parameters $\{\alpha_i,\gamma_j,\beta_j\} \in \theta$ that is conditional on the observed data $Y_{ij}$, or $Pr(\theta|Y_{ij})$. To do so, I define independent priors on $\theta$, $Pr(\theta)$, which is multiplied by the likelihood $L(\cdot)$ that is defined by @eq-Bayes:

$$
Pr(\theta|Y_{ij}) \propto Pr(\theta)L(Y_{ij}|\theta)
$$ {#eq-Bayes}

I use the proportional symbol $\propto$ to reflect the fact that the term in the denominator representing the probability of the data $Pr(Y_{ij})$ is normally omitted in computational Bayesian analysis as it is fixed with respect to the parameters [@gelman2013].

My presentation of the Bayesian ideal point model departs somewhat from conventional parameterizations in order to be more flexible and to incorporate recent advances in measurement modeling. I start by specifying the priors in terms of the distributions assigned to each parameter in $\theta$:

\begin{align}
\alpha_i &\sim \text{Normal}(0,3)\\
\gamma_j &\sim \text{GeneralizedBeta}(2,2)\\
\beta_j &\sim \text{Normal}(0,3)
\end{align}\label{eq-genprior}

The ideal points $\alpha_i$ and the discrimination parameters $\beta_j$ are given a loose $N(0,3)$ prior that is weakly informative for most scales and is only needed to make the posterior proper. Identification of the posterior, which is an important issue as these types of measurement models can have multiple possible solutions with equal likelihood [@bafumiPracticalIssuesImplementing2005], happens via the Generalized Beta prior on the discrimination parameters $\gamma_j$.

Constraining the discrimination parameters builds on work that shows that identification of the items (i.e., votes) in an ideal point model has superior theoretical properties as it allows for the ideal points to float freely, which are usually the main subject of interest in terms of measurement [@morucciMeasurementThatMatches2024]. Furthermore, in a manner similar to @pooleScalingRollCall2008, I implicitly scale the discrimination parameters by using the Generalized Beta distribution in which the distribution has support over the open interval $(-1,1)$. I assign values of +2 to each of the distribution's parameters (scale and shape) so that values of 0, or the midpoint of the distribution, are weakly preferred over the end points. Because this distribution is bounded, it will prevent the well-known scaling issues of ideal point models given that the latent estimates have no natural scale. With this prior, no further work is necessary to fix a scale and all other parameters can have weakly informative priors. In fact, if there is enough data, the discrimination parameters can have the uninformative Generalized Beta prior of +1, +1 so that all values are equally likely in the $(-1, 1)$ interval.

The effect $\phi$ of a matrix of covariates $X$ measured on the persons $i$ can also be added to this model for ideal points as shown in @gelman2005 and @jackman2004 by including them as hierarchical predictors:

$$
\alpha_i \sim N(X\phi',3)
$$ {#eq-hier}

I also consider the possibility of including hierarchical covariates to predict the discrimination of the $J$ items, which has not been estimated previously in the literature though it is straightforward in a Bayesian framework:

$$
\gamma_j \sim N(X\phi',1)
$$ {#eq-hieritem}

Given this definition of the model, I next show how this framework can incorporate missing data when not all of $Y_{ij}$ is observed, time-varying inference when $\alpha_i$ is measured at different points in time and the estimation of diverse distributions for $Y_{ij}$ whether singly or jointly.

## Missing Data

Missing data is a tricky problem to address in latent variable models because latent variables are themselves defined as missing data. For this reason, standard imputation techniques cannot be easily applied. Because ideal point models are representations of social aggregation processes, a straightforward approach of imputing data conditional on the model assumes that missingness is orthogonal to observed ideal points [@rubin2002]. In many situations, this assumption may not be realistic when there is reason to believe that social actors, such as legislators, are absent for reasons that are related to their own ideal points.

The most rigorous treatment of the problem of missing data and ideal points can be found in @rosas2015, who put forward a model in which non-ignorable missingness occurs when legislators disagree with the position of their party. Their analysis shows that missingness in ideal point models often reflects strategic behavior and it is unwise to treat it as either missing-at-random (MAR) or missing-conditionally-at-random (MCAR). In this section I define a model for non-ignorable missingness that aims for widespread applicability while taking into account the strategic nature of ideal point actors.

I extend my notation of the outcome $Y_{ij}$ by adding a subscript $r \in \{0,1\}$ for whether person $i$ chose to vote or answer item $j$ ($r=1$) or chose not to vote/answer ($r=0$). We can then add a separate selection model that first estimates $Pr(r=0)$ and deflates the likelihood $L(Y_{ijr}|\theta)$ accordingly:

$$
    L(Y_{ijr}|\alpha_i,\gamma_j,\beta_j,\nu_j,\omega_j) = 
    \prod^{I}_{i=1} \prod^{J}_{j=1}
    \begin{cases}
    \zeta(\alpha_{i}'\nu_j - \omega_j ) & \text{if } r=0, \text{ and} \\
    (1-\zeta({\alpha_{i}'\nu_j - \omega_j}))L(Y_{ijr}|\alpha_i,\gamma_j,\beta_j) & \text{if } r=1
    \end{cases}
$$ {#eq-inflate2}

I let $\zeta(\cdot)$ stand for the inverse logit function in the equation above. As can be seen, the selection model for missing data $\zeta(\alpha_i'\nu_j - \omega_j)$ is very similar to @eq-basic except that I have substituted a new set of discrimination $\nu_j$ and difficulty $\omega_j$ parameters. The ideal points $\alpha_i$ enter into both the selection model and the main ideal point model $L(Y_{ijr}|\alpha_i,\gamma_j,\beta_j)$. As such, the selection model is able to inflate or deflate the ideal points by taking into account a first stage process. The inclusion of $\nu_j$ and $\omega_j$ in the first-stage selection model creates a new "missingness" space where person $i$ first chooses whether she will cast a vote, send a tweet or answer a survey question, as the case may be. Only if the item is close to person $i$ in this missingness space will person $i$ then also decide to participate and provide information $Y_{ij}$ that is informative of their ideal point. This model can be interpreted as a censoring model where persons may choose to self-censor their ideal points.

The general form of the selection model--which is essentially a first-stage ideal point model--allows it to pick up a range of non-ignorable missingness patterns if missingness correlates with ideal points. The model does not make a priori assumptions about exactly why ideal points may determine missingness, and as such the model cannot provide such an interpretation a posteriori without taking time to interpret what the parameters of the selection model reveal about actors' intentions. The discrimination parameter $\nu_j$ is very helpful for this purpose because it will indicate which set of items show correlation between missingness and one of the poles of the latent variable. For example, if the latent variable is constrained positive for liberal Senators, then high positive discrimination would indicate that more liberal Senators tend to be absent on a particular bill. A naive model that assumed that absences were ignorable would miss this pattern and treat these Senators as more moderate based solely on their observed voting record.

This model is also general enough to allow for the possibility that data is actually missing at random at the item level. If the discrimination parameter $\nu_j$ is zero then ideal points do not enter into the equation for missingness and $Pr(r=0)$ is equal to the item-specific intercept $\omega_j$. In the legislative context, this would be the same as estimating the probability of absence by the proportion of legislators who do not show up for particular bill. Including this parameter will also separate that item-specific random missingness from missingness patterns suspiciously associated with ideal points. For this reason, the selection model will perform at least as well as a standard imputation method where the missing ideal points are MCAR conditional on each item.

While this missingness model is readily applicable to a legislative context where legislators may not want to show up on votes depending on what the vote would reveal about their ideal points, it is also useful for other non-ignorable missingness patterns. Twitter data is an excellent example. It is well-established that estimating a latent trait like political polarization will be vastly over-stated if the tendency of Twitter users to select who they choose to follow is not taken into account [@Barbera2015]. The two-stage selection model can be used to estimate the people's propensity to retweet ideological content net of their self-selection into whom they follow on Twitter [@kubinec2018].

## Time-varying Inference

While ideal points are usually conceived of as relatively time-invariant constructs, time-varying ideal point models have been available to researchers for quite some time. The most well-known time-varying version is the DW-NOMINATE model [@rosenthal2007; @armstrong2014; @Caughey2016], which employs a linear time trend to allow for limited over-time variation at the legislator level over time. The other existing approach is the random walk model of @quinn2002 in which ideal points in time $t$ are equal to ideal points in $t-1$ plus a Normally-distributed random jump. This much more flexible model is quite helpful at determining the probable trend in ideal points over time because it imposes virtually no structure on the time-series trends, although as I discuss below, it adds a considerable greater demand in terms of finding a unique identified solution. There are extensions of the random walk, such as employing a Student T's distribution to allow for more stable patterns over time [@reuning2019], though these approaches have the same difficulties with identification.

<!--# Both of these approaches share in common an emphasis measuring over-time trends. By contrast, in this paper I also want to consider effects of hierarchical predictors on ideal points that vary over time. Including such predictors would enable scholars to test much more precise questions about how time-varying covariates influence ideal points, especially as political scientists obtain ever-larger and more fine-grained datasets. However, existing methods cannot easily handle such covariates. The DW-NOMINATE approach is designed to produce accurate and relatively non-parametric estimates of legislator's ideal points versus inference on those trends, while the random-walk approach cannot include additional covariates without radically changing the model. Any covariate with a constant effect in a random-walk will push the time series in a constant direction over time (i.e., static drift). It is too simple of a model to separate the effect of a covariate from the time series trend.  -->

To produce a wider range of possibilities to time-varying inference, I introduce three alternative time series models: AR(1) processes, Gaussian processes and basis splines. I begin by defining the random walk by placing a subscript $t$ on the ideal points $\alpha_i$ and stipulating a relationship in the prior between time points:

$$
\alpha_{it} \sim N(\delta_i+ \alpha_{it-1},\sigma_i)
$$ {#eq-rwc}

We give every person $i$ a separate over-time variance parameter $\sigma_i$ and intercept/offset $\delta_i$. It is possible to identify this model by placing a tight upper bound on the variance parameters and adding a polarity constraint in the intercepts $\delta_i$, though as I discuss later, this approach can still fail. For computational reasons, the implementation of this model in `idealstan` employs a non-centered parameterization [@betancourt2013] that reduces dependence between the variance $\sigma_i$ and the prior value of $\alpha_{it-1}$:

$$
\begin{split}
\epsilon_{it} &\sim N(0,1)\\
\sigma_i &\sim E(1)\\
\alpha_{it} &= \delta_i + \alpha_{it-1} + \sigma_i\epsilon_{it}
\end{split}
$$ {#eq-rwnc}

The first new time-series model that I consider is an AR(1) or autoregressive parameterization, which can be understood as a generalization of the random-walk model. To do so, we must add a parameter to the model, $\psi_i$ that is constrained to lie in the $(-1,1)$ interval:

$$
\begin{split}
\epsilon_{it} &\sim N(0,1)\\
\sigma_i &\sim E(1)\\
\alpha_{it} &= \delta_i + \psi_i\alpha_{it-1} + \sigma_i\epsilon_{it}
\end{split}
$$ {#eq-ar1}

If $\psi_i$ lies in the $(-1,1)$ interval, then the series is stationary and will always return or decay to the long-term mean of the series $\delta_i$. By comparison, the random walk model can be thought of as an AR(1) model where $\psi_i$ is fixed at 1. This type of time series model implies that a given person or legislator has a stable over-time average ideal point and that movement away from that over-time average is some kind of exogenous shock that will eventually decay. For these reasons, it makes a substantively different statement about how ideal points might or might not change over time, emphasizing short-run factors over the long-term dynamics implicit in the random-walk formulation.

Both the AR(1) model and the random walk are models of discrete time, or a set number of time points. The next two time series processes that I consider are both continuous models of time; given any set of time points, they are capable of producing smooth interpolations between observed time points. The first such time process I consider is a Gaussian process ideal point model in which the ideal points $\alpha_{it}$ vary in terms of a Gaussian process with a squared-exponential kernel. A Gaussian process (GP) is chosen because it represents the most flexible framework possible for semi-parametric inference of either spatial or temporal autocorrelation [@rasmussen2006]. Given the limited application of this model in political science research, let alone ideal point models specifically, I briefly review the notation for a GP before combining it with the ideal point model.

A GP is usually defined as a "distribution over functions" [@rasmussen2006, 13]. We assume that underlying an observed time series $x_t$ is an unobserved function $f(x_t)$. To estimate the most likely values of $f(x_t)$, I can assume $f(x_t)$ is multivariate Normally-distributed where the mean and covariance of this distribution are themselves functions of the mean and covariance of $f(x_t)$:

$$
f(x_t) \sim N(\mu(f(x_t)),\Sigma(f(x_t)))
$$ {#eq-gp1}

Usually the $\mu(f(x_t))$ function is assumed to be 0 as it does not vary with $t$. Instead, much of the flexibility of the distribution involves specifying a function for $\Sigma(f(x_t))$, the covariance. The function most often used, and incorporated here, is the squared-exponential kernel $k_t(\cdot)$:

$$
k_t(x_t,x_{t'}) = \large \sigma^2_f  e^{\displaystyle -\frac{(x_t - x_{t'})^2}{ 2l^2}} + \sigma^2_{x_t}  
$$ {#eq-gpcov}

What this function does is convert all the distances in terms of time between $x_t$ and $x_{t'}$ into a positive semi-definite covariance matrix $\Sigma$. We can then sample from this multivariate Normal with covariance $\Sigma$ to estimate a distribution over the possible time-series functions $f(x_t)$. The GP framework is so flexible that it can fit an *infinite* number of basis functions of $x_t$ [@rasmussen2006, 14]. Other semi-parametric functions, such as splines and polynomials, are special cases of the GP for certain values of the hyper parameters used in the covariance function [@rasmussen2006, 137-140]. What is even more appealing is that the GP can handle time-varying covariates $X_t\phi'$ simply by including them in place of $\mu(x_t)$. The multivariate Normal will then sample from the specified covariate matrix while averaging over the possible values of the covariates.

What gives the GP the ability to fit so many different functions are the three hyper-parameters in @eq-gpcov. $\sigma^2_{if}$ can be referred to as the marginal standard deviation and represents the total amount of variance in $x_t$ explained by the covariance function $k(x_t,x_{t'})$. A higher value for this hyperparameter will result in more bounce in the time series. $\sigma^2_{ix_t}$, on the other hand, represents residual variance in the time series $x_t$ that the covariance function does not fully explain (which could be interpreted as additional measurement error). Finally, the length-scale parameter $l^2_i$ represents a smoothing factor controlling how much different in time are correlated together. A very low length-scale will allow the time-series to cross the origin at a much higher rate. Conversely, higher length-scales result in smoother functions. The three hyper-parameters both overlap and interact, which makes isolating the effect of any one hyper-parameter difficult.

To employ the GP as a time process for ideal points, we simply replace the observed $x_t$ with the latent ideal points $\alpha_{it}$:

$$
\alpha_i \sim N(0,k_t(\alpha_{it},\alpha_{it'}))
$$ {#eq-gpideal}

This straightforward parameterization shows some of the power of Bayesian modeling. We can include virtually any time process by simply defining the time series over a set of parameters rather than a set of observed data. Of course, whether this model can be identified in a latent variable model with measurement error is a different question. Because of the GP's complexity, I turn next to consider a different option, basis splines.

Splines are a method of constructing semi-parametric time-series by combining polynomial functions of time [@perperoglou2019]. While there are many different kinds, I consider here basis splines due to their ability to fit into the latent variable framework. A basis spline separates a polynomial function of time into individual basis functions that the space of all possible polynomial functions for a given polynomial degree [@deboor1972; @gordon1974]. Because each of the individual functions are relatively simple, this construction allows for a wide variety of polynomial functions to be derived while also enabling efficient sampling.

One central feature of splines is that they are polynomial functions defined over a series of sequential partitions of a given time series known as knots. A spline of a given order is a polynomial function of a given degree (by convention, one less than the order) that is defined for each sequential partition given by the location of the knots. Each piecewise polynomial function must begin and end at the same point of each sequential partition, ensuring a smooth combined function.

The presentation below follows that of @kharratzadeh2017. Given a spline order $d$ and a given number $s$ of sequential knots $q$, we can define a basis matrix $B_{q,d}$ as a function of continuous time points $t \in \mathbb{R}|t_{min} < t < T$:

$$
B_{s,d}(t) = \omega_{s,d} B_{s,d}(t) + (1 - \omega_{s+1,d}B_{s+1,d-1}(t))
$$ {#eq-bfunc}

where the function $\omega(\cdot)$ can be defined as:

$$
\omega_{s,d} = \begin{cases} 
\frac{t-q_s}{q_{s+d-1}-q_s}, & \text{if } q_s \neq q_{s+d-1} \\
0, & \text{otherwise}
\end{cases}
$$

The resulting basis matrix $B_{s,d}(t)$ is a set of vectors of number of columns $T$ and number rows equal to the order of the spline. The polynomial function of $t$ can then be generated by multiplying the basis matrix by a vector of spline coefficients $A_i$ :

$$
S_{q,d}(t) = B_{s,d}(t)A_i
$$ {#eq-splinefunc}

where the length of spline coefficient vector $A_i$ is equal to the number of knots plus the degree of the spline. While in practice $t$ is in a set of discrete time points, defining the function over a continuous interval permits interpolation of the time series, a convenient feature of the spline technique.

We can then sample the spline coefficients by defining a loose prior distribution for $A_i$ as a function of a scaling parameter $\tau$ and a location parameter $A'_i$:

\begin{align}
A'_i &\sim N(0,1)\\
\tau &\sim E(1)\\
A_i &= A_i'\tau
\end{align} \label{eq-splineprior}

The adjustment with $A_i'$ and $\tau$ is to permit non-centered sampling which is more efficient.

The main advantage of using basis splines as I demonstrate in the empirical application is that it is possible to fine-tune the complexity of the time series function. Increasing the degree of the polynomial or the number of knots increases the number of coefficients per person by one. In cases of high sparsity, a low number of knots (or even no knots at all) and a low degree polynomial can be used to avoid multi-modal posteriors.

As a result, these different time series models can be understood in terms of order of complexity. Except for splines, all of the time series models require at least one parameter per time point $t$. The random walk is the simplest of these models as it only requires one additional parameter, $\sigma_i$, per person $i$. The AR(1) model is more complex as it introduces an additional parameter to control the decay rate in the series, $\psi_i$. The GP is the most complex as it has three separate parameters per person, $\sigma^2_{if}$, $l_i^2$ and $\sigma^2_{iY}$, in addition to one estimate per time point $t$. For these reasons, the GP is the most demanding model to fit for a latent variable model as there needs to be a substantial amount of data per time point to estimate a flexible time trend while also separating our measurement noise.

For these reasons, I argue that lower-dimensional time series methods like splines are most useful in situations where data are relatively sparse per time point. As the ideal point model is a measurement model, it can be difficult to separate measurement noise from the additional noise arising from flexibile time series processes.

Having defined these time series models, I know show that it is possible to combine these time-series processes with the model of missing data in @eq-inflate1 to permit inference on ideal points that vary over time and may have non-ignorable missingness:

$$
    L(Y_{ijtr}|\alpha_{it},\gamma_j,\beta_j,\nu_j,\omega_j) = 
    \prod^{T}_{t=1} \prod^{I}_{i=1} \prod^{J}_{j=1}
    \begin{cases}
    \zeta(\alpha_{it}'\nu_j - \omega_j ) & \text{if } r=0, \text{ and} \\
    (1-\zeta({\alpha_{it}'\nu_j - \omega_j}))L(Y_{ijtr}|\alpha_{it},\gamma_j,\beta_j) & \text{if } r=1
    \end{cases}
$$ {#eq-inflate1}

To change from a random-walk to a different time-series model for $\alpha_{it}$, we can simply change the prior for $\alpha_{it}$ that we include in $Pr(\theta)$ with one of the previously-defined time processes. I do not index the item parameters by $t$ because in many applications of the model, the item parameters only occur at one point in time, such as bills in a legislature. It is possible in some situations, such as students taking a test multiple times, for item parameters to vary over time as well. Inference on these parameters' time-series trends would then involve defining time-series priors over these parameters, whether jointly with $\alpha_{it}$ or separately, which is a possible area for future research.

As I have mentioned, identification for models with time-varying $\alpha_{it}$ is a challenging endeavor without extensive discussion in the literature. @quinn2002 addressed this issue for random-walk models by restricting the variance $\sigma_i$ to values strictly between 0 and 0.1. This hard-coded constraint forces the time series to change little over time, though it also limits inference as the time series can only differ so much from each other. By contrast, I impose generally diffuse priors, or priors strong enough to impose some scaling identifiability, without forcing hard constraints on variance or other parameters. If the data are too sparse to permit identification of a latent variable via assigning the polarity of items, as I recommend in this paper, then a simpler time series model in terms of basis splines or a similar design should be considered.

## Diverse Distributions

Up to this point, I have followed convention in assuming that the observed data $Y_{ij}$ is a binary outcome (Bernoulli distribution). Virtually any distribution for $Y_{ij}$ is possible, especially with the flexibility of Bayesian modeling. In this section I incorporate existing distributions used for ideal points while also introducing the reader to distributions, especially ordinal models, that have been rarely employed for this purpose but could have considerable utility. Furthermore, I implement the @kropko2013 method to combine distributions into one joint posterior distribution, permitting models to use mixed datasets, such as both discrete and continuous variables. This method is especially useful for building time-varying indices incorporating measurement indicators that could be binary, ordinal, continuous or count variables, such as what is often available in survey data or panel datasets.

There are two possible ordinal models that can be implemented in the ideal point framework. The first is known as a rating-scale model in the IRT framework and is structurally similar to the ordered logit model. @imai2016a are the first to implement this model for ideal points using an EM algorithm.

Following the notation I introduced, a rating-scale model for $k \in K$ outcomes can be modeled as follows for observed data:

$$
    L(Y_{ijtk}|\alpha_{it},\gamma_j,\beta_j) =  \prod_{i-1}^{I} \prod_{j=1}^{J} \prod_{t=1}^{T}
    \begin{cases} 
    1 -  \zeta(\gamma_j \alpha_i - \beta_j - c_1) & \text{if } K = 0 \\
    \zeta(\gamma_j \alpha_i - \beta_j - c_{k-1}) - \zeta(\gamma_j \alpha_i - \beta_j - c_{k})       & \text{if } 0 < k < K, \text{ and} \\
    \zeta(\gamma_j \alpha_i - \beta_j - c_{k-1}) - 0 & \text{if } k=K
    \end{cases}
$$

where again $\zeta(\cdot)$ represents the logit function. In this version, each ordinal category $k$ less one is assigned a cutpoint $c_k$. In ideal point terms, these cutpoints divide the ideal point space across categories. A direct appliction is to use this model to include abstentions as a middle category between yea and nay votes. In this three outcome setting, two cutting planes exist in the ideal point space such that one end of the spectrum would vote No, the opposite end of the spectrum would vote Yes, and those in between the cutpoints would vote Abstain. As the polarity of items switches, Abstain will always remain in the center of the ideal point distribution, but the Yes and No positions can flip sides. This simple but efficient model helps to model situations where abstentions are in fact quite important, such as parliamentary systems [@brauninger2016].

A different parameterization is known as the graded response model in IRT. This model allows the cutpoints $c_k$ to vary by bill. As a result, instead of including separate cutpoints, we can change the model by indexing each item-specific intercept for each category $k$:

$$
    L(Y_{ijtk}|\alpha_{it},\gamma_j,\beta_j) =  \prod_{i-1}^{I} \prod_{j=1}^{J} \prod_{t=1}^{T}
    \begin{cases} 
    1 -  \zeta(\gamma_j \alpha_i - \beta_{jk-1}) & \text{if } K = 0 \\
    \zeta(\gamma_j \alpha_i - \beta_{jk-1}) - \zeta(\gamma_j \alpha_i - \beta_{jk-1})       & \text{if } 0 < k < K, \text{ and} \\
    \zeta(\gamma_j \alpha_i - \beta_{jk-1}) - 0 & \text{if } k=K
    \end{cases}
$$

Though not employed in political science to any great extent, this alternative formulation to ordered logit permits more flexible inference on the cutpoints. Allowing cutpoints to vary by item will enable much more accurate prediction of categories with fewer numbers of observations. It also increases information about the items/bills, though of course at a cost of including $K$ additional parameters per item. My intention in including it here is not to suggest that it is a superior model to the standard rating-scale approach, but rather as an alternative that may be useful in some situations. Either model can be profitably used to model rollcall data with middle outcomes like abstentions.

I do not review the distributions for the Poisson count model as that has been widely applied in political science [@slapin2008]. However, I do specify two other distributions that have not been widely used in the ideal point literature: the Normal distribution for continuous and the log-normal distribution for positive-continuous outcomes. While @kropko2013 employed the Normal distribution in a traditional IRT framework, no one has yet used the log-Normal distribution.

For a normal distribution, the ideal point model is used to predict the mean of $Y_{ijt}$ given residual standard deviation $\sigma_N$:

$$
  L(Y_{ijt}|\alpha_{it},\gamma_j,\beta_j) =  \prod_{i-1}^{I} \prod_{j=1}^{J} \prod_{t=1}^{T} \frac{1}{ \sqrt{2\pi \sigma_N^2 }} e^{-\frac{(Y_{ijt}-(\gamma_j\alpha_{it}-\beta_j)^2}{2\sigma_N^2}}
$$

The log-normal distribution is the same distribution except that the domain of the distribution is on $log(Y_{ijt})$:

$$
  L(Y_{ijt}|\alpha_{it},\gamma_j,\beta_j) =  \prod_{i-1}^{I} \prod_{j=1}^{J} \prod_{t=1}^{T} \frac{1}{ \sqrt{2\pi \sigma_N^2 }} e^{-\frac{(log(Y_{ijt})-(\gamma_j\alpha_{it}-\beta_j)^2}{2\sigma_N^2}}
$$

The log-normal parameterization is useful for continuous variables that are only defined on positive real numbers. For variables with substantial skew, such as income, this parameterization has a more useful interpretation than the more general Normal distribution. With both of these distributions, most continuous distributions can be modeled with reasonable accuracy.

The utility of including all of these kinds of distributions, besides granting scholars more options with which data to use for ideal point estimation, is to permit modeling of different distributions in the same data. It is very common to have panel data sets and survey data with mixed data types, such as binary, ordinal, counts and continuous variables. Other than @kropko2013 who worked within a traditional IRT framework (i.e., fixed polarity of items), no one has attempted to implement this kind of joint modeling. It promises to offer scholars the ability to easily build time-varying indices such as those employed by V-DEM [@vdem2017] and the transparency index [@jrv2018]. In addition, these joint distributions incorporate missing-data inference and time-varying inference defined previously in this paper.

It is straightforward to do so using the Bayesian framework defined in this paper. I simply index $Y_{ij}$ by $m$ for the $M$ models that each relate to a distinct outcome. These outcomes could be binary, ordinal, counts, continuous or positive-continuous per the notation presented:

$$
  L(Y_{ijtm}|\alpha_{it},\gamma_j,\beta_j) =  \prod_{i-1}^{I} \prod_{j=1}^{J} \prod_{t=1}^{T} \prod_{m=1}^{M} L_m(\gamma_j\alpha_{it}-\beta_j)
$$

where $L_m$ represents the likelihood function for the $m$th distribution. While I suppressed notation for missing data for the sake of brevity, all of these models can also be estimated marginal of missing data $r$ using the notation presented earlier in addition to incorporating time-series dynamics.

## Big Data Inference

The Achilles heel of Bayesian inference is generalizing to large data sets. Markov chains, which form the core of the estimation engine, cannot be parallelized as each iteration depends on the value of the previous iteration. However, recently the Stan team has implemented a way to parallelize the *posterior gradient* computation within a Markov chain, which are necessary and very expensive to calculate the iteration trajectories of Hamiltonian MCMC. Any model likelihood can be parallelized in terms of its gradient calculations so long as the parameters are conditionally independent. For a static ideal point model, this means that the likelihood can be parallelized across persons or items. For a dynamic model, parallelization can be performed across persons as ideal points depend on prior values.

This type of parallelization of the gradient calculations is implemented for all of the models previously described. This allows for the computation of Bayesian ideal points for datasets that were previously intractable due to the length of time required. However, it should be noted that this type of parallelization is relatively costly as not all computations of the Markov Chain are included, and significant information must be shared across parallel threads. Furthermore, this type of parallelization is only possible using in-memory parallel techniques, which restricts the application to single nodes on a computer cluster. Even with these restrictions, though, this type of computation greatly expands the possibilities for Bayesian inference on big datasets, reducing the time required for model convergence from months to days.

## Ideal Point Marginal Effects

Having specified the types of models being proposed, I next turn to a new post-estimation quantity that helps to both clarify what ideal points substantively mean and also to allow for a novel type of covariate relationship. As mentioned previously, the role of external covariates as hierarchical predictors of ideal points is included for both static and time-varying models using a standard linear model framework. For a given ideal point $\alpha_{it}$, the marginal effect of a covariate $x$ on an outcome $Y_{ijt}$ with a link function $g(\cdot)$ is equivalent to:

$$
\frac{\partial Y_{ijt}}{\partial x} \left( g(\gamma_j(\alpha_{it} + \phi x) - \beta_j) \right) = \phi \gamma_j g'(\gamma_j(\alpha_{it} + \phi x) - \beta_j)
$$ {#eq-deriv}

which follows directly from the chain rule.

What is important is that the quantity in @eq-deriv can be decomposed into three constituent parts: the derivative of the link function used to scale the IRT ideal point specification $g'(\cdot)$, the value of the discrimination parameter $\gamma_j$, and the value of the coefficient of the covariate, $\phi$. Multiplying these terms together will obtain the simultaneous change in $Y_{ijt}$ for a one-unit change in the covariate $x$.

There are two methods to calculate these types of marginal effects. The first would be to analytically calculate these quantities for each specification. While this is possible to do, it is quite complicated as different linear model specifications, such as including a quadratic term for $x$, would necessitate a separate analytical derivative. A much more flexible method is to adopt numerical differentiation instead.

As @arel-bundock2022 and @leeper2017a describe, numerical differentiation can calculate marginal effects through a simple formula. For a given covariate $x$ and link function $g(\cdot)$, we can calculate $g'(x)$ by calculating two points on the function $g(x)$ that are only a very small value $\epsilon$ apart:

$$
g'(x) = lim_{\epsilon\rightarrow0}\frac{g(x + \epsilon) - g(x-\epsilon)}{2\epsilon}
$$ {#eq-numderiv} We can apply the formula in @eq-numderiv to our derivative specification in @eq-deriv to calculate ideal point marginal effects for a covariate $x$ separately for each item $j \in \{1, 2, ... J\}$:

$$
\frac{\partial_j Y_{itj=j}}{\partial_j x} \left( g(\gamma_j(\alpha_{it} + \phi x) - \beta_j) \right) = lim_{\epsilon\rightarrow0}\frac{g(\gamma_j(\alpha_{it} + \phi (x + \epsilon)) - \beta_j) - g(\gamma_j(\alpha_{it} + \phi (x-\epsilon)) - \beta_j)}{2\epsilon}
$$ {#eq-idealderiv}

Given these item-specific marginal effects, we can also calculate a sample-average ideal point marginal effect of $x$ by averaging over all items $j$:

$$
\frac{\partial Y_{itj}}{\partial x} \left( g(\gamma_j(\alpha_{it} + \phi x) - \beta_j) \right) = \frac{\sum_{j=1}^J \frac{\partial_j Y_{itj=j}}{\partial_j x}}{J}
$$ {#eq-avgmarg}

However, this sample-average quantity is less interesting as it averages over item discrimination, which is one of the most useful aspects of this quantity of interest. An ideal point marginal effect of a covariate $x$ defined for persons $i$ and item $j$ is the change in that item given its relative polarity and the relative ideal point of the person. As I demonstrate in the empirical section, this quantity intuitively captures the latent variable information encoded in the parameters in a way that cleanly measures a relevant covariate relationship.

The power of numerical differentiation is in allowing flexible linear model specifications. For example, to calculate these marginal effects when including a squared term for $x$, we simply extend the formula by including this term in the limit,

```{=tex}
\begin{equation}
\frac{\partial_j Y_{itj=j}}{\partial_j x} \left( g(\gamma_j(\alpha_{it} + \phi_1 x + \phi_2 x^2) - \beta_j) \right) = lim_{\epsilon\rightarrow0}\frac{\begin{split} &g(\gamma_j(\alpha_{it} + \phi_1 (x + \epsilon) + \phi_2((x + \epsilon)^2)) - \beta_j)\\ - &g(\gamma_j(\alpha_{it} + \phi_1 (x-\epsilon) + \phi_2((x - \epsilon)^2)) - \beta_j)\end{split}}{2\epsilon}
\end{equation}
```
and so on for any differentiable function of $x$.

Finally, it is important to note that because all the parameters in @eq-idealderiv have posterior distributions, it is trivial to obtain uncertainty in these estimates by iterating @eq-idealderiv over the number of posterior draws. The main computational cost is calculating model predictions over $x + \epsilon$ and $x - \epsilon$, although this computation is embarrassingly parallel in the items $j$ and thus not a constraint given adequate computational resources.

# Empirical Example: The 115th Congress

To show how time-varying estimation can increase our ability to learn about ideal points, I fit a time-varying ideal point model with the most commonly used dataset in ideal point analysis: U.S. Congressional roll-call votes, specifically for House votes in the 115th Congress. The research question that I consider is that of the effect of economic disruption on political polarization in Congress. Recent research has proposed that changes in economic conditions, whether in terms of economic inequality and trade, may be affecting increasing polarization in the U.S. Congress. It has long been understood that there is an important interaction in terms of how voters perceive economic recession and their willingness to vote for parties in a polarized environment [@alesina1995].

However, we also know that the salience of policy-making varies with economic conditions that correlate with external shocks [@baker2016]. Most of the studies of trade and other shocks on Congress employ outcomes at the year level, whether the unit of analysis are states or Congressional districts. To show the power of the time-series models I employ, I analyze month-level variation in unemployment by Congressional district and its effect on legislator ideal points in the House to see to what extent localized economic shocks either polarize or de-polarize voting behavior.

<!--# Furthermore, I interact unemployment with the level that these districts experienced adverse shocks due to exposure to Chinese trade to see whether heightened salience due to recession increases polarization among legislators in those districts even further. It has been shown that exposure to Chinese trade has important consequences for manufacturing across the United States [@autor2013], and also has implications for how legislators vote on trade-related legislation [@hall2015] and foreign policy hositility towards China [@kuk2017]. As such, exposure to trade with China is an important potential mediator of the effect of unemployment on legislative behavior.  -->

The data I collect come from the Bureau of Labor Statistic's Local Area Unemployment Statistics program, which produces unadjusted estimates of monthly unemployment by local-level units. To aggregate these numbers to the Congressional district level, I performed a spatial join by areal interpolation (i.e., weighted by amount of overlap) between the county-level monthly series and Congressional district boundaries from @lewis2013. The resulting dataset represents the average monthly unemployment data that would be most relevant to a particular Congressperson's district. Even with this high level of disaggregation, the dataset has very low missingness (50 district-year-month observations), which I impute non-parametrically using the technique of @StekhovenBuehlmann2012.

<!--# I then merge this dataset with a decade-level measure of exposure to Chinese imports from @hall2015 who aggregated information from @autor2013 to produce district-level measures of import exposure per worker. I use this measure as a direct way of determining how strongly Chinese trade has affected a particular district. The measure only has two over-time values, unfortunately, but there is substantial variation across districts. Their measure does not have specific units as it is a weighted multiplication of workers in a district in a given industry and that industry's trade with China, although the measure is scaled to isolate Chinese dependence as opposed to other kinds of trade behavior.  -->

```{r histun}
#| label: fig-hist
#| fig-cap: "Histogram of District-Level Unemployment"

over_states <- readRDS('data/over_states.rds')

out_plot <- over_states %>% 
  ggplot(aes(x=unemp_rate)) +
  geom_histogram() +
  theme(panel.grid=element_blank(),
        panel.background = element_blank()) +
  ylab("") +
  xlab("Unemployment Rates") +
  scale_x_continuous(labels=scales::percent)

print(out_plot)

```

```{r plotcount}
#| label: fig-plotcount
#| fig-cap: Monthly Unemployment Rates by U.S. County, 1990-2018

county_series <- readRDS('data/county_series.rds')

county_series %>% 
  mutate(unemp_rate=unemp/labor_force) %>% 
  filter(state %in% c("AL","CA","NY","MI")) %>% 
    distinct %>% 
  ggplot(aes(y=unemp_rate,x=date_recode)) + 
    geom_line(alpha=0.2,aes(group=fips_county)) +
    theme(panel.grid=element_blank(),
          panel.background = element_blank(),
          strip.background = element_blank(),
          axis.text=element_text(face="bold"),
          strip.text = element_text(face="bold")) +
    facet_wrap(~state,scales='free_y',ncol=2) +
    xlab("") +
    ylab("Unemployment Rates") +
    scale_y_continuous(labels=scales::percent)
  
ggsave("month_unemp.png",width=5,height=4)
```

@fig-hist shows the overall distribution of district-level unemployment rates from 1990 to 2018 inclusive, with a distinct mode at 5.0%, the so-called natural rate of unemployment. @fig-plotcount breaks out the distribution of county-level unemployment rates over time by four states: Alabama, California, Michigan and New York. As can be seen, there is substantial variation both over time and across counties within a state. As such, this series is an excellent data source for showing the utility of time-varying ideal point models. We can also learn empirically from seeing how the effect of these unemployment rates affects legislators in both parties in the House. For the unemployment data itself, we can consider the following hypothesis about the effect of recession on legislators' ideal points:

> H1: When monthly unemployment rates rise, legislator voting behavior in each party become less polarized.

This hypothesis expresses the idea that in times of recession legislators feel more pressure to work across the aisle to produce bipartisan legislation, which we can measure as the spread of ideal points between both parties. We can test this hypothesis by including unemployment rates as an exogenous covariate in the ideal point model while interacting it with an indicator for each legislator's party ID. The effect of the covariate then affects voting behavior differentially depending on the legislator's ideal point, party membership and the relative level of discrimination (polarization) of the vote.

In addition, we might think that any polarization may be asymmetric in terms of different parties. In the 115th Congress, the Republicans held a majority with a Republican president in office. As a result, their response to unemployment shocks could vary from Democrats who were in the opposition. This leads us to a second hypothesis:

> H2: When monthly unemployment rates rise, the resulting polarization of legislative voting behavior is moderated by each legislator's party affiliation.


To test these hypotheses, I fit each of the described time-series models separately to rollcall vote data from the 115th Congress. I do not include the 1st-stage missing-data selection equation. Recorded absences are fairly low for this session of Congress, and so this additional level of complexity is not as useful, especially when the aim is inference on covariates. To identify the model, I pin the discrimination parameters $\gamma_j$ for four party-line votes in which all Republicans voted for a measure as +1, and four Democrat party-line votes in which all Democrats voted against a measure as -1. Utilizing the same pinned parameters for all models allows me to compare their relative ability to obtain identification of a single rotation of the ideal points.

For the spline specifications, I consider splines of degrees 2, 3 and 4 while using only one knot (i.e., one polynomial function for the whole time period). If I were modeling multiple Congressional sessions, it would be possible to use one knot per session to allow for period effects in the time series. With only one Congressional session, however, a single polynomial function is sufficient as it would not be expected for a member of Congress to switch their ideological position in a noticeable way more than once or twice per term.

As a result, I do find that the simpler time series models, i.e. the splines, are more stable and easier to identify with this dataset and the same eight restrictions on item discriminations. @fig-rhat shows convergence diagnostics for all models employing the split-Rhat metric, which measures the ratio of variance of a Markov chain to itself over time and to other chains run independently. For this estimation, three chains were run with 500 post-warmup draws each, providing a difficult test for convergence. Only one model, the spline model with the 2nd-degree spline, had all of its parameters with an Rhat of less than 1.1 with three independent chains, as is commonly advised for Markov Chain Monte Carlo convergence. The other model that converged reasonably well was the random walk specification, though that model had a non-trivial number--236--of parameters with Rhats greater than 1.1, signifying that it did not completely converge to a single mode of posterior density.

```{r loadmodels,include=FALSE}

if(run_all) {
  
unemp1_fit <- readRDS('data/1151_12_1_fit.rds')
unemp2_fit <- readRDS('data/unemp1151_12_2_fit.rds')
unemp3_fit <- readRDS('data/unemp1151_12_3_fit.rds')

unemp1_fitm <- readRDS('data/1152_12_1_fit.rds')
unemp2_fitm <- readRDS('data/unemp1152_12_2_fit.rds')
unemp3_fitm <- readRDS('data/unemp1152_12_3_fit.rds')

unemp_gp_fit <- readRDS('data/unemp115_1_12__gp_fit.rds')
unemp_ar_fit <- readRDS("data/unemp115_1_12_1_ar_fit.rds")
unemp_rw_fit <- readRDS("data/unemp115_1_12_1_rw_fit.rds")

unemp_gp_fitm <- readRDS('data/unemp115_2_12__gp_fit.rds')
unemp_ar_fitm <- readRDS("data/unemp115_2_12_1_ar_fit.rds")
unemp_rw_fitm <- readRDS("data/unemp115_2_12_1_rw_fit.rds")

rhats1 <- id_plot_rhats(unemp1_fit) + ggtitle("Spline 2nd Degree") + labs(caption="",x="") + xlim(c(0.95, 2))

saveRDS(rhats1, "data/rhats1.rds")

rhats2 <- id_plot_rhats(unemp2_fit) + ggtitle("Spline 3rd Degree") + labs(caption="",y="",x="") + xlim(c(0.95, 2))

saveRDS(rhats2, "data/rhats2.rds")

rhats3 <- id_plot_rhats(unemp3_fit) + ggtitle("Spline 4th Degree") + labs(caption="",y="",x="") + xlim(c(0.95, 2))

saveRDS(rhats3, "data/rhats3.rds")

rhatgp <- id_plot_rhats(unemp_gp_fit) + ggtitle("Gaussian Process") + labs(caption="") + xlim(c(0.95, 2))

saveRDS(rhatgp, "data/rhatgp.rds")

rhatar <- id_plot_rhats(unemp_ar_fit) + ggtitle("AR(1)") + labs(caption="",y="") + xlim(c(0.95, 2))

saveRDS(rhatar, "data/rhatar.rds")

rhatrw <- id_plot_rhats(unemp_rw_fit) + ggtitle("Random Walk") + labs(caption="",y="") + xlim(c(0.95, 2))

saveRDS(rhatrw, "data/rhatrw.rds")

un1 <- id_plot_legis_dyn(unemp1_fit,use_ci=F,plot_text = F,person_line_alpha = 0.1) + scale_color_manual(values=c(R="red",
                              D="blue",I="green"),name="") + ggtitle("Spline of 2nd Degree") + labs(y="") +
  scale_x_date(guide = guide_axis(n.dodge = 2))
  
  saveRDS(un1, "data/un1.rds")

un2 <- id_plot_legis_dyn(unemp2_fit,use_ci=F,plot_text = F,person_line_alpha = 0.1) + scale_color_manual(values=c(R="red",
                              D="blue",I="green"),name="") + ggtitle("Spline of 3rd Degree") + labs(y="") +
  scale_x_date(guide = guide_axis(n.dodge = 2))

  saveRDS(un2, "data/un2.rds")

un3 <- id_plot_legis_dyn(unemp3_fit,use_ci=F,plot_text = F,person_line_alpha = 0.1) + scale_color_manual(values=c(R="red",
                              D="blue",I="green"),name="") + ggtitle("Spline of 4th Degree") + labs(y="") +
  scale_x_date(guide = guide_axis(n.dodge = 2))

  saveRDS(un3, "data/un3.rds")

gp <- id_plot_legis_dyn(unemp_gp_fit,use_ci=F,plot_text = F,person_line_alpha = 0.1) + scale_color_manual(values=c(R="red",
                              D="blue",I="green"),name="") + ggtitle("Gaussian Process") + labs(y="") +
  scale_x_date(guide = guide_axis(n.dodge = 2))

  saveRDS(gp, "data/gp.rds")

ar <- id_plot_legis_dyn(unemp_ar_fit,use_ci=F,plot_text = F,person_line_alpha = 0.1) + scale_color_manual(values=c(R="red",
                              D="blue",I="green"),name="") + ggtitle("AR(1) Process") + labs(y="") +
  scale_x_date(guide = guide_axis(n.dodge = 2))

  saveRDS(ar, "data/ar.rds")

rw <- id_plot_legis_dyn(unemp_rw_fit,use_ci=F,plot_text = F,person_line_alpha = 0.1) + scale_color_manual(values=c(R="red",
                              D="blue",I="green"),name="") + ggtitle("Random Walk") +
  scale_x_date(guide = guide_axis(n.dodge = 2)) + labs(y="")

  saveRDS(rw, "data/rw.rds")
  
un1m <- id_plot_legis_dyn(unemp1_fitm,use_ci=F,plot_text = F,person_line_alpha = 0.1) + scale_color_manual(values=c(R="red",
                              D="blue",I="green"),name="") + ggtitle("Spline of 2nd Degree") + labs(y="") +
  scale_x_date(guide = guide_axis(n.dodge = 2))
  
  saveRDS(un1m, "data/un1m.rds")

un2m <- id_plot_legis_dyn(unemp2_fitm,use_ci=F,plot_text = F,person_line_alpha = 0.1) + scale_color_manual(values=c(R="red",
                              D="blue",I="green"),name="") + ggtitle("Spline of 3rd Degree") + labs(y="") +
  scale_x_date(guide = guide_axis(n.dodge = 2))

  saveRDS(un2m, "data/un2m.rds")

un3m <- id_plot_legis_dyn(unemp3_fitm,use_ci=F,plot_text = F,person_line_alpha = 0.1) + scale_color_manual(values=c(R="red",
                              D="blue",I="green"),name="") + ggtitle("Spline of 4th Degree") + labs(y="") +
  scale_x_date(guide = guide_axis(n.dodge = 2))

  saveRDS(un3m, "data/un3m.rds")

gpm <- id_plot_legis_dyn(unemp_gp_fitm,use_ci=F,plot_text = F,person_line_alpha = 0.1) + scale_color_manual(values=c(R="red",
                              D="blue",I="green"),name="") + ggtitle("Gaussian Process") + labs(y="") +
  scale_x_date(guide = guide_axis(n.dodge = 2))

  saveRDS(gpm, "data/gpm.rds")

arm <- id_plot_legis_dyn(unemp_ar_fitm,use_ci=F,plot_text = F,person_line_alpha = 0.1) + scale_color_manual(values=c(R="red",
                              D="blue",I="green"),name="") + ggtitle("AR(1) Process") + labs(y="") +
  scale_x_date(guide = guide_axis(n.dodge = 2))

  saveRDS(arm, "data/arm.rds")

rwm <- id_plot_legis_dyn(unemp_rw_fitm,use_ci=F,plot_text = F,person_line_alpha = 0.1) + scale_color_manual(values=c(R="red",
                              D="blue",I="green"),name="") + ggtitle("Random Walk") +
  scale_x_date(guide = guide_axis(n.dodge = 2)) + labs(y="")

  saveRDS(rwm, "data/rwm.rds")
  
un1_a <- id_plot_legis_dyn(unemp1_fit,include="AMASH, Justin",
                         use_ci=T,plot_text = F) + scale_color_manual(values=c(R="red",
                              D="blue",I="green"),name="") + ggtitle("Spline of 2nd Degree") + labs(y="") +
  scale_x_date(guide = guide_axis(n.dodge = 2))
  
  saveRDS(un1_a, "data/un1_a.rds")

un2_a <- id_plot_legis_dyn(unemp2_fit,use_ci=T,include="AMASH, Justin",plot_text = F) + scale_color_manual(values=c(R="red",
                              D="blue",I="green"),name="") + ggtitle("Spline of 3rd Degree") + labs(y="") +
  scale_x_date(guide = guide_axis(n.dodge = 2))
 
  saveRDS(un2_a, "data/un2_a.rds")

un3_a <- id_plot_legis_dyn(unemp3_fit,use_ci=T,include="AMASH, Justin",plot_text = F) + scale_color_manual(values=c(R="red",
                              D="blue",I="green"),name="") + ggtitle("Spline of 4th Degree") + labs(y="") +
  scale_x_date(guide = guide_axis(n.dodge = 2))
  
  saveRDS(un3_a, "data/un3_a.rds")

gp_a <- id_plot_legis_dyn(unemp_gp_fit,use_ci=T,include="AMASH, Justin",plot_text = F) + scale_color_manual(values=c(R="red",
                              D="blue",I="green"),name="") + ggtitle("Gaussian Process") + labs(y="") +
  scale_x_date(guide = guide_axis(n.dodge = 2))

  saveRDS(gp_a, "data/gp_a.rds")

ar_a <- id_plot_legis_dyn(unemp_ar_fit,use_ci=T,include="AMASH, Justin",plot_text = F) + scale_color_manual(values=c(R="red",
                              D="blue",I="green"),name="") + ggtitle("AR(1) Process") + labs(y="") +
  scale_x_date(guide = guide_axis(n.dodge = 2))

  saveRDS(ar_a, "data/ar_a.rds")

rw_a <- id_plot_legis_dyn(unemp_rw_fit,use_ci=T,include="AMASH, Justin",plot_text = F) + scale_color_manual(values=c(R="red",
                              D="blue",I="green"),name="") + ggtitle("Random Walk") +
  scale_x_date(guide = guide_axis(n.dodge = 2)) + labs(y="") +
  scale_y_reverse()

  saveRDS(rw_a, "data/rw_a.rds")
  
  un1_am <- id_plot_legis_dyn(unemp1_fitm,include="AMASH, Justin",
                         use_ci=T,plot_text = F) + scale_color_manual(values=c(R="red",
                              D="blue",I="green"),name="") + ggtitle("Spline of 2nd Degree") + labs(y="") +
  scale_x_date(guide = guide_axis(n.dodge = 2))
  
  saveRDS(un1_am, "data/un1_am.rds")

un2_am <- id_plot_legis_dyn(unemp2_fitm,use_ci=T,include="AMASH, Justin",plot_text = F) + scale_color_manual(values=c(R="red",
                              D="blue",I="green"),name="") + ggtitle("Spline of 3rd Degree") + labs(y="") +
  scale_x_date(guide = guide_axis(n.dodge = 2))
 
  saveRDS(un2_am, "data/un2_am.rds")

un3_am <- id_plot_legis_dyn(unemp3_fitm,use_ci=T,include="AMASH, Justin",plot_text = F) + scale_color_manual(values=c(R="red",
                              D="blue",I="green"),name="") + ggtitle("Spline of 4th Degree") + labs(y="") +
  scale_x_date(guide = guide_axis(n.dodge = 2))
  
  saveRDS(un3_am, "data/un3_am.rds")

gp_am <- id_plot_legis_dyn(unemp_gp_fitm,use_ci=T,include="AMASH, Justin",plot_text = F) + scale_color_manual(values=c(R="red",
                              D="blue",I="green"),name="") + ggtitle("Gaussian Process") + labs(y="") +
  scale_x_date(guide = guide_axis(n.dodge = 2))

  saveRDS(gp_am, "data/gp_am.rds")

ar_am <- id_plot_legis_dyn(unemp_ar_fitm,use_ci=T,include="AMASH, Justin",plot_text = F) + scale_color_manual(values=c(R="red",
                              D="blue",I="green"),name="") + ggtitle("AR(1) Process") + labs(y="") +
  scale_x_date(guide = guide_axis(n.dodge = 2))

  saveRDS(ar_am, "data/ar_am.rds")

rw_am <- id_plot_legis_dyn(unemp_rw_fitm,use_ci=T,include="AMASH, Justin",plot_text = F) + scale_color_manual(values=c(R="red",
                              D="blue",I="green"),name="") + ggtitle("Random Walk") +
  scale_x_date(guide = guide_axis(n.dodge = 2)) + labs(y="") +
  scale_y_reverse()

  saveRDS(rw_am, "data/rw_am.rds")
  
# Now do Tim Walz  
  
un1_tw <- id_plot_legis_dyn(unemp1_fit,include="WALZ, Tim",
                         use_ci=T,plot_text = F) + scale_color_manual(values=c(R="red",
                              D="blue",I="green"),name="") + ggtitle("Spline of 2nd Degree") + labs(y="") +
  scale_x_date(guide = guide_axis(n.dodge = 2))
  
  saveRDS(un1_tw, "data/un1_tw.rds")

un2_tw <- id_plot_legis_dyn(unemp2_fit,use_ci=T,include="WALZ, Tim",plot_text = F) + scale_color_manual(values=c(R="red",
                              D="blue",I="green"),name="") + ggtitle("Spline of 3rd Degree") + labs(y="") +
  scale_x_date(guide = guide_axis(n.dodge = 2))
 
  saveRDS(un2_tw, "data/un2_tw.rds")

un3_tw <- id_plot_legis_dyn(unemp3_fit,use_ci=T,include="WALZ, Tim",plot_text = F) + scale_color_manual(values=c(R="red",
                              D="blue",I="green"),name="") + ggtitle("Spline of 4th Degree") + labs(y="") +
  scale_x_date(guide = guide_axis(n.dodge = 2))
  
  saveRDS(un3_tw, "data/un3_tw.rds")

gp_tw <- id_plot_legis_dyn(unemp_gp_fit,use_ci=T,include="WALZ, Tim",plot_text = F) + scale_color_manual(values=c(R="red",
                              D="blue",I="green"),name="") + ggtitle("Gaussian Process") + labs(y="") +
  scale_x_date(guide = guide_axis(n.dodge = 2))

  saveRDS(gp_tw, "data/gp_tw.rds")

ar_tw <- id_plot_legis_dyn(unemp_ar_fit,use_ci=T,include="WALZ, Tim",plot_text = F) + scale_color_manual(values=c(R="red",
                              D="blue",I="green"),name="") + ggtitle("AR(1) Process") + labs(y="") +
  scale_x_date(guide = guide_axis(n.dodge = 2))

  saveRDS(ar_tw, "data/ar_tw.rds")

rw_tw <- id_plot_legis_dyn(unemp_rw_fit,use_ci=T,include="WALZ, Tim",plot_text = F) + scale_color_manual(values=c(R="red",
                              D="blue",I="green"),name="") + ggtitle("Random Walk") +
  scale_x_date(guide = guide_axis(n.dodge = 2)) + labs(y="") +
  scale_y_reverse()

  saveRDS(rw_tw, "data/rw_tw.rds")
  
# Tim Walz with missing data
  
un1_twm <- id_plot_legis_dyn(unemp1_fitm,include="WALZ, Tim",
                         use_ci=T,plot_text = F) + scale_color_manual(values=c(R="red",
                              D="blue",I="green"),name="") + ggtitle("Spline of 2nd Degree") + labs(y="") +
  scale_x_date(guide = guide_axis(n.dodge = 2))
  
  saveRDS(un1_twm, "data/un1_twm.rds")

un2_twm <- id_plot_legis_dyn(unemp2_fitm,use_ci=T,include="WALZ, Tim",plot_text = F) + scale_color_manual(values=c(R="red",
                              D="blue",I="green"),name="") + ggtitle("Spline of 3rd Degree") + labs(y="") +
  scale_x_date(guide = guide_axis(n.dodge = 2))
 
  saveRDS(un2_twm, "data/un2_twm.rds")

un3_twm <- id_plot_legis_dyn(unemp3_fitm,use_ci=T,include="WALZ, Tim",plot_text = F) + scale_color_manual(values=c(R="red",
                              D="blue",I="green"),name="") + ggtitle("Spline of 4th Degree") + labs(y="") +
  scale_x_date(guide = guide_axis(n.dodge = 2))
  
  saveRDS(un3_twm, "data/un3_twm.rds")

gp_twm <- id_plot_legis_dyn(unemp_gp_fitm,use_ci=T,include="WALZ, Tim",plot_text = F) + scale_color_manual(values=c(R="red",
                              D="blue",I="green"),name="") + ggtitle("Gaussian Process") + labs(y="") +
  scale_x_date(guide = guide_axis(n.dodge = 2))

  saveRDS(gp_twm, "data/gp_twm.rds")

ar_twm <- id_plot_legis_dyn(unemp_ar_fitm,use_ci=T,include="WALZ, Tim",plot_text = F) + scale_color_manual(values=c(R="red",
                              D="blue",I="green"),name="") + ggtitle("AR(1) Process") + labs(y="") +
  scale_x_date(guide = guide_axis(n.dodge = 2))

  saveRDS(ar_twm, "data/ar_twm.rds")

rw_twm <- id_plot_legis_dyn(unemp_rw_fitm,use_ci=T,include="WALZ, Tim",plot_text = F) + scale_color_manual(values=c(R="red",
                              D="blue",I="green"),name="") + ggtitle("Random Walk") +
  scale_x_date(guide = guide_axis(n.dodge = 2)) + labs(y="") +
  scale_y_reverse()

  saveRDS(rw_twm, "data/rw_twm.rds")
  
# need to export covariates, anything else we might need
  
s1_cov <- unemp1_fit@stan_samples$draws("legis_x") %>% 
  summarize_draws %>% 
  mutate(Model="Spline 2nd Degree",Missingness="Unmodeled")
s2_cov <- unemp2_fit@stan_samples$draws("legis_x") %>% 
  summarize_draws %>% 
  mutate(Model="Spline 3rd Degree",Missingness="Unmodeled")
s3_cov <- unemp3_fit@stan_samples$draws("legis_x") %>% 
  summarize_draws %>% 
  mutate(Model="Spline 4th Degree",Missingness="Unmodeled")
sgp_cov <- unemp_gp_fit@stan_samples$draws("legis_x") %>% 
  summarize_draws %>% 
  mutate(Model="Gaussian Process",Missingness="Unmodeled")
sar_cov <- unemp_ar_fit@stan_samples$draws("legis_x") %>% 
  summarize_draws %>% 
  mutate(Model="AR(1)",Missingness="Unmodeled")
srw_cov <- unemp_rw_fit@stan_samples$draws("legis_x") %>% 
  summarize_draws %>% 
  mutate(Model="Random Walk",Missingness="Unmodeled")

s1_covm <- unemp1_fitm@stan_samples$draws("legis_x") %>% 
  summarize_draws %>% 
  mutate(Model="Spline 2nd Degree",Missingness="Modeled")
s2_covm <- unemp2_fitm@stan_samples$draws("legis_x") %>% 
  summarize_draws %>% 
  mutate(Model="Spline 3rd Degree",Missingness="Modeled")
s3_covm <- unemp3_fitm@stan_samples$draws("legis_x") %>% 
  summarize_draws %>% 
  mutate(Model="Spline 4th Degree",Missingness="Modeled")
sgp_covm <- unemp_gp_fitm@stan_samples$draws("legis_x") %>% 
  summarize_draws %>% 
  mutate(Model="Gaussian Process",Missingness="Modeled")
sar_covm <- unemp_ar_fitm@stan_samples$draws("legis_x") %>% 
  summarize_draws %>% 
  mutate(Model="AR(1)",Missingness="Modeled")
srw_covm <- unemp_rw_fitm@stan_samples$draws("legis_x") %>% 
  summarize_draws %>% 
  mutate(Model="Random Walk",Missingness="Unmodeled")

cov_combined <- bind_rows(s1_cov, s2_cov, s3_cov,
                          sgp_cov, sar_cov, srw_cov,
                          s1_covm, s2_covm, s3_covm,
                          sgp_covm, sar_covm, srw_covm)

saveRDS(cov_combined, "data/cov_combined.rds")
  
  
}

```

```{r loadmodelsbig}

if(run_all) {
  
  unemp1_all_fit <- readRDS('data/unempall1_12_1_fit.rds')
  unemp2_all_fit <- readRDS('data/unempall1_12_2_fit.rds')
  unemp3_all_fit <- readRDS('data/unempall1_12_3_fit.rds')
  
  # do plots/rhats here
  
  un1big <- id_plot_legis_dyn(unemp1_all_fit,use_ci=F,plot_text = F,person_line_alpha = 0.1) + scale_color_manual(values=c(R="red",
                              D="blue",I="green"),name="") + ggtitle("Spline of 2nd Degree") + labs(y="") +
  scale_x_date(guide = guide_axis(n.dodge = 2))
  
  saveRDS(un1big, "data/un1big.rds")

un2big <- id_plot_legis_dyn(unemp2_all_fit,use_ci=F,plot_text = F,person_line_alpha = 0.1) + scale_color_manual(values=c(R="red",
                              D="blue",I="green"),name="") + ggtitle("Spline of 3rd Degree") + labs(y="") +
  scale_x_date(guide = guide_axis(n.dodge = 2))

  saveRDS(un2big, "data/un2big.rds")

un3big <- id_plot_legis_dyn(unemp3_all_fit,use_ci=F,plot_text = F,person_line_alpha = 0.1) + scale_color_manual(values=c(R="red",
                              D="blue",I="green"),name="") + ggtitle("Spline of 4th Degree") + labs(y="") +
  scale_x_date(guide = guide_axis(n.dodge = 2))

  saveRDS(un3big, "data/un3big.rds")
  
  # need legis covariates
  
  
  
}



```

```{r loadmodelsbigm}

if(run_all) {
  
  unemp1_all_fitm <- readRDS('data/unempall2_12_1_fit.rds')
  unemp2_all_fitm <- readRDS('data/unempall2_12_2_fit.rds')
  unemp3_all_fitm <- readRDS('data/unempall2_12_3_fit.rds')
  
  # do plots/rhats here
  
}



```

I next examine the overall distributions of the ideal point scores over time for each model in @fig-alldist. To help with plotting these distributions, uncertainty intervals are removed, and the scale is limited to ideal point scores of -3 to +3. As can be seen, the different methods have differing interpretations of ideal point trajectories, although most of the parameterizations show periods where voting appears to be less polarized in the middle of 2017. As might be expected, while on average the two parties remain well-separated, in practice there are periods of more bipartisan cooperation. This type of monthly heterogeneity is masked when the ideal points are aggregated to the yearly level, and is a substantively interesting form of variation.

```{r dists}
#| label: fig-alldist
#| fig-cap: "Comparison of Time-varying Ideal Point Methods for the 115th Congress"
#| message: false
#| warning: false
#| fig-height: 6
#| fig-width: 6

  un1 <- readRDS("data/un1.rds")
  un2 <- readRDS("data/un2.rds")
  un3 <- readRDS("data/un3.rds")
  rw <- readRDS("data/rw.rds")
  ar <- readRDS("data/ar.rds")
  gp <- readRDS("data/gp.rds")

un1 + un2 + un3 + ar + gp + rw + plot_layout(nrow=2,
                                             guides="collect") + 
  plot_annotation(caption="Posterior average values shown for 115th Congress legislator ideal points.\nItem discrimination for Republican party-line votes was constrained to be positive.") & theme(legend.position = "top",
                                                                                                                                                                                                    plot.title=element_text(size=9))

```

```{r distscompwalz}
#| include: false

if(run_all) {
  
  un1_tw <- readRDS("data/un1_tw.rds")
  un2_tw <- readRDS("data/un2_tw.rds")
  un3_tw <- readRDS("data/un3_tw.rds")
  gp_tw <- readRDS("data/gp_tw.rds")
  rw_tw <- readRDS("data/rw_tw.rds")
  ar_tw <- readRDS("data/ar_tw.rds")
  
}




```

```{r distsamash}
#| label: fig-amashdist
#| fig-cap: "Comparison of Justin Amash Time-varying Ideal Points for the 115th Congress"
#| message: false
#| warning: false
#| fig-height: 6
#| fig-width: 6

  un1_a <- readRDS("data/un1_a.rds")
  un2_a <- readRDS("data/un2_a.rds")
  un3_a <- readRDS("data/un3_a.rds")
  gp_a <- readRDS("data/gp_a.rds")
  rw_a <- readRDS("data/rw_a.rds")
  ar_a <- readRDS("data/ar_a.rds")

un1_a + un2_a + un3_a + ar_a + gp_a + rw_a + plot_layout(nrow=2,
                                             guides="collect") + 
  plot_annotation(caption="Posterior average values shown for 115th Congress legislator ideal points.\nItem discrimination for Republican party-line votes was constrained to be positive.") & theme(legend.position = "top",
                                                                                                                                                                                                    plot.title=element_text(size=9))

```

```{r distsamash1}
#| label: fig-amashdist1
#| fig-cap: "Comparison of Justin Amash Time-varying Ideal Points for the 115th Congress without Missing Votes"
#| message: false
#| warning: false
#| fig-height: 6
#| fig-width: 6

  un1_am <- readRDS("data/un1_am.rds")
  un2_am <- readRDS("data/un1_am.rds")
  un3_am <- readRDS("data/un1_am.rds")
  gp_am <- readRDS("data/gp_am.rds")
  rw_am <- readRDS("data/rw_am.rds")
  ar_am <- readRDS("data/ar_am.rds")

un1_am + un2_am + un3_am + ar_am + gp_am + rw_am + plot_layout(nrow=2,
                                             guides="collect") + 
  plot_annotation(caption="Posterior average values shown for 115th Congress legislator ideal points.\nItem discrimination for Republican party-line votes was constrained to be positive.") & theme(legend.position = "top",
                                                                                                                                                                                                    plot.title=element_text(size=9))

```



```{r absence}
#| fig-cap: "Time-varying Absence Rates in the 115th Senate"

# need vertical lines for beginning of sessions

rollcalls %>% 
  arrange(congress, date) %>% 
  group_by(congress) %>%      
  mutate(session_day_counter = ifelse(row_number()==n(),date_month,NA_Date_)) %>% 
  ungroup() %>% 
  filter(party_code %in% c("R","D")) %>% 
  group_by(party_code,date_month,session_day_counter) %>% 
  summarise(`Absences`=mean(cast_code=="Abstention"),
            session_day_counter=session_day_counter[1]) %>% 
  ggplot(aes(y=Absences,x=date_month)) +
  geom_line(aes(colour=party_code,linetype=party_code)) +
  stat_smooth(aes(linetype=party_code),method="lm") +
  geom_vline(aes(xintercept=session_day_counter),linetype=3) +
  theme_minimal() + 
  ylim(c(0,0.27))

```

```{r olsabs}

rollcalls_abs <- rollcalls %>% 
  arrange(congress, date) %>% 
  group_by(congress) %>%      
  mutate(session_day_counter = as.numeric(date) - min(as.numeric(date)) + 1) %>% 
  ungroup() %>% 
  filter(party_code %in% c("R","D")) %>% 
  group_by(party_code,date_month,congress) %>% 
  summarise(`Absences`=mean(cast_code=="Abstention"),
            session_day_counter=max(session_day_counter))

c1 <- lm(Absences ~ session_day_counter + 
             I(session_day_counter^2) + 
           I(session_day_counter^3) +
             factor(congress),data=rollcalls_abs)

plot_predictions(c1,condition = "session_day_counter")

```

We can see as well that the more flexible methods that estimate one parameter per time point--AR(1), random walk and Gaussian processes--do not reveal much additional variation of interest beyond the much simpler spline models. The Guassian process model does show more individual-level heterogeneity, but on the whole the party-level trends are if anything more stable. Of course, the model is not completely identified, so it could be that some of the heterogeneity is being masked for that reason.

I next look at the coefficients $\phi$ that relate the legislator-level covariates to the ideal points scores. These values are shown in @tbl-coef. Because some models did not adequately converge across chains, I only use the chain that had the highest posterior log-probability. As can be seen, there is variation across the models in terms of the magnitude and even the sign of the coefficients. There is relatively less variation between splines of different degree, which in addition the descriptive plots in @fig-alldist, suggests that the specific form of the spline is relatively minor, at least when studying legislative ideal point processes. In a situation in which a latent variable changed more quickly, it could be more necessary to have a higher spline degree and more knots as well.

```{r legisx}
#| label: tbl-coef
#| tbl-cap: Coefficients of Legislator Hierarchical Covariates ($\phi$)



readRDS("data/cov_combined.rds") %>% 
  bind_rows %>% 
  mutate(variable=recode(variable,
                         `legis_x[1]`="Unemployment",
                         `legis_x[2]`="GOP",
                         `legis_x[3]`="UnemploymentXGOP")) %>% 
  select(Model,Variable="variable",
         Missingness,
         `Posterior Median`="median",
         `5% Quantile`="q5",
         `95% Quantile`="q95") %>% 
  arrange(Variable,Model) %>% 
  filter(Variable %in% c("Unemployment",
                         "GOP",
                         "UnemploymentXGOP")) %>% 
  knitr::kable(digit=3,booktabs=T)

```

We can next examine the ideal point marginal effects of unemployment. Given that we are including an interaction, we can consider the marginal effect of unemployment for all legislators and also separately for Democrats and Republicans. We show item-level marginal effects in @fig-allplot from the second-degree spline model as it showed the best convergence behavior. Each rollcall vote is colored by its relative level of discrimination $\gamma_j$, where higher values signify more polarizing votes. As can be seen, we do see a relationship between district-level unemployment and legislator voting in the 115th Congress. For bills that are the most polarizing, the probability of a legislator voting for one of these bills increases by 2 percent on average as unemployment in their district increases by 1 percent. The effect is also assymetric--because negative discrimination values signify more liberal votes, legislators tend to vote more in favor of very conservative bills and are less likely to vote for very liberal bills when unemployment is increasing.

```{r loadmarg}
#| include: false

by_party <- readRDS("data/by_party.rds")

```

```{r calcmargeffs}
#| label: fig-allplot
#| fig-cap: "Item-level Ideal Point Marginal Effects of Monthly District Unemployment on Legislator Ideal Points"
#| out-width: 100%
#| fig-height: 6

c1 <-   by_party %>% 
    mutate(group_id=factor(group_id,levels=c("D","R"),
                           labels=c("Democrats", "Republicans"))) %>% 
    filter(!(item_orig %in% c("115_1050","115_588"))) %>% 
    ggplot(aes(y=mean_est,
               x=reorder(item_id,mean_est))) +
    geom_linerange(aes(ymin=low_est,
                       ymax=high_est,
                       colour=`median`)) +
    facet_wrap(~group_id) +
    ggthemes::theme_tufte() + 
    scale_colour_viridis_c(name="Discrimination") +
    coord_flip() +
    labs(y="Marginal Change in Probability of Voting",
         x="Rollcalls",
         caption="Marginal effect of unemployment on voting on a specific rollcall in the 115th Congress.\nEach rollcall uncertainty interval is colored by the average posterior discrimination of that rollcall.\nMore positive rollcalls tend to be more conservative votes,\nwhile more negative rollcalls tend to be more liberal votes.") +
    geom_hline(yintercept=0,linetype=2) +
    ggdark::dark_mode() +
    theme(axis.text.y=element_blank(),
          axis.ticks.y=element_blank()) +
    # ggtitle("Marginal Effect of Monthly Unemployment on Rollcall Votes in 115th Congress") +
  theme(legend.position = "bottom")

ggsave("idealpt_marg_eff.png",plot=c1,width=6,height=4)
  
print(c1)
```

```{r calcmargeffsp,include=FALSE, eval=FALSE}
#| label: fig-allplotp
#| fig-cap: "Comparison of Item-level Ideal Point Marginal Effects of Unemployment on Legislator Ideal Points and Party Moderation"

by_party %>% 
    ungroup %>% 
    mutate(group_id=factor(group_id,levels=c("D","R"),
                           labels=c("Democrats", "Republicans")),
           item_rank=rank(mean_est)) %>% 
    filter(!(item_orig %in% c("115_1050","115_588"))) %>% 
    ggplot(aes(y=mean_est,
               x=item_rank)) +
    geom_ribbon(aes(ymin=low_est,
                    ymax=high_est,
                    fill=group_id),alpha=0.5) +
    #facet_wrap(~group_id) +
    ggthemes::theme_tufte() + 
    scale_fill_manual(values=c(Republicans="red",
                              Democrats="blue",I="green"),name="") +
    coord_flip() +
    labs(y="Marginal Change in Probability of Voting",
         x="Rollcalls") +
    geom_hline(yintercept=0,linetype=2) +
    ggdark::dark_mode() +
    theme(axis.text.y=element_blank(),
          axis.ticks.y=element_blank()) +
    ggtitle("Marginal Effect of District Monthly Unemployment on Rollcall Votes in 115th Congress",
            subtitle="Marginal Effect of Unemployment Mediated by Legislator Ideal Point and Bill Discrimination")

```

Overall, the results of these models do show that legislative behavior changes during times of higher local unemployment. Furthermore, we can measure these movements in ideal points down to the monthly-level, permitting very precise statements about how legislative behavior changes. In general, the effects of 1% change in district-level unemployment on legislators are modest, though we would expect modest effects as this 1% change represents one-month's worth of unemployment data. Aggregated over time, these effects could grow to be substantial.

# Discussion

The aim of this paper was to put forward a general framework for ideal point estimation that would both integrate existing approaches while providing new areas for analysis, particularly in missing data and over-time inference. The intention is not to suggest, however, that this framework represents all that can be done with ideal points. This paper's approach is general and designed to fit a wide variety of applications, but investigating the intricacies of social situations will always require more custom modeling. The utility of this study is rather to promote the use of ideal point models as a core part of empirical research in political science, not just for legislative studies or rollcall voting data.

The reason that ideal point models have wider applicability is because social choice processes where individuals decide between competing alternatives happen very often. If these alternatives are polarizing, or represent competing poles, than an ideal point model can help locate people's positions along the axis of competition while also determine the relative weight of the items they are choosing. The ultimate payoff is that this kind of modeling will help political scientists focus on inference on the constructs they care about, which are usually unobservable, such as partisanship, ethnic identity, corruption, and political ideology. Instead of being forced into arbitrary measurement decisions over the available indicators, political scientists can maximize the information in their data by collapsing multiple indicators to a single dimension.

As a result, this paper largely sidesteps the debate about how to determine when ideal points represent political ideology. Defining a the latent construct can only be done in the context of a specific empirical application. The political ideology interpretation is most useful for American politics where legislators have considerable latitude in voting behavior, but this does not mean that ideal point representations of other polities or social situations are not as useful. So long as a social choice process is involved, ideal point models can extract useful information from noisy data.

# Conclusion

In this paper, I presented a generalization of the increasingly popular ideal point model in the Bayesian IRT framework. I extended the model with new modes of missing-data, time-varying processes, joint distributions, and quantities of interest. the contribution of this paper is an analytical tool that can extend the domain and applicability of ideal point models across the discipline as all of these models are available in a single R package, `idealstan`. Crucially, all of these methods build on each other so that missing data can be incorporated in time-varying models or in joint distributions and vice versa.

All of these models are available in an R package called `idealstan`. This package is designed to automate the sometimes arduous process of preparing data for ideal point modeling, including identifying parameters. Furthermore, the use of a single R package for all models enables researchers to compare different models on the same data with ease. Parallelization of standard Bayesian inference is also incorporated in the package to enable estimation of the ever larger data sets available to political scientists.

<!-- With the Chinese import shock data, we can consider the following hypothesis: -->

<!-- > H2: When monthly unemployment rates rise in districts with a higher exposure to Chinese import competition, legislators in each party become more polarized. -->

<!-- This hypothesis expresses a conditional relationship that runs in the opposite direction of the first hypothesis. It would seem that, as existing literature has shown, exposure to Chinese import shocks shifts legislators' ideal points on trade policy and increases polarization, that there should be a strongly interactive relationshp between district-level unemployment and Chinese import exposure. When both are high, we would have a strong prior that legislators will polarize away from each other. This hypothesis can be tested with a three-way interaction of party ID, unemployment rates and Chinese shock exposure at the district level. -->

<!-- Finally we can also consider the following hypothesis: -->

<!-- > H3: When monthly unemployment rates rise in districts with a higher exposure to Chinese import competition, legislators in each party become more anti-trade. -->

<!-- Instead of testing overall polarization as H2 proposes, we can see if the movement in ideal points is concentrated on bills on trade policy by adding an indicator for trade-related legislation to the ideal point model and interacting it with party ID, unemployment, and China import shock exposure. -->

# References
